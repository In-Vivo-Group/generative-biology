[
  {
    "URL": "https://academic.oup.com/nar/article/26/1/94/2379498",
    "type": "webpage",
    "id": "yOdF9F1J",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://academic.oup.com/nar/article/26/1/94/2379498"
  },
  {
    "id": "xBZDDNz8",
    "type": "paper-conference",
    "abstract": "The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models.",
    "container-title": "Proceedings of the 20th Workshop on Biomedical Language Processing",
    "DOI": "10.18653/v1/2021.bionlp-1.24",
    "event-place": "Online",
    "event-title": "BioNLP 2021",
    "page": "221–227",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Online",
    "source": "ACLWeb",
    "title": "BioM-Transformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA",
    "title-short": "BioM-Transformers",
    "URL": "https://aclanthology.org/2021.bionlp-1.24",
    "author": [
      {
        "family": "Alrowili",
        "given": "Sultan"
      },
      {
        "family": "Shanker",
        "given": "Vijay"
      }
    ],
    "editor": [
      {
        "family": "Demner-Fushman",
        "given": "Dina"
      },
      {
        "family": "Cohen",
        "given": "Kevin Bretonnel"
      },
      {
        "family": "Ananiadou",
        "given": "Sophia"
      },
      {
        "family": "Tsujii",
        "given": "Junichi"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          6
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://aclanthology.org/2021.bionlp-1.24"
  },
  {
    "id": "2aufANiR",
    "type": "paper-conference",
    "abstract": "We propose a new task, Text2Mol, to retrieve molecules using natural language descriptions as queries. Natural language and molecules encode information in very different ways, which leads to the exciting but challenging problem of integrating these two very different modalities. Although some work has been done on text-based retrieval and structure-based retrieval, this new task requires integrating molecules and natural language more directly. Moreover, this can be viewed as an especially challenging cross-lingual retrieval problem by considering the molecules as a language with a very unique grammar. We construct a paired dataset of molecules and their corresponding text descriptions, which we use to learn an aligned common semantic embedding space for retrieval. We extend this to create a cross-modal attention-based model for explainability and reranking by interpreting the attentions as association rules. We also employ an ensemble approach to integrate our different architectures, which significantly improves results from 0.372 to 0.499 MRR. This new multimodal approach opens a new perspective on solving problems in chemistry literature understanding and molecular machine learning.",
    "container-title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    "DOI": "10.18653/v1/2021.emnlp-main.47",
    "event-place": "Online and Punta Cana, Dominican Republic",
    "event-title": "EMNLP 2021",
    "page": "595–607",
    "publisher": "Association for Computational Linguistics",
    "publisher-place": "Online and Punta Cana, Dominican Republic",
    "source": "ACLWeb",
    "title": "Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries",
    "title-short": "Text2Mol",
    "URL": "https://aclanthology.org/2021.emnlp-main.47",
    "author": [
      {
        "family": "Edwards",
        "given": "Carl"
      },
      {
        "family": "Zhai",
        "given": "ChengXiang"
      },
      {
        "family": "Ji",
        "given": "Heng"
      }
    ],
    "editor": [
      {
        "family": "Moens",
        "given": "Marie-Francine"
      },
      {
        "family": "Huang",
        "given": "Xuanjing"
      },
      {
        "family": "Specia",
        "given": "Lucia"
      },
      {
        "family": "Yih",
        "given": "Scott Wen-tau"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          11
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://aclanthology.org/2021.emnlp-main.47"
  },
  {
    "id": "eM1kmLmc",
    "type": "book",
    "abstract": "2 v. : 24 cm; Includes bibliographical references and index; v. 1. Basic characterization -- v. 2. Structure prediction",
    "ISBN": "9780387333212",
    "language": "eng",
    "number-of-pages": "424",
    "publisher": "New York, N.Y. : Springer",
    "source": "Internet Archive",
    "title": "Computational methods for protein structure prediction and modeling",
    "URL": "http://archive.org/details/computationalmet0000unse_u4q5",
    "contributor": [
      {
        "literal": "Internet Archive"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2007"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://archive.org/details/computationalmet0000unse_u4q5"
  },
  {
    "id": "jMSKwR4o",
    "type": "webpage",
    "title": "arXiv.org e-Print archive",
    "URL": "https://arxiv.org/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org"
  },
  {
    "id": "12eS31RFj",
    "type": "article",
    "abstract": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.",
    "DOI": "10.48550/arXiv.1312.6114",
    "note": "arXiv:1312.6114 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1312.6114",
    "number": "arXiv:1312.6114",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Auto-Encoding Variational Bayes",
    "URL": "http://arxiv.org/abs/1312.6114",
    "author": [
      {
        "family": "Kingma",
        "given": "Diederik P."
      },
      {
        "family": "Welling",
        "given": "Max"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          12,
          10
        ]
      ]
    }
  },
  {
    "id": "J5FKNc97",
    "type": "article",
    "abstract": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
    "DOI": "10.48550/arXiv.1406.2661",
    "note": "arXiv:1406.2661 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1406.2661",
    "number": "arXiv:1406.2661",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Generative Adversarial Networks",
    "URL": "http://arxiv.org/abs/1406.2661",
    "author": [
      {
        "family": "Goodfellow",
        "given": "Ian J."
      },
      {
        "family": "Pouget-Abadie",
        "given": "Jean"
      },
      {
        "family": "Mirza",
        "given": "Mehdi"
      },
      {
        "family": "Xu",
        "given": "Bing"
      },
      {
        "family": "Warde-Farley",
        "given": "David"
      },
      {
        "family": "Ozair",
        "given": "Sherjil"
      },
      {
        "family": "Courville",
        "given": "Aaron"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2014",
          6,
          10
        ]
      ]
    }
  },
  {
    "id": "7jG4ORkP",
    "type": "article",
    "abstract": "The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.",
    "DOI": "10.48550/arXiv.1511.08458",
    "note": "arXiv:1511.08458 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1511.08458",
    "number": "arXiv:1511.08458",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "An Introduction to Convolutional Neural Networks",
    "URL": "http://arxiv.org/abs/1511.08458",
    "author": [
      {
        "family": "O'Shea",
        "given": "Keiron"
      },
      {
        "family": "Nash",
        "given": "Ryan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2015",
          12,
          2
        ]
      ]
    }
  },
  {
    "id": "gdGFbXoj",
    "type": "article",
    "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
    "DOI": "10.48550/arXiv.1706.03762",
    "note": "arXiv:1706.03762 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1706.03762",
    "number": "arXiv:1706.03762",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Attention Is All You Need",
    "URL": "http://arxiv.org/abs/1706.03762",
    "author": [
      {
        "family": "Vaswani",
        "given": "Ashish"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Parmar",
        "given": "Niki"
      },
      {
        "family": "Uszkoreit",
        "given": "Jakob"
      },
      {
        "family": "Jones",
        "given": "Llion"
      },
      {
        "family": "Gomez",
        "given": "Aidan N."
      },
      {
        "family": "Kaiser",
        "given": "Lukasz"
      },
      {
        "family": "Polosukhin",
        "given": "Illia"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          8,
          1
        ]
      ]
    }
  },
  {
    "id": "1AbLepXdc",
    "type": "article",
    "abstract": "We present a novel method for obtaining high-quality, domain-targeted multiple choice questions from crowd workers. Generating these questions can be difficult without trading away originality, relevance or diversity in the answer options. Our method addresses these problems by leveraging a large corpus of domain-specific text and a small set of existing questions. It produces model suggestions for document selection and answer distractor choice which aid the human question generation process. With this method we have assembled SciQ, a dataset of 13.7K multiple choice science exam questions (Dataset available at http://allenai.org/data.html). We demonstrate that the method produces in-domain questions by providing an analysis of this new dataset and by showing that humans cannot distinguish the crowdsourced questions from original questions. When using SciQ as additional training data to existing questions, we observe accuracy improvements on real science exams.",
    "DOI": "10.48550/arXiv.1707.06209",
    "note": "arXiv:1707.06209 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1707.06209",
    "number": "arXiv:1707.06209",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Crowdsourcing Multiple Choice Science Questions",
    "URL": "http://arxiv.org/abs/1707.06209",
    "author": [
      {
        "family": "Welbl",
        "given": "Johannes"
      },
      {
        "family": "Liu",
        "given": "Nelson F."
      },
      {
        "family": "Gardner",
        "given": "Matt"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2017",
          7,
          19
        ]
      ]
    }
  },
  {
    "id": "cnHJ2ZPM",
    "type": "article",
    "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
    "DOI": "10.48550/arXiv.1810.04805",
    "note": "arXiv:1810.04805 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1810.04805",
    "number": "arXiv:1810.04805",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    "title-short": "BERT",
    "URL": "http://arxiv.org/abs/1810.04805",
    "author": [
      {
        "family": "Devlin",
        "given": "Jacob"
      },
      {
        "family": "Chang",
        "given": "Ming-Wei"
      },
      {
        "family": "Lee",
        "given": "Kenton"
      },
      {
        "family": "Toutanova",
        "given": "Kristina"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          5,
          24
        ]
      ]
    }
  },
  {
    "id": "1EHGNXskO",
    "type": "article",
    "abstract": "We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.",
    "DOI": "10.48550/arXiv.1812.04948",
    "note": "arXiv:1812.04948 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1812.04948",
    "number": "arXiv:1812.04948",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "A Style-Based Generator Architecture for Generative Adversarial Networks",
    "URL": "http://arxiv.org/abs/1812.04948",
    "author": [
      {
        "family": "Karras",
        "given": "Tero"
      },
      {
        "family": "Laine",
        "given": "Samuli"
      },
      {
        "family": "Aila",
        "given": "Timo"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          3,
          29
        ]
      ]
    }
  },
  {
    "id": "11KKjDIVI",
    "type": "article-journal",
    "abstract": "Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.",
    "container-title": "Bioinformatics",
    "DOI": "10.1093/bioinformatics/btz682",
    "ISSN": "1367-4803, 1367-4811",
    "issue": "4",
    "note": "arXiv:1901.08746 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1901.08746",
    "page": "1234-1240",
    "source": "arXiv.org",
    "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
    "title-short": "BioBERT",
    "URL": "http://arxiv.org/abs/1901.08746",
    "volume": "36",
    "author": [
      {
        "family": "Lee",
        "given": "Jinhyuk"
      },
      {
        "family": "Yoon",
        "given": "Wonjin"
      },
      {
        "family": "Kim",
        "given": "Sungdong"
      },
      {
        "family": "Kim",
        "given": "Donghyeon"
      },
      {
        "family": "Kim",
        "given": "Sunkyu"
      },
      {
        "family": "So",
        "given": "Chan Ho"
      },
      {
        "family": "Kang",
        "given": "Jaewoo"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          2,
          15
        ]
      ]
    }
  },
  {
    "id": "eMFlKM40",
    "type": "article",
    "abstract": "Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as \"A woman sits at a piano,\" a machine must select the most likely followup: \"She sets her fingers on the keys.\" With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.",
    "DOI": "10.48550/arXiv.1905.07830",
    "note": "arXiv:1905.07830 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1905.07830",
    "number": "arXiv:1905.07830",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "HellaSwag: Can a Machine Really Finish Your Sentence?",
    "title-short": "HellaSwag",
    "URL": "http://arxiv.org/abs/1905.07830",
    "author": [
      {
        "family": "Zellers",
        "given": "Rowan"
      },
      {
        "family": "Holtzman",
        "given": "Ari"
      },
      {
        "family": "Bisk",
        "given": "Yonatan"
      },
      {
        "family": "Farhadi",
        "given": "Ali"
      },
      {
        "family": "Choi",
        "given": "Yejin"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          5,
          19
        ]
      ]
    }
  },
  {
    "id": "FLyjjIcB",
    "type": "article",
    "abstract": "Synthesis planning is the process of recursively decomposing target molecules into available precursors. Computer-aided retrosynthesis can potentially assist chemists in designing synthetic routes, but at present it is cumbersome and provides results of dissatisfactory quality. In this study, we develop a template-free self-corrected retrosynthesis predictor (SCROP) to perform a retrosynthesis prediction task trained by using the Transformer neural network architecture. In the method, the retrosynthesis planning is converted as a machine translation problem between molecular linear notations of reactants and the products. Coupled with a neural network-based syntax corrector, our method achieves an accuracy of 59.0% on a standard benchmark dataset, which increases >21% over other deep learning methods, and >6% over template-based methods. More importantly, our method shows an accuracy 1.7 times higher than other state-of-the-art methods for compounds not appearing in the training set.",
    "DOI": "10.48550/arXiv.1907.01356",
    "note": "arXiv:1907.01356 [physics]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1907.01356",
    "number": "arXiv:1907.01356",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Predicting Retrosynthetic Reaction using Self-Corrected Transformer Neural Networks",
    "URL": "http://arxiv.org/abs/1907.01356",
    "author": [
      {
        "family": "Zheng",
        "given": "Shuangjia"
      },
      {
        "family": "Rao",
        "given": "Jiahua"
      },
      {
        "family": "Zhang",
        "given": "Zhongyue"
      },
      {
        "family": "Xu",
        "given": "Jun"
      },
      {
        "family": "Yang",
        "given": "Yuedong"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          7,
          2
        ]
      ]
    }
  },
  {
    "id": "C6CjQZug",
    "type": "article",
    "abstract": "We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.",
    "DOI": "10.48550/arXiv.1909.06146",
    "note": "arXiv:1909.06146 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1909.06146",
    "number": "arXiv:1909.06146",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "PubMedQA: A Dataset for Biomedical Research Question Answering",
    "title-short": "PubMedQA",
    "URL": "http://arxiv.org/abs/1909.06146",
    "author": [
      {
        "family": "Jin",
        "given": "Qiao"
      },
      {
        "family": "Dhingra",
        "given": "Bhuwan"
      },
      {
        "family": "Liu",
        "given": "Zhengping"
      },
      {
        "family": "Cohen",
        "given": "William W."
      },
      {
        "family": "Lu",
        "given": "Xinghua"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          13
        ]
      ]
    }
  },
  {
    "id": "1DdwoI1Kr",
    "type": "article",
    "abstract": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.",
    "DOI": "10.48550/arXiv.1910.10683",
    "note": "arXiv:1910.10683 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1910.10683",
    "number": "arXiv:1910.10683",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "URL": "http://arxiv.org/abs/1910.10683",
    "author": [
      {
        "family": "Raffel",
        "given": "Colin"
      },
      {
        "family": "Shazeer",
        "given": "Noam"
      },
      {
        "family": "Roberts",
        "given": "Adam"
      },
      {
        "family": "Lee",
        "given": "Katherine"
      },
      {
        "family": "Narang",
        "given": "Sharan"
      },
      {
        "family": "Matena",
        "given": "Michael"
      },
      {
        "family": "Zhou",
        "given": "Yanqi"
      },
      {
        "family": "Li",
        "given": "Wei"
      },
      {
        "family": "Liu",
        "given": "Peter J."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          9,
          19
        ]
      ]
    }
  },
  {
    "id": "To07TIhw",
    "type": "article",
    "abstract": "State-of-the-art solutions in the areas of \"Language Modelling & Generating Text\", \"Speech Recognition\", \"Generating Image Descriptions\" or \"Video Tagging\" have been using Recurrent Neural Networks as the foundation for their approaches. Understanding the underlying concepts is therefore of tremendous importance if we want to keep up with recent or upcoming publications in those areas. In this work we give a short overview over some of the most important concepts in the realm of Recurrent Neural Networks which enables readers to easily understand the fundamentals such as but not limited to \"Backpropagation through Time\" or \"Long Short-Term Memory Units\" as well as some of the more recent advances like the \"Attention Mechanism\" or \"Pointer Networks\". We also give recommendations for further reading regarding more complex topics where it is necessary.",
    "DOI": "10.48550/arXiv.1912.05911",
    "note": "arXiv:1912.05911 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/1912.05911",
    "number": "arXiv:1912.05911",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Recurrent Neural Networks (RNNs): A gentle Introduction and Overview",
    "title-short": "Recurrent Neural Networks (RNNs)",
    "URL": "http://arxiv.org/abs/1912.05911",
    "author": [
      {
        "family": "Schmidt",
        "given": "Robin M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          11,
          23
        ]
      ]
    }
  },
  {
    "id": "LoYLzWSf",
    "type": "article",
    "abstract": "An autoencoder is a specific type of a neural network, which is mainly designed to encode the input into a compressed and meaningful representation, and then decode it back such that the reconstructed input is similar as possible to the original one. This chapter surveys the different types of autoencoders that are mainly used today. It also describes various applications and use-cases of autoencoders.",
    "DOI": "10.48550/arXiv.2003.05991",
    "note": "arXiv:2003.05991 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2003.05991",
    "number": "arXiv:2003.05991",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Autoencoders",
    "URL": "http://arxiv.org/abs/2003.05991",
    "author": [
      {
        "family": "Bank",
        "given": "Dor"
      },
      {
        "family": "Koenigstein",
        "given": "Noam"
      },
      {
        "family": "Giryes",
        "given": "Raja"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          4,
          3
        ]
      ]
    }
  },
  {
    "id": "1EwWbaeg2",
    "type": "article",
    "abstract": "Generative modeling for protein engineering is key to solving fundamental problems in synthetic biology, medicine, and material science. We pose protein engineering as an unsupervised sequence generation problem in order to leverage the exponentially growing set of proteins that lack costly, structural annotations. We train a 1.2B-parameter language model, ProGen, on ~280M protein sequences conditioned on taxonomic and keyword tags such as molecular function and cellular component. This provides ProGen with an unprecedented range of evolutionary sequence diversity and allows it to generate with fine-grained control as demonstrated by metrics based on primary sequence similarity, secondary structure accuracy, and conformational energy.",
    "DOI": "10.48550/arXiv.2004.03497",
    "note": "arXiv:2004.03497 [cs, q-bio, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2004.03497",
    "number": "arXiv:2004.03497",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "ProGen: Language Modeling for Protein Generation",
    "title-short": "ProGen",
    "URL": "http://arxiv.org/abs/2004.03497",
    "author": [
      {
        "family": "Madani",
        "given": "Ali"
      },
      {
        "family": "McCann",
        "given": "Bryan"
      },
      {
        "family": "Naik",
        "given": "Nikhil"
      },
      {
        "family": "Keskar",
        "given": "Nitish Shirish"
      },
      {
        "family": "Anand",
        "given": "Namrata"
      },
      {
        "family": "Eguchi",
        "given": "Raphael R."
      },
      {
        "family": "Huang",
        "given": "Po-Ssu"
      },
      {
        "family": "Socher",
        "given": "Richard"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          3,
          7
        ]
      ]
    }
  },
  {
    "id": "ihwUK6Sk",
    "type": "article",
    "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
    "DOI": "10.48550/arXiv.2005.14165",
    "note": "arXiv:2005.14165 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2005.14165",
    "number": "arXiv:2005.14165",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Language Models are Few-Shot Learners",
    "URL": "http://arxiv.org/abs/2005.14165",
    "author": [
      {
        "family": "Brown",
        "given": "Tom B."
      },
      {
        "family": "Mann",
        "given": "Benjamin"
      },
      {
        "family": "Ryder",
        "given": "Nick"
      },
      {
        "family": "Subbiah",
        "given": "Melanie"
      },
      {
        "family": "Kaplan",
        "given": "Jared"
      },
      {
        "family": "Dhariwal",
        "given": "Prafulla"
      },
      {
        "family": "Neelakantan",
        "given": "Arvind"
      },
      {
        "family": "Shyam",
        "given": "Pranav"
      },
      {
        "family": "Sastry",
        "given": "Girish"
      },
      {
        "family": "Askell",
        "given": "Amanda"
      },
      {
        "family": "Agarwal",
        "given": "Sandhini"
      },
      {
        "family": "Herbert-Voss",
        "given": "Ariel"
      },
      {
        "family": "Krueger",
        "given": "Gretchen"
      },
      {
        "family": "Henighan",
        "given": "Tom"
      },
      {
        "family": "Child",
        "given": "Rewon"
      },
      {
        "family": "Ramesh",
        "given": "Aditya"
      },
      {
        "family": "Ziegler",
        "given": "Daniel M."
      },
      {
        "family": "Wu",
        "given": "Jeffrey"
      },
      {
        "family": "Winter",
        "given": "Clemens"
      },
      {
        "family": "Hesse",
        "given": "Christopher"
      },
      {
        "family": "Chen",
        "given": "Mark"
      },
      {
        "family": "Sigler",
        "given": "Eric"
      },
      {
        "family": "Litwin",
        "given": "Mateusz"
      },
      {
        "family": "Gray",
        "given": "Scott"
      },
      {
        "family": "Chess",
        "given": "Benjamin"
      },
      {
        "family": "Clark",
        "given": "Jack"
      },
      {
        "family": "Berner",
        "given": "Christopher"
      },
      {
        "family": "McCandlish",
        "given": "Sam"
      },
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Sutskever",
        "given": "Ilya"
      },
      {
        "family": "Amodei",
        "given": "Dario"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          7,
          22
        ]
      ]
    }
  },
  {
    "id": "tzRTBD2w",
    "type": "article",
    "abstract": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion",
    "DOI": "10.48550/arXiv.2006.11239",
    "note": "arXiv:2006.11239 [cs, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2006.11239",
    "number": "arXiv:2006.11239",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Denoising Diffusion Probabilistic Models",
    "URL": "http://arxiv.org/abs/2006.11239",
    "author": [
      {
        "family": "Ho",
        "given": "Jonathan"
      },
      {
        "family": "Jain",
        "given": "Ajay"
      },
      {
        "family": "Abbeel",
        "given": "Pieter"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          12,
          16
        ]
      ]
    }
  },
  {
    "id": "1E2ZqNklN",
    "type": "article-journal",
    "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding & Reasoning Benchmark) at https://aka.ms/BLURB.",
    "container-title": "ACM Transactions on Computing for Healthcare",
    "DOI": "10.1145/3458754",
    "ISSN": "2691-1957, 2637-8051",
    "issue": "1",
    "journalAbbreviation": "ACM Trans. Comput. Healthcare",
    "note": "arXiv:2007.15779 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2007.15779",
    "page": "1-23",
    "source": "arXiv.org",
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "URL": "http://arxiv.org/abs/2007.15779",
    "volume": "3",
    "author": [
      {
        "family": "Gu",
        "given": "Yu"
      },
      {
        "family": "Tinn",
        "given": "Robert"
      },
      {
        "family": "Cheng",
        "given": "Hao"
      },
      {
        "family": "Lucas",
        "given": "Michael"
      },
      {
        "family": "Usuyama",
        "given": "Naoto"
      },
      {
        "family": "Liu",
        "given": "Xiaodong"
      },
      {
        "family": "Naumann",
        "given": "Tristan"
      },
      {
        "family": "Gao",
        "given": "Jianfeng"
      },
      {
        "family": "Poon",
        "given": "Hoifung"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          1,
          31
        ]
      ]
    }
  },
  {
    "id": "1DdXyopJh",
    "type": "article",
    "abstract": "We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.",
    "DOI": "10.48550/arXiv.2009.03300",
    "note": "arXiv:2009.03300 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2009.03300",
    "number": "arXiv:2009.03300",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Measuring Massive Multitask Language Understanding",
    "URL": "http://arxiv.org/abs/2009.03300",
    "author": [
      {
        "family": "Hendrycks",
        "given": "Dan"
      },
      {
        "family": "Burns",
        "given": "Collin"
      },
      {
        "family": "Basart",
        "given": "Steven"
      },
      {
        "family": "Zou",
        "given": "Andy"
      },
      {
        "family": "Mazeika",
        "given": "Mantas"
      },
      {
        "family": "Song",
        "given": "Dawn"
      },
      {
        "family": "Steinhardt",
        "given": "Jacob"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          1,
          12
        ]
      ]
    }
  },
  {
    "id": "WWUD4rgh",
    "type": "article",
    "abstract": "There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of named entity recognition, relation extraction, and question answering. Model checkpoints and code are available at [https://ngc.nvidia.com] and [https://github.com/NVIDIA/NeMo].",
    "DOI": "10.48550/arXiv.2010.06060",
    "note": "arXiv:2010.06060 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2010.06060",
    "number": "arXiv:2010.06060",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "BioMegatron: Larger Biomedical Domain Language Model",
    "title-short": "BioMegatron",
    "URL": "http://arxiv.org/abs/2010.06060",
    "author": [
      {
        "family": "Shin",
        "given": "Hoo-Chang"
      },
      {
        "family": "Zhang",
        "given": "Yang"
      },
      {
        "family": "Bakhturina",
        "given": "Evelina"
      },
      {
        "family": "Puri",
        "given": "Raul"
      },
      {
        "family": "Patwary",
        "given": "Mostofa"
      },
      {
        "family": "Shoeybi",
        "given": "Mohammad"
      },
      {
        "family": "Mani",
        "given": "Raghav"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          10,
          13
        ]
      ]
    }
  },
  {
    "id": "5ndwLMAS",
    "type": "article",
    "abstract": "Designing novel protein sequences for a desired 3D topological fold is a fundamental yet non-trivial task in protein engineering. Challenges exist due to the complex sequence--fold relationship, as well as the difficulties to capture the diversity of the sequences (therefore structures and functions) within a fold. To overcome these challenges, we propose Fold2Seq, a novel transformer-based generative framework for designing protein sequences conditioned on a specific target fold. To model the complex sequence--structure relationship, Fold2Seq jointly learns a sequence embedding using a transformer and a fold embedding from the density of secondary structural elements in 3D voxels. On test sets with single, high-resolution and complete structure inputs for individual folds, our experiments demonstrate improved or comparable performance of Fold2Seq in terms of speed, coverage, and reliability for sequence design, when compared to existing state-of-the-art methods that include data-driven deep generative models and physics-based RosettaDesign. The unique advantages of fold-based Fold2Seq, in comparison to a structure-based deep model and RosettaDesign, become more evident on three additional real-world challenges originating from low-quality, incomplete, or ambiguous input structures. Source code and data are available at https://github.com/IBM/fold2seq.",
    "DOI": "10.48550/arXiv.2106.13058",
    "note": "arXiv:2106.13058 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2106.13058",
    "number": "arXiv:2106.13058",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design",
    "title-short": "Fold2Seq",
    "URL": "http://arxiv.org/abs/2106.13058",
    "author": [
      {
        "family": "Cao",
        "given": "Yue"
      },
      {
        "family": "Das",
        "given": "Payel"
      },
      {
        "family": "Chenthamarakshan",
        "given": "Vijil"
      },
      {
        "family": "Chen",
        "given": "Pin-Yu"
      },
      {
        "family": "Melnyk",
        "given": "Igor"
      },
      {
        "family": "Shen",
        "given": "Yang"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          6,
          24
        ]
      ]
    }
  },
  {
    "id": "z4KRV9wF",
    "type": "article",
    "abstract": "We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",
    "DOI": "10.48550/arXiv.2109.07958",
    "note": "arXiv:2109.07958 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2109.07958",
    "number": "arXiv:2109.07958",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
    "title-short": "TruthfulQA",
    "URL": "http://arxiv.org/abs/2109.07958",
    "author": [
      {
        "family": "Lin",
        "given": "Stephanie"
      },
      {
        "family": "Hilton",
        "given": "Jacob"
      },
      {
        "family": "Evans",
        "given": "Owain"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          5,
          7
        ]
      ]
    }
  },
  {
    "id": "ws5eobRv",
    "type": "article",
    "abstract": "Understanding protein sequences is vital and urgent for biology, healthcare, and medicine. Labeling approaches are expensive yet time-consuming, while the amount of unlabeled data is increasing quite faster than that of the labeled data due to low-cost, high-throughput sequencing methods. In order to extract knowledge from these unlabeled data, representation learning is of significant value for protein-related tasks and has great potential for helping us learn more about protein functions and structures. The key problem in the protein sequence representation learning is to capture the co-evolutionary information reflected by the inter-residue co-variation in the sequences. Instead of leveraging multiple sequence alignment as is usually done, we propose a novel method to capture this information directly by pre-training via a dedicated language model, i.e., Pairwise Masked Language Model (PMLM). In a conventional masked language model, the masked tokens are modeled by conditioning on the unmasked tokens only, but processed independently to each other. However, our proposed PMLM takes the dependency among masked tokens into consideration, i.e., the probability of a token pair is not equal to the product of the probability of the two tokens. By applying this model, the pre-trained encoder is able to generate a better representation for protein sequences. Our result shows that the proposed method can effectively capture the inter-residue correlations and improves the performance of contact prediction by up to 9% compared to the MLM baseline under the same setting. The proposed model also significantly outperforms the MSA baseline by more than 7% on the TAPE contact prediction benchmark when pre-trained on a subset of the sequence database which the MSA is generated from, revealing the potential of the sequence pre-training method to surpass MSA based methods in general.",
    "DOI": "10.48550/arXiv.2110.15527",
    "note": "arXiv:2110.15527 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2110.15527",
    "number": "arXiv:2110.15527",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Pre-training Co-evolutionary Protein Representation via A Pairwise Masked Language Model",
    "URL": "http://arxiv.org/abs/2110.15527",
    "author": [
      {
        "family": "He",
        "given": "Liang"
      },
      {
        "family": "Zhang",
        "given": "Shizhuo"
      },
      {
        "family": "Wu",
        "given": "Lijun"
      },
      {
        "family": "Xia",
        "given": "Huanhuan"
      },
      {
        "family": "Ju",
        "given": "Fusong"
      },
      {
        "family": "Zhang",
        "given": "He"
      },
      {
        "family": "Liu",
        "given": "Siyuan"
      },
      {
        "family": "Xia",
        "given": "Yingce"
      },
      {
        "family": "Zhu",
        "given": "Jianwei"
      },
      {
        "family": "Deng",
        "given": "Pan"
      },
      {
        "family": "Shao",
        "given": "Bin"
      },
      {
        "family": "Qin",
        "given": "Tao"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          10,
          29
        ]
      ]
    }
  },
  {
    "id": "emwEKYnI",
    "type": "article",
    "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .",
    "DOI": "10.48550/arXiv.2112.10752",
    "note": "arXiv:2112.10752 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2112.10752",
    "number": "arXiv:2112.10752",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
    "URL": "http://arxiv.org/abs/2112.10752",
    "author": [
      {
        "family": "Rombach",
        "given": "Robin"
      },
      {
        "family": "Blattmann",
        "given": "Andreas"
      },
      {
        "family": "Lorenz",
        "given": "Dominik"
      },
      {
        "family": "Esser",
        "given": "Patrick"
      },
      {
        "family": "Ommer",
        "given": "Björn"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          4,
          13
        ]
      ]
    }
  },
  {
    "id": "7oKEC5H3",
    "type": "article",
    "abstract": "Language model (LM) pretraining can learn various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data at https://github.com/michiyasunaga/LinkBERT.",
    "DOI": "10.48550/arXiv.2203.15827",
    "note": "arXiv:2203.15827 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2203.15827",
    "number": "arXiv:2203.15827",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "LinkBERT: Pretraining Language Models with Document Links",
    "title-short": "LinkBERT",
    "URL": "http://arxiv.org/abs/2203.15827",
    "author": [
      {
        "family": "Yasunaga",
        "given": "Michihiro"
      },
      {
        "family": "Leskovec",
        "given": "Jure"
      },
      {
        "family": "Liang",
        "given": "Percy"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          3,
          29
        ]
      ]
    }
  },
  {
    "id": "o5h39u4V",
    "type": "article",
    "abstract": "In this work we introduce RITA: a suite of autoregressive generative models for protein sequences, with up to 1.2 billion parameters, trained on over 280 million protein sequences belonging to the UniRef-100 database. Such generative models hold the promise of greatly accelerating protein design. We conduct the first systematic study of how capabilities evolve with model size for autoregressive transformers in the protein domain: we evaluate RITA models in next amino acid prediction, zero-shot fitness, and enzyme function prediction, showing benefits from increased scale. We release the RITA models openly, to the benefit of the research community.",
    "DOI": "10.48550/arXiv.2205.05789",
    "note": "arXiv:2205.05789 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2205.05789",
    "number": "arXiv:2205.05789",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "RITA: a Study on Scaling Up Generative Protein Sequence Models",
    "title-short": "RITA",
    "URL": "http://arxiv.org/abs/2205.05789",
    "author": [
      {
        "family": "Hesslow",
        "given": "Daniel"
      },
      {
        "family": "Zanichelli",
        "given": "Niccoló"
      },
      {
        "family": "Notin",
        "given": "Pascal"
      },
      {
        "family": "Poli",
        "given": "Iacopo"
      },
      {
        "family": "Marks",
        "given": "Debora"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          7,
          14
        ]
      ]
    }
  },
  {
    "id": "NNPUXrCp",
    "type": "article",
    "abstract": "Attention-based models trained on protein sequences have demonstrated incredible success at classification and generation tasks relevant for artificial intelligence-driven protein design. However, we lack a sufficient understanding of how very large-scale models and data play a role in effective protein model development. We introduce a suite of protein language models, named ProGen2, that are scaled up to 6.4B parameters and trained on different sequence datasets drawn from over a billion proteins from genomic, metagenomic, and immune repertoire databases. ProGen2 models show state-of-the-art performance in capturing the distribution of observed evolutionary sequences, generating novel viable sequences, and predicting protein fitness without additional finetuning. As large model sizes and raw numbers of protein sequences continue to become more widely accessible, our results suggest that a growing emphasis needs to be placed on the data distribution provided to a protein sequence model. We release the ProGen2 models and code at https://github.com/salesforce/progen.",
    "DOI": "10.48550/arXiv.2206.13517",
    "note": "arXiv:2206.13517 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2206.13517",
    "number": "arXiv:2206.13517",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "ProGen2: Exploring the Boundaries of Protein Language Models",
    "title-short": "ProGen2",
    "URL": "http://arxiv.org/abs/2206.13517",
    "author": [
      {
        "family": "Nijkamp",
        "given": "Erik"
      },
      {
        "family": "Ruffolo",
        "given": "Jeffrey"
      },
      {
        "family": "Weinstein",
        "given": "Eli N."
      },
      {
        "family": "Naik",
        "given": "Nikhil"
      },
      {
        "family": "Madani",
        "given": "Ali"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          6,
          27
        ]
      ]
    }
  },
  {
    "id": "17n2DOU6H",
    "type": "article-journal",
    "abstract": "Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e., BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large scale biomedical literature. We evaluate BioGPT on six biomedical NLP tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms. Code is available at https://github.com/microsoft/BioGPT.",
    "container-title": "Briefings in Bioinformatics",
    "DOI": "10.1093/bib/bbac409",
    "ISSN": "1467-5463, 1477-4054",
    "issue": "6",
    "note": "arXiv:2210.10341 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2210.10341",
    "page": "bbac409",
    "source": "arXiv.org",
    "title": "BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining",
    "title-short": "BioGPT",
    "URL": "http://arxiv.org/abs/2210.10341",
    "volume": "23",
    "author": [
      {
        "family": "Luo",
        "given": "Renqian"
      },
      {
        "family": "Sun",
        "given": "Liai"
      },
      {
        "family": "Xia",
        "given": "Yingce"
      },
      {
        "family": "Qin",
        "given": "Tao"
      },
      {
        "family": "Zhang",
        "given": "Sheng"
      },
      {
        "family": "Poon",
        "given": "Hoifung"
      },
      {
        "family": "Liu",
        "given": "Tie-Yan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          11,
          19
        ]
      ]
    }
  },
  {
    "id": "vPDj8bGo",
    "type": "article",
    "abstract": "There is increasing adoption of artificial intelligence in drug discovery. However, existing studies use machine learning to mainly utilize the chemical structures of molecules but ignore the vast textual knowledge available in chemistry. Incorporating textual knowledge enables us to realize new drug design objectives, adapt to text-based instructions and predict complex biological activities. Here we present a multi-modal molecule structure-text model, MoleculeSTM, by jointly learning molecules' chemical structures and textual descriptions via a contrastive learning strategy. To train MoleculeSTM, we construct a large multi-modal dataset, namely, PubChemSTM, with over 280,000 chemical structure-text pairs. To demonstrate the effectiveness and utility of MoleculeSTM, we design two challenging zero-shot tasks based on text instructions, including structure-text retrieval and molecule editing. MoleculeSTM has two main properties: open vocabulary and compositionality via natural language. In experiments, MoleculeSTM obtains the state-of-the-art generalization ability to novel biochemical concepts across various benchmarks.",
    "DOI": "10.48550/arXiv.2212.10789",
    "note": "arXiv:2212.10789 [cs, q-bio, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2212.10789",
    "number": "arXiv:2212.10789",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing",
    "URL": "http://arxiv.org/abs/2212.10789",
    "author": [
      {
        "family": "Liu",
        "given": "Shengchao"
      },
      {
        "family": "Nie",
        "given": "Weili"
      },
      {
        "family": "Wang",
        "given": "Chengpeng"
      },
      {
        "family": "Lu",
        "given": "Jiarui"
      },
      {
        "family": "Qiao",
        "given": "Zhuoran"
      },
      {
        "family": "Liu",
        "given": "Ling"
      },
      {
        "family": "Tang",
        "given": "Jian"
      },
      {
        "family": "Xiao",
        "given": "Chaowei"
      },
      {
        "family": "Anandkumar",
        "given": "Anima"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          1,
          29
        ]
      ]
    }
  },
  {
    "id": "1DrOhLdUs",
    "type": "article",
    "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, but the quality bar for medical and clinical applications is high. Today, attempts to assess models' clinical knowledge typically rely on automated evaluations on limited benchmarks. There is no standard to evaluate model predictions and reasoning across a breadth of tasks. To address this, we present MultiMedQA, a benchmark combining six existing open question answering datasets spanning professional medical exams, research, and consumer queries; and HealthSearchQA, a new free-response dataset of medical questions searched online. We propose a framework for human evaluation of model answers along multiple axes including factuality, precision, possible harm, and bias. In addition, we evaluate PaLM (a 540-billion parameter LLM) and its instruction-tuned variant, Flan-PaLM, on MultiMedQA. Using a combination of prompting strategies, Flan-PaLM achieves state-of-the-art accuracy on every MultiMedQA multiple-choice dataset (MedQA, MedMCQA, PubMedQA, MMLU clinical topics), including 67.6% accuracy on MedQA (US Medical License Exam questions), surpassing prior state-of-the-art by over 17%. However, human evaluation reveals key gaps in Flan-PaLM responses. To resolve this we introduce instruction prompt tuning, a parameter-efficient approach for aligning LLMs to new domains using a few exemplars. The resulting model, Med-PaLM, performs encouragingly, but remains inferior to clinicians. We show that comprehension, recall of knowledge, and medical reasoning improve with model scale and instruction prompt tuning, suggesting the potential utility of LLMs in medicine. Our human evaluations reveal important limitations of today's models, reinforcing the importance of both evaluation frameworks and method development in creating safe, helpful LLM models for clinical applications.",
    "DOI": "10.48550/arXiv.2212.13138",
    "note": "arXiv:2212.13138 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2212.13138",
    "number": "arXiv:2212.13138",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Large Language Models Encode Clinical Knowledge",
    "URL": "http://arxiv.org/abs/2212.13138",
    "author": [
      {
        "family": "Singhal",
        "given": "Karan"
      },
      {
        "family": "Azizi",
        "given": "Shekoofeh"
      },
      {
        "family": "Tu",
        "given": "Tao"
      },
      {
        "family": "Mahdavi",
        "given": "S. Sara"
      },
      {
        "family": "Wei",
        "given": "Jason"
      },
      {
        "family": "Chung",
        "given": "Hyung Won"
      },
      {
        "family": "Scales",
        "given": "Nathan"
      },
      {
        "family": "Tanwani",
        "given": "Ajay"
      },
      {
        "family": "Cole-Lewis",
        "given": "Heather"
      },
      {
        "family": "Pfohl",
        "given": "Stephen"
      },
      {
        "family": "Payne",
        "given": "Perry"
      },
      {
        "family": "Seneviratne",
        "given": "Martin"
      },
      {
        "family": "Gamble",
        "given": "Paul"
      },
      {
        "family": "Kelly",
        "given": "Chris"
      },
      {
        "family": "Scharli",
        "given": "Nathaneal"
      },
      {
        "family": "Chowdhery",
        "given": "Aakanksha"
      },
      {
        "family": "Mansfield",
        "given": "Philip"
      },
      {
        "family": "Arcas",
        "given": "Blaise Aguera",
        "dropping-particle": "y"
      },
      {
        "family": "Webster",
        "given": "Dale"
      },
      {
        "family": "Corrado",
        "given": "Greg S."
      },
      {
        "family": "Matias",
        "given": "Yossi"
      },
      {
        "family": "Chou",
        "given": "Katherine"
      },
      {
        "family": "Gottweis",
        "given": "Juraj"
      },
      {
        "family": "Tomasev",
        "given": "Nenad"
      },
      {
        "family": "Liu",
        "given": "Yun"
      },
      {
        "family": "Rajkomar",
        "given": "Alvin"
      },
      {
        "family": "Barral",
        "given": "Joelle"
      },
      {
        "family": "Semturs",
        "given": "Christopher"
      },
      {
        "family": "Karthikesalingam",
        "given": "Alan"
      },
      {
        "family": "Natarajan",
        "given": "Vivek"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          12,
          26
        ]
      ]
    }
  },
  {
    "id": "1HFrE3NU1",
    "type": "article",
    "abstract": "Current protein language models (pLMs) predominantly focus on single-chain protein sequences and often have not accounted for constraints on generative design imposed by protein-protein interactions. To address this gap, we present paired Antibody T5 (pAbT5), an encoder-decoder model to generate complementary heavy or light chain from its pairing partner. We show that our model respects conservation in framework regions and variability in hypervariable domains, demonstrated by agreement with sequence alignment and variable-length CDR loops. We also show that our model captures chain pairing preferences through the recovery of ground-truth chain type and gene families. Our results showcase the potential of pAbT5 in generative antibody design, incorporating biological constraints from chain pairing preferences.",
    "DOI": "10.48550/arXiv.2301.02748",
    "note": "arXiv:2301.02748 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2301.02748",
    "number": "arXiv:2301.02748",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Generative Antibody Design for Complementary Chain Pairing Sequences through Encoder-Decoder Language Model",
    "URL": "http://arxiv.org/abs/2301.02748",
    "author": [
      {
        "family": "Chu",
        "given": "Simon K. S."
      },
      {
        "family": "Wei",
        "given": "Kathy Y."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          11,
          20
        ]
      ]
    }
  },
  {
    "id": "zATp0wNO",
    "type": "article",
    "abstract": "As opposed to scaling-up protein language models (PLMs), we seek improving performance via protein-specific optimization. Although the proportionality between the language model size and the richness of its learned representations is validated, we prioritize accessibility and pursue a path of data-efficient, cost-reduced, and knowledge-guided optimization. Through over twenty experiments ranging from masking, architecture, and pre-training data, we derive insights from protein-specific experimentation into building a model that interprets the language of life, optimally. We present Ankh, the first general-purpose PLM trained on Google's TPU-v4 surpassing the state-of-the-art performance with fewer parameters (<10% for pre-training, <7% for inference, and <30% for the embedding dimension). We provide a representative range of structure and function benchmarks where Ankh excels. We further provide a protein variant generation analysis on High-N and One-N input data scales where Ankh succeeds in learning protein evolutionary conservation-mutation trends and introducing functional diversity while retaining key structural-functional characteristics. We dedicate our work to promoting accessibility to research innovation via attainable resources.",
    "DOI": "10.48550/arXiv.2301.06568",
    "note": "arXiv:2301.06568 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2301.06568",
    "number": "arXiv:2301.06568",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling",
    "title-short": "Ankh",
    "URL": "http://arxiv.org/abs/2301.06568",
    "author": [
      {
        "family": "Elnaggar",
        "given": "Ahmed"
      },
      {
        "family": "Essam",
        "given": "Hazem"
      },
      {
        "family": "Salah-Eldin",
        "given": "Wafaa"
      },
      {
        "family": "Moustafa",
        "given": "Walid"
      },
      {
        "family": "Elkerdawy",
        "given": "Mohamed"
      },
      {
        "family": "Rochereau",
        "given": "Charlotte"
      },
      {
        "family": "Rost",
        "given": "Burkhard"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          1,
          16
        ]
      ]
    }
  },
  {
    "id": "m1jSSz9o",
    "type": "article",
    "abstract": "Current protein language models (PLMs) learn protein representations mainly based on their sequences, thereby well capturing co-evolutionary information, but they are unable to explicitly acquire protein functions, which is the end goal of protein representation learning. Fortunately, for many proteins, their textual property descriptions are available, where their various functions are also described. Motivated by this fact, we first build the ProtDescribe dataset to augment protein sequences with text descriptions of their functions and other important properties. Based on this dataset, we propose the ProtST framework to enhance Protein Sequence pre-training and understanding by biomedical Texts. During pre-training, we design three types of tasks, i.e., unimodal mask prediction, multimodal representation alignment and multimodal mask prediction, to enhance a PLM with protein property information with different granularities and, at the same time, preserve the PLM's original representation power. On downstream tasks, ProtST enables both supervised learning and zero-shot prediction. We verify the superiority of ProtST-induced PLMs over previous ones on diverse representation learning benchmarks. Under the zero-shot setting, we show the effectiveness of ProtST on zero-shot protein classification, and ProtST also enables functional protein retrieval from a large-scale database without any function annotation.",
    "DOI": "10.48550/arXiv.2301.12040",
    "note": "arXiv:2301.12040 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2301.12040",
    "number": "arXiv:2301.12040",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts",
    "title-short": "ProtST",
    "URL": "http://arxiv.org/abs/2301.12040",
    "author": [
      {
        "family": "Xu",
        "given": "Minghao"
      },
      {
        "family": "Yuan",
        "given": "Xinyu"
      },
      {
        "family": "Miret",
        "given": "Santiago"
      },
      {
        "family": "Tang",
        "given": "Jian"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          7,
          4
        ]
      ]
    }
  },
  {
    "id": "R8VlFsXM",
    "type": "article",
    "abstract": "This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to up to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >60% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins)",
    "DOI": "10.48550/arXiv.2302.01649",
    "note": "arXiv:2302.01649 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2302.01649",
    "number": "arXiv:2302.01649",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Structure-informed Language Models Are Protein Designers",
    "URL": "http://arxiv.org/abs/2302.01649",
    "author": [
      {
        "family": "Zheng",
        "given": "Zaixiang"
      },
      {
        "family": "Deng",
        "given": "Yifan"
      },
      {
        "family": "Xue",
        "given": "Dongyu"
      },
      {
        "family": "Zhou",
        "given": "Yi"
      },
      {
        "family": "YE",
        "given": "Fei"
      },
      {
        "family": "Gu",
        "given": "Quanquan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          9
        ]
      ]
    }
  },
  {
    "id": "tP9JVlEF",
    "type": "article",
    "abstract": "Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins' high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90\\% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.",
    "DOI": "10.48550/arXiv.2302.04611",
    "note": "arXiv:2302.04611 [cs, q-bio, stat]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2302.04611",
    "number": "arXiv:2302.04611",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "A Text-guided Protein Design Framework",
    "URL": "http://arxiv.org/abs/2302.04611",
    "author": [
      {
        "family": "Liu",
        "given": "Shengchao"
      },
      {
        "family": "Li",
        "given": "Yanjing"
      },
      {
        "family": "Li",
        "given": "Zhuoxinran"
      },
      {
        "family": "Gitter",
        "given": "Anthony"
      },
      {
        "family": "Zhu",
        "given": "Yutao"
      },
      {
        "family": "Lu",
        "given": "Jiarui"
      },
      {
        "family": "Xu",
        "given": "Zhao"
      },
      {
        "family": "Nie",
        "given": "Weili"
      },
      {
        "family": "Ramanathan",
        "given": "Arvind"
      },
      {
        "family": "Xiao",
        "given": "Chaowei"
      },
      {
        "family": "Tang",
        "given": "Jian"
      },
      {
        "family": "Guo",
        "given": "Hongyu"
      },
      {
        "family": "Anandkumar",
        "given": "Anima"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          12,
          3
        ]
      ]
    }
  },
  {
    "id": "ZUDBmsmY",
    "type": "article",
    "abstract": "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",
    "DOI": "10.48550/arXiv.2302.13971",
    "note": "arXiv:2302.13971 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2302.13971",
    "number": "arXiv:2302.13971",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "LLaMA: Open and Efficient Foundation Language Models",
    "title-short": "LLaMA",
    "URL": "http://arxiv.org/abs/2302.13971",
    "author": [
      {
        "family": "Touvron",
        "given": "Hugo"
      },
      {
        "family": "Lavril",
        "given": "Thibaut"
      },
      {
        "family": "Izacard",
        "given": "Gautier"
      },
      {
        "family": "Martinet",
        "given": "Xavier"
      },
      {
        "family": "Lachaux",
        "given": "Marie-Anne"
      },
      {
        "family": "Lacroix",
        "given": "Timothée"
      },
      {
        "family": "Rozière",
        "given": "Baptiste"
      },
      {
        "family": "Goyal",
        "given": "Naman"
      },
      {
        "family": "Hambro",
        "given": "Eric"
      },
      {
        "family": "Azhar",
        "given": "Faisal"
      },
      {
        "family": "Rodriguez",
        "given": "Aurelien"
      },
      {
        "family": "Joulin",
        "given": "Armand"
      },
      {
        "family": "Grave",
        "given": "Edouard"
      },
      {
        "family": "Lample",
        "given": "Guillaume"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          2,
          27
        ]
      ]
    }
  },
  {
    "id": "11eq54zNQ",
    "type": "article",
    "abstract": "New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",
    "DOI": "10.48550/arXiv.2305.08322",
    "note": "arXiv:2305.08322 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2305.08322",
    "number": "arXiv:2305.08322",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models",
    "title-short": "C-Eval",
    "URL": "http://arxiv.org/abs/2305.08322",
    "author": [
      {
        "family": "Huang",
        "given": "Yuzhen"
      },
      {
        "family": "Bai",
        "given": "Yuzhuo"
      },
      {
        "family": "Zhu",
        "given": "Zhihao"
      },
      {
        "family": "Zhang",
        "given": "Junlei"
      },
      {
        "family": "Zhang",
        "given": "Jinghan"
      },
      {
        "family": "Su",
        "given": "Tangjun"
      },
      {
        "family": "Liu",
        "given": "Junteng"
      },
      {
        "family": "Lv",
        "given": "Chuancheng"
      },
      {
        "family": "Zhang",
        "given": "Yikai"
      },
      {
        "family": "Lei",
        "given": "Jiayi"
      },
      {
        "family": "Fu",
        "given": "Yao"
      },
      {
        "family": "Sun",
        "given": "Maosong"
      },
      {
        "family": "He",
        "given": "Junxian"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          11,
          6
        ]
      ]
    }
  },
  {
    "id": "nIxULWpe",
    "type": "article",
    "abstract": "Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using LLMs in BioNLP applications. We make the datasets, baselines, and results publicly available to the community via https://github.com/qingyu-qc/gpt_bionlp_benchmark.",
    "DOI": "10.48550/arXiv.2305.16326",
    "note": "arXiv:2305.16326 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2305.16326",
    "number": "arXiv:2305.16326",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations",
    "title-short": "Large language models in biomedical natural language processing",
    "URL": "http://arxiv.org/abs/2305.16326",
    "author": [
      {
        "family": "Chen",
        "given": "Qingyu"
      },
      {
        "family": "Du",
        "given": "Jingcheng"
      },
      {
        "family": "Hu",
        "given": "Yan"
      },
      {
        "family": "Keloth",
        "given": "Vipina Kuttichi"
      },
      {
        "family": "Peng",
        "given": "Xueqing"
      },
      {
        "family": "Raja",
        "given": "Kalpana"
      },
      {
        "family": "Zhang",
        "given": "Rui"
      },
      {
        "family": "Lu",
        "given": "Zhiyong"
      },
      {
        "family": "Xu",
        "given": "Hua"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          1,
          20
        ]
      ]
    }
  },
  {
    "id": "10tIBLTT8",
    "type": "article",
    "abstract": "Generative protein language models are a natural way to design new proteins with desired functions. However, current models are either difficult to direct to produce a protein from a specific family of interest, or must be trained on a large multiple sequence alignment (MSA) from the specific family of interest, making them unable to benefit from transfer learning across families. To address this, we propose $\\textbf{P}$r$\\textbf{o}$tein $\\textbf{E}$volutionary $\\textbf{T}$ransformer (PoET), an autoregressive generative model of whole protein families that learns to generate sets of related proteins as sequences-of-sequences across tens of millions of natural protein sequence clusters. PoET can be used as a retrieval-augmented language model to generate and score arbitrary modifications conditioned on any protein family of interest, and can extrapolate from short context lengths to generalize well even for small families. This is enabled by a unique Transformer layer; we model tokens sequentially within sequences while attending between sequences order invariantly, allowing PoET to scale to context lengths beyond those used during training. In extensive experiments on deep mutational scanning datasets, we show that PoET outperforms existing protein language models and evolutionary sequence models for variant function prediction across proteins of all MSA depths. We also demonstrate PoET's ability to controllably generate new protein sequences.",
    "DOI": "10.48550/arXiv.2306.06156",
    "note": "arXiv:2306.06156 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2306.06156",
    "number": "arXiv:2306.06156",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "PoET: A generative model of protein families as sequences-of-sequences",
    "title-short": "PoET",
    "URL": "http://arxiv.org/abs/2306.06156",
    "author": [
      {
        "family": "Truong Jr",
        "given": "Timothy F."
      },
      {
        "family": "Bepler",
        "given": "Tristan"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          11,
          1
        ]
      ]
    }
  },
  {
    "id": "1sH9m3JL",
    "type": "article",
    "abstract": "Large Language Models (LLMs), with their remarkable task-handling capabilities and innovative outputs, have catalyzed significant advancements across a spectrum of fields. However, their proficiency within specialized domains such as biomolecular studies remains limited. To address this challenge, we introduce Mol-Instructions, a comprehensive instruction dataset designed for the biomolecular domain. Mol-Instructions encompasses three key components: molecule-oriented instructions, protein-oriented instructions, and biomolecular text instructions. Each component aims to improve the understanding and prediction capabilities of LLMs concerning biomolecular features and behaviors. Through extensive instruction tuning experiments on LLMs, we demonstrate the effectiveness of Mol-Instructions in enhancing large models' performance in the intricate realm of biomolecular studies, thus fostering progress in the biomolecular research community. Mol-Instructions is publicly available for ongoing research and will undergo regular updates to enhance its applicability.",
    "DOI": "10.48550/arXiv.2306.08018",
    "note": "arXiv:2306.08018 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2306.08018",
    "number": "arXiv:2306.08018",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models",
    "title-short": "Mol-Instructions",
    "URL": "http://arxiv.org/abs/2306.08018",
    "author": [
      {
        "family": "Fang",
        "given": "Yin"
      },
      {
        "family": "Liang",
        "given": "Xiaozhuan"
      },
      {
        "family": "Zhang",
        "given": "Ningyu"
      },
      {
        "family": "Liu",
        "given": "Kangwei"
      },
      {
        "family": "Huang",
        "given": "Rui"
      },
      {
        "family": "Chen",
        "given": "Zhuo"
      },
      {
        "family": "Fan",
        "given": "Xiaohui"
      },
      {
        "family": "Chen",
        "given": "Huajun"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          3,
          4
        ]
      ]
    }
  },
  {
    "id": "q1jtH9xA",
    "type": "article",
    "abstract": "Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenization. Based on these insights, we introduce DNABERT-2, a refined genome foundation model that adapts an efficient tokenizer and employs multiple strategies to overcome input length constraints, reduce time and memory expenditure, and enhance model capability. Furthermore, we identify the absence of a comprehensive and standardized benchmark for genome understanding as another significant impediment to fair comparative analysis. In response, we propose the Genome Understanding Evaluation (GUE), a comprehensive multi-species genome classification dataset that amalgamates $36$ distinct datasets across $9$ tasks, with input lengths ranging from $70$ to $10000$. Through comprehensive experiments on the GUE benchmark, we demonstrate that DNABERT-2 achieves comparable performance to the state-of-the-art model with $21 \\times$ fewer parameters and approximately $92 \\times$ less GPU time in pre-training.",
    "DOI": "10.48550/arXiv.2306.15006",
    "note": "arXiv:2306.15006 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2306.15006",
    "number": "arXiv:2306.15006",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome",
    "title-short": "DNABERT-2",
    "URL": "http://arxiv.org/abs/2306.15006",
    "author": [
      {
        "family": "Zhou",
        "given": "Zhihan"
      },
      {
        "family": "Ji",
        "given": "Yanrong"
      },
      {
        "family": "Li",
        "given": "Weijian"
      },
      {
        "family": "Dutta",
        "given": "Pratik"
      },
      {
        "family": "Davuluri",
        "given": "Ramana"
      },
      {
        "family": "Liu",
        "given": "Han"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          3,
          18
        ]
      ]
    }
  },
  {
    "id": "171NrBohw",
    "type": "article",
    "abstract": "Genomic (DNA) sequences encode an enormous amount of information for gene regulation and protein synthesis. Similar to natural language models, researchers have proposed foundation models in genomics to learn generalizable features from unlabeled genome data that can then be fine-tuned for downstream tasks such as identifying regulatory elements. Due to the quadratic scaling of attention, previous Transformer-based genomic models have used 512 to 4k tokens as context (<0.001% of the human genome), significantly limiting the modeling of long-range interactions in DNA. In addition, these methods rely on tokenizers or fixed k-mers to aggregate meaningful DNA units, losing single nucleotide resolution where subtle genetic variations can completely alter protein function via single nucleotide polymorphisms (SNPs). Recently, Hyena, a large language model based on implicit convolutions was shown to match attention in quality while allowing longer context lengths and lower time complexity. Leveraging Hyena's new long-range capabilities, we present HyenaDNA, a genomic foundation model pretrained on the human reference genome with context lengths of up to 1 million tokens at the single nucleotide-level - an up to 500x increase over previous dense attention-based models. HyenaDNA scales sub-quadratically in sequence length (training up to 160x faster than Transformer), uses single nucleotide tokens, and has full global context at each layer. We explore what longer context enables - including the first use of in-context learning in genomics. On fine-tuned benchmarks from the Nucleotide Transformer, HyenaDNA reaches state-of-the-art (SotA) on 12 of 18 datasets using a model with orders of magnitude less parameters and pretraining data. On the GenomicBenchmarks, HyenaDNA surpasses SotA on 7 of 8 datasets on average by +10 accuracy points. Code at https://github.com/HazyResearch/hyena-dna.",
    "DOI": "10.48550/arXiv.2306.15794",
    "note": "arXiv:2306.15794 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2306.15794",
    "number": "arXiv:2306.15794",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution",
    "title-short": "HyenaDNA",
    "URL": "http://arxiv.org/abs/2306.15794",
    "author": [
      {
        "family": "Nguyen",
        "given": "Eric"
      },
      {
        "family": "Poli",
        "given": "Michael"
      },
      {
        "family": "Faizi",
        "given": "Marjan"
      },
      {
        "family": "Thomas",
        "given": "Armin"
      },
      {
        "family": "Birch-Sykes",
        "given": "Callum"
      },
      {
        "family": "Wornow",
        "given": "Michael"
      },
      {
        "family": "Patel",
        "given": "Aman"
      },
      {
        "family": "Rabideau",
        "given": "Clayton"
      },
      {
        "family": "Massaroli",
        "given": "Stefano"
      },
      {
        "family": "Bengio",
        "given": "Yoshua"
      },
      {
        "family": "Ermon",
        "given": "Stefano"
      },
      {
        "family": "Baccus",
        "given": "Stephen A."
      },
      {
        "family": "Ré",
        "given": "Chris"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          11,
          14
        ]
      ]
    }
  },
  {
    "id": "1A950Jvos",
    "type": "article-journal",
    "abstract": "In recent years, significant progress has been made in the field of protein function prediction with the development of various machine-learning approaches. However, most existing methods formulate the task as a multi-classification problem, i.e. assigning predefined labels to proteins. In this work, we propose a novel approach, Prot2Text, which predicts a protein's function in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including protein sequence, structure, and textual annotation and description. This multimodal approach allows for a holistic representation of proteins' functions, enabling the generation of detailed and accurate functional descriptions. To evaluate our model, we extracted a multimodal protein dataset from SwissProt, and demonstrate empirically the effectiveness of Prot2Text. These results highlight the transformative impact of multimodal models, specifically the fusion of GNNs and LLMs, empowering researchers with powerful tools for more accurate function prediction of existing as well as first-to-see proteins.",
    "container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
    "DOI": "10.1609/aaai.v38i10.28948",
    "ISSN": "2374-3468, 2159-5399",
    "issue": "10",
    "journalAbbreviation": "AAAI",
    "note": "arXiv:2307.14367 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2307.14367",
    "page": "10757-10765",
    "source": "arXiv.org",
    "title": "Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers",
    "title-short": "Prot2Text",
    "URL": "http://arxiv.org/abs/2307.14367",
    "volume": "38",
    "author": [
      {
        "family": "Abdine",
        "given": "Hadi"
      },
      {
        "family": "Chatzianastasis",
        "given": "Michail"
      },
      {
        "family": "Bouyioukos",
        "given": "Costas"
      },
      {
        "family": "Vazirgiannis",
        "given": "Michalis"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          3,
          24
        ]
      ]
    }
  },
  {
    "id": "J8Pydzc8",
    "type": "article",
    "abstract": "Foundation models (FMs) have exhibited remarkable performance across a wide range of downstream tasks in many domains. Nevertheless, general-purpose FMs often face challenges when confronted with domain-specific problems, due to their limited access to the proprietary training data in a particular domain. In biomedicine, there are various biological modalities, such as molecules, proteins, and cells, which are encoded by the language of life and exhibit significant modality gaps with human natural language. In this paper, we introduce BioMedGPT, an open multimodal generative pre-trained transformer (GPT) for biomedicine, to bridge the gap between the language of life and human natural language. BioMedGPT allows users to easily ``communicate'' with diverse biological modalities through free text, which is the first of its kind. BioMedGPT aligns different biological modalities with natural language via a large generative language model, namely, BioMedGPT-LM. We publish BioMedGPT-10B, which unifies the feature spaces of molecules, proteins, and natural language via encoding and alignment. Through fine-tuning, BioMedGPT-10B outperforms or is on par with human and significantly larger general-purpose foundation models on the biomedical QA task. It also demonstrates promising performance in the molecule QA and protein QA tasks, which could greatly accelerate the discovery of new drugs and therapeutic targets. In addition, BioMedGPT-LM-7B is the first large generative language model based on Llama2 in the biomedical domain, therefore is commercial friendly. Both BioMedGPT-10B and BioMedGPT-LM-7B are open-sourced to the research community. In addition, we publish the datasets that are meticulously curated for the alignment of multi-modalities, i.e., PubChemQA and UniProtQA. All the models, codes, and datasets are available at \\url{https://github.com/PharMolix/OpenBioMed}.",
    "DOI": "10.48550/arXiv.2308.09442",
    "note": "arXiv:2308.09442 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2308.09442",
    "number": "arXiv:2308.09442",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine",
    "title-short": "BioMedGPT",
    "URL": "http://arxiv.org/abs/2308.09442",
    "author": [
      {
        "family": "Luo",
        "given": "Yizhen"
      },
      {
        "family": "Zhang",
        "given": "Jiahuan"
      },
      {
        "family": "Fan",
        "given": "Siqi"
      },
      {
        "family": "Yang",
        "given": "Kai"
      },
      {
        "family": "Wu",
        "given": "Yushuai"
      },
      {
        "family": "Qiao",
        "given": "Mu"
      },
      {
        "family": "Nie",
        "given": "Zaiqing"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          8,
          21
        ]
      ]
    }
  },
  {
    "URL": "https://arxiv.org/abs/2308.13149",
    "type": "webpage",
    "id": "AU2DrFMM",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2308.13149"
  },
  {
    "id": "hhou3YWO",
    "type": "article",
    "abstract": "Artificial intelligence (AI) promises immense benefits across sectors, yet also poses risks from dual-use potentials, biases, and unintended behaviors. This paper reviews emerging issues with opaque and uncontrollable AI systems and proposes an integrative framework called violet teaming to develop reliable and responsible AI. Violet teaming combines adversarial vulnerability probing (red teaming) with solutions for safety and security (blue teaming) while prioritizing ethics and social benefit. It emerged from AI safety research to manage risks proactively by design. The paper traces the evolution of red, blue, and purple teaming toward violet teaming, and then discusses applying violet techniques to address biosecurity risks of AI in biotechnology. Additional sections review key perspectives across law, ethics, cybersecurity, macrostrategy, and industry best practices essential for operationalizing responsible AI through holistic technical and social considerations. Violet teaming provides both philosophy and method for steering AI trajectories toward societal good. With conscience and wisdom, the extraordinary capabilities of AI can enrich humanity. But without adequate precaution, the risks could prove catastrophic. Violet teaming aims to empower moral technology for the common welfare.",
    "DOI": "10.48550/arXiv.2308.14253",
    "note": "arXiv:2308.14253 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2308.14253",
    "number": "arXiv:2308.14253",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward",
    "URL": "http://arxiv.org/abs/2308.14253",
    "author": [
      {
        "family": "Titus",
        "given": "Alexander J."
      },
      {
        "family": "Russell",
        "given": "Adam H."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          8,
          27
        ]
      ]
    }
  },
  {
    "id": "1HaTNUuyf",
    "type": "article",
    "abstract": "This paper presents the Ensemble Nucleotide Byte-level Encoder-Decoder (ENBED) foundation model, analyzing DNA sequences at byte-level precision with an encoder-decoder Transformer architecture. ENBED uses a sub-quadratic implementation of attention to develop an efficient model capable of sequence-to-sequence transformations, generalizing previous genomic models with encoder-only or decoder-only architectures. We use Masked Language Modeling to pre-train the foundation model using reference genome sequences and apply it in the following downstream tasks: (1) identification of enhancers, promotors and splice sites, (2) recognition of sequences containing base call mismatches and insertion/deletion errors, an advantage over tokenization schemes involving multiple base pairs, which lose the ability to analyze with byte-level precision, (3) identification of biological function annotations of genomic sequences, and (4) generating mutations of the Influenza virus using the encoder-decoder architecture and validating them against real-world observations. In each of these tasks, we demonstrate significant improvement as compared to the existing state-of-the-art results.",
    "DOI": "10.48550/arXiv.2311.02333",
    "note": "arXiv:2311.02333 [cs, q-bio]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2311.02333",
    "number": "arXiv:2311.02333",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision",
    "URL": "http://arxiv.org/abs/2311.02333",
    "author": [
      {
        "family": "Malusare",
        "given": "Aditya"
      },
      {
        "family": "Kothandaraman",
        "given": "Harish"
      },
      {
        "family": "Tamboli",
        "given": "Dipesh"
      },
      {
        "family": "Lanman",
        "given": "Nadia A."
      },
      {
        "family": "Aggarwal",
        "given": "Vaneet"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          2,
          13
        ]
      ]
    }
  },
  {
    "id": "eRP3HctH",
    "type": "article",
    "abstract": "Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource. We also maintain a regularly updated list of practical guides on medical LLMs at: https://github.com/AI-in-Health/MedLLMsPracticalGuide.",
    "DOI": "10.48550/arXiv.2311.05112",
    "note": "arXiv:2311.05112 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2311.05112",
    "number": "arXiv:2311.05112",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge",
    "title-short": "A Survey of Large Language Models in Medicine",
    "URL": "http://arxiv.org/abs/2311.05112",
    "author": [
      {
        "family": "Zhou",
        "given": "Hongjian"
      },
      {
        "family": "Liu",
        "given": "Fenglin"
      },
      {
        "family": "Gu",
        "given": "Boyang"
      },
      {
        "family": "Zou",
        "given": "Xinyu"
      },
      {
        "family": "Huang",
        "given": "Jinfa"
      },
      {
        "family": "Wu",
        "given": "Jinge"
      },
      {
        "family": "Li",
        "given": "Yiru"
      },
      {
        "family": "Chen",
        "given": "Sam S."
      },
      {
        "family": "Zhou",
        "given": "Peilin"
      },
      {
        "family": "Liu",
        "given": "Junling"
      },
      {
        "family": "Hua",
        "given": "Yining"
      },
      {
        "family": "Mao",
        "given": "Chengfeng"
      },
      {
        "family": "You",
        "given": "Chenyu"
      },
      {
        "family": "Wu",
        "given": "Xian"
      },
      {
        "family": "Zheng",
        "given": "Yefeng"
      },
      {
        "family": "Clifton",
        "given": "Lei"
      },
      {
        "family": "Li",
        "given": "Zheng"
      },
      {
        "family": "Luo",
        "given": "Jiebo"
      },
      {
        "family": "Clifton",
        "given": "David A."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          5,
          15
        ]
      ]
    }
  },
  {
    "id": "xoXi6JBv",
    "type": "article",
    "abstract": "Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of \"scientific language\", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.",
    "DOI": "10.48550/arXiv.2401.14656",
    "note": "arXiv:2401.14656 [cs]\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://arxiv.org/abs/2401.14656",
    "number": "arXiv:2401.14656",
    "publisher": "arXiv",
    "source": "arXiv.org",
    "title": "Scientific Large Language Models: A Survey on Biological & Chemical Domains",
    "title-short": "Scientific Large Language Models",
    "URL": "http://arxiv.org/abs/2401.14656",
    "author": [
      {
        "family": "Zhang",
        "given": "Qiang"
      },
      {
        "family": "Ding",
        "given": "Keyang"
      },
      {
        "family": "Lyv",
        "given": "Tianwen"
      },
      {
        "family": "Wang",
        "given": "Xinda"
      },
      {
        "family": "Yin",
        "given": "Qingyu"
      },
      {
        "family": "Zhang",
        "given": "Yiwen"
      },
      {
        "family": "Yu",
        "given": "Jing"
      },
      {
        "family": "Wang",
        "given": "Yuhao"
      },
      {
        "family": "Li",
        "given": "Xiaotong"
      },
      {
        "family": "Xiang",
        "given": "Zhuoyi"
      },
      {
        "family": "Zhuang",
        "given": "Xiang"
      },
      {
        "family": "Wang",
        "given": "Zeyuan"
      },
      {
        "family": "Qin",
        "given": "Ming"
      },
      {
        "family": "Zhang",
        "given": "Mengyao"
      },
      {
        "family": "Zhang",
        "given": "Jinlu"
      },
      {
        "family": "Cui",
        "given": "Jiyu"
      },
      {
        "family": "Xu",
        "given": "Renjun"
      },
      {
        "family": "Chen",
        "given": "Hongyang"
      },
      {
        "family": "Fan",
        "given": "Xiaohui"
      },
      {
        "family": "Xing",
        "given": "Huabin"
      },
      {
        "family": "Chen",
        "given": "Huajun"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          1,
          26
        ]
      ]
    }
  },
  {
    "id": "1C5bcQVLo",
    "type": "article",
    "abstract": "SMILES is a line notation for entering and representing molecules. Being inherently a language construct, it allows estimating molecular data in a self-supervised fashion by employing machine learning methods for natural language processing (NLP). The recent success of attention-based neural networks in NLP has made large-corpora transformer pretraining a de facto standard for learning representations and transferring knowledge to downstream tasks. In this work, we attempt to adapt transformer capabilities to a large SMILES corpus by constructing a GPT-2-like language model. We experimentally show that a pretrained causal transformer captures general knowledge that can be successfully transferred to such downstream tasks as focused molecule generation and single-/multi-output molecular-property prediction. For each task, we freeze model parameters and attach trainable lightweight networks between attention blocks—adapters—as alternative to fine-tuning. With a relatively modest setup, our transformer outperforms the recently proposed ChemBERTa transformer and approaches state-of-the-art MoleculeNet and Chemprop results. Overall, transformers pretrained on SMILES corpora are promising alternatives that do not require handcrafted feature engineering, make few assumptions about structure of data, and scale well with the pretraining data size.",
    "DOI": "10.26434/chemrxiv-2021-5fwjd",
    "language": "en",
    "publisher": "ChemRxiv",
    "source": "Cambridge Engage Preprints",
    "title": "Generative Pre-Training from Molecules",
    "URL": "https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e",
    "author": [
      {
        "family": "Adilov",
        "given": "Sanjar"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          9,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e"
  },
  {
    "id": "3WhKhYXF",
    "type": "post-weblog",
    "abstract": "“AI red-teaming” is currently a hot topic, but what does it actually mean? This blog post explains the term’s cybersecurity origins, why AI red-teaming should incorporate cybersecurity practices, and how its evolving definition and sometimes inconsistent usage can be misleading for policymakers interested in exploring testing requirements for AI systems.",
    "container-title": "Center for Security and Emerging Technology",
    "language": "en-US",
    "title": "What Does AI Red-Teaming Actually Mean?",
    "URL": "https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean/",
    "author": [
      {
        "family": "Baker",
        "given": "Tessa"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          10,
          24
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean"
  },
  {
    "URL": "https://febs.onlinelibrary.wiley.com/toc/14321033/5/2",
    "type": "webpage",
    "id": "ibvKtnrl",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://febs.onlinelibrary.wiley.com/toc/14321033/5/2"
  },
  {
    "id": "OrFhTkp9",
    "type": "webpage",
    "abstract": "The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems",
    "language": "en",
    "title": "GLUE Benchmark",
    "URL": "https://gluebenchmark.com/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://gluebenchmark.com"
  },
  {
    "id": "TKrwMi6z",
    "type": "article-journal",
    "abstract": "This paper introduces Purple Teaming as a comprehensive and collaborative approach to cyber security, emphasising the need for organisations to adapt their cyber security testing methodologies in response to evolving cyber threats. Traditionally, cyber security efforts were divided into offensive (Red Team) and defensive (Blue Team) units; however, the concept of Purple Teaming has gained prominence, advocating for the integration of these units to create a dynamic and cooperative cyber security environment. The paper covers various topics including the significance of adversary emulation, the role of the MITRE ATT&CK framework in standardising communication, the value of traditional Red Team exercises and how Purple Teaming activities can complement these exercises. It differentiates between types of Purple Teaming activities and proposes an approach and architecture to support continuous Purple Teaming efforts. Adversary emulation, a key aspect of Purple Teaming, involves replicating the tactics, techniques and procedures (TTPs) of real-world threat actors to evaluate an organisation’s defences. The paper outlines how, when properly combined, Red and Purple Team efforts can significantly enhance an organisation’s capability to proactively improve its preventative, detection and response mechanisms against adversary tactics. Through its comprehensive coverage, the paper underscores the vital role of Purple Teaming in modern cyber security, highlighting its potential to foster a more resilient and proactive security posture for organisations.",
    "container-title": "Cyber Security: A Peer-Reviewed Journal",
    "issue": "3",
    "language": "en",
    "page": "207-216",
    "source": "ideas.repec.org",
    "title": "Purple Teaming: A comprehensive and collaborative approach to cyber security",
    "title-short": "Purple Teaming",
    "URL": "https://ideas.repec.org//a/aza/csj000/y2024v7i3p207-216.html",
    "volume": "7",
    "author": [
      {
        "family": "Van Buggenhout",
        "given": "Erik"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://ideas.repec.org/a/aza/csj000/y2024v7i3p207-216.html"
  },
  {
    "id": "dae8L6vr",
    "type": "webpage",
    "title": "Expert systems: An overview | IEEE Journals & Magazine | IEEE Xplore",
    "URL": "https://ieeexplore.ieee.org/document/1145205",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://ieeexplore.ieee.org/document/1145205"
  },
  {
    "id": "IWQdHNWs",
    "type": "webpage",
    "title": "Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approach | IEEE Journals & Magazine | IEEE Xplore",
    "URL": "https://ieeexplore.ieee.org/document/8716527",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://ieeexplore.ieee.org/document/8716527"
  },
  {
    "id": "bLNWjb9u",
    "type": "webpage",
    "title": "Unsupervised K-Means Clustering Algorithm | IEEE Journals & Magazine | IEEE Xplore",
    "URL": "https://ieeexplore.ieee.org/document/9072123",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://ieeexplore.ieee.org/document/9072123"
  },
  {
    "id": "12hNAUYKX",
    "type": "webpage",
    "abstract": "A KPMG UK blog following the Digital Ethics Summit. This blog focuses on how AI is set to transform the healthcare and life science sectors.",
    "container-title": "KPMG",
    "language": "en-GB",
    "title": "Navigating the legal and ethical challenges of AI in healthcare - KPMG UK",
    "URL": "https://kpmg.com/uk/en/home/insights/2024/03/navigating-the-legal-and-ethical-challenges-of-ai-in-healthcare.html",
    "author": [
      {
        "family": "Simpson",
        "given": "Caroline Rivett",
        "suffix": "Isabel"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          3,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://kpmg.com/uk/en/home/insights/2024/03/navigating-the-legal-and-ethical-challenges-of-ai-in-healthcare.html"
  },
  {
    "id": "yxIOlxEU",
    "type": "article-journal",
    "abstract": "In this paper three problems for a connectionist account of language are considered1.What is the nature of linguistic representations?2.How can complex structural relationships such as constituent be represented?3.How can the apparently open-ended nature of language be accommodated by a fixed-resource system?",
    "container-title": "Machine Learning",
    "DOI": "10.1007/BF00114844",
    "ISSN": "1573-0565",
    "issue": "2",
    "journalAbbreviation": "Mach Learn",
    "language": "en",
    "page": "195-225",
    "source": "Springer Link",
    "title": "Distributed representations, simple recurrent networks, and grammatical structure",
    "URL": "https://doi.org/10.1007/BF00114844",
    "volume": "7",
    "author": [
      {
        "family": "Elman",
        "given": "Jeffrey L."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1991",
          9,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://link.springer.com/article/10.1007/BF00114844"
  },
  {
    "id": "RqzQpRL0",
    "type": "article-journal",
    "abstract": "This is the story of how Nick Metropolis came to build the MANIAC and about the scientific uses to which it was put in the early days of electronic computing. Among the illustrious scientists attracted to the MANIAC were Fermi, Teller, von Neumann, Bethe, Gamow, and Ulam. Many of the scientific contributions were “firsts” that had a profound influence on subsequent developments over a wide spectrum of scientific activities. These included the pion-proton phase shift analysis, the nonlinear coupled oscillators, the study of the genetic code, importance sampling, two-dimensional hydrodynamics, Monte Carlo calculation of nuclear cascades, universalities of iterative functions and “anticlerical” chess.",
    "container-title": "Journal of Statistical Physics",
    "DOI": "10.1007/BF02628301",
    "ISSN": "1572-9613",
    "issue": "5",
    "journalAbbreviation": "J Stat Phys",
    "language": "en",
    "page": "731-748",
    "source": "Springer Link",
    "title": "Scientific uses of the MANIAC",
    "URL": "https://doi.org/10.1007/BF02628301",
    "volume": "43",
    "author": [
      {
        "family": "Anderson",
        "given": "H. L."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1986",
          6,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://link.springer.com/article/10.1007/BF02628301"
  },
  {
    "id": "1BWjfloX3",
    "type": "article-journal",
    "abstract": "“Volunteer computing” is the use of consumer digital devices for high-throughput scientific computing. It can provide large computing capacity at low cost, but presents challenges due to device heterogeneity, unreliability, and churn. BOINC, a widely-used open-source middleware system for volunteer computing, addresses these challenges. We describe BOINC’s features, architecture, implementation, and algorithms.",
    "container-title": "Journal of Grid Computing",
    "DOI": "10.1007/s10723-019-09497-9",
    "ISSN": "1572-9184",
    "issue": "1",
    "journalAbbreviation": "J Grid Computing",
    "language": "en",
    "page": "99-122",
    "source": "Springer Link",
    "title": "BOINC: A Platform for Volunteer Computing",
    "title-short": "BOINC",
    "URL": "https://doi.org/10.1007/s10723-019-09497-9",
    "volume": "18",
    "author": [
      {
        "family": "Anderson",
        "given": "David P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          3,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://link.springer.com/article/10.1007/s10723-019-09497-9"
  },
  {
    "id": "117qAPYe7",
    "type": "article-journal",
    "abstract": "Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.",
    "container-title": "Machine Learning",
    "DOI": "10.1007/s10994-019-05855-6",
    "ISSN": "1573-0565",
    "issue": "2",
    "journalAbbreviation": "Mach Learn",
    "language": "en",
    "page": "373-440",
    "source": "Springer Link",
    "title": "A survey on semi-supervised learning",
    "URL": "https://doi.org/10.1007/s10994-019-05855-6",
    "volume": "109",
    "author": [
      {
        "family": "Engelen",
        "given": "Jesper E.",
        "non-dropping-particle": "van"
      },
      {
        "family": "Hoos",
        "given": "Holger H."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          2,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://link.springer.com/article/10.1007/s10994-019-05855-6"
  },
  {
    "id": "17EunIaqv",
    "type": "chapter",
    "abstract": "Lasergene comprises eight applications, organized into functional units. A user with the full Lasergene system might employ the software as follows:",
    "container-title": "Bioinformatics Methods and Protocols",
    "event-place": "Totowa, NJ",
    "ISBN": "9781592591923",
    "language": "en",
    "note": "DOI: 10.1385/1-59259-192-2:71\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://link.springer.com/protocol/10.1385/1-59259-192-2:71",
    "page": "71-91",
    "publisher": "Humana Press",
    "publisher-place": "Totowa, NJ",
    "source": "Springer Link",
    "title": "DNASTAR’s Lasergene Sequence Analysis Software",
    "URL": "https://doi.org/10.1385/1-59259-192-2:71",
    "author": [
      {
        "family": "Burland",
        "given": "Timothy G."
      }
    ],
    "editor": [
      {
        "family": "Misener",
        "given": "Stephen"
      },
      {
        "family": "Krawetz",
        "given": "Stephen A."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1999"
        ]
      ]
    }
  },
  {
    "URL": "https://openai.com/index/chatgpt",
    "type": "webpage",
    "id": "UgpiYzZc",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://openai.com/index/chatgpt"
  },
  {
    "URL": "https://openreview.net/forum?id",
    "type": "webpage",
    "id": "3AjKbAlp",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://openreview.net/forum?id"
  },
  {
    "id": "1DNwvIpHH",
    "type": "webpage",
    "abstract": "PubMed® comprises more than 37 million citations for biomedical literature from MEDLINE, life science journals, and online books. Citations may include links to full text content from PubMed Central and publisher web sites.",
    "container-title": "PubMed",
    "language": "en",
    "title": "PubMed",
    "URL": "https://pubmed.ncbi.nlm.nih.gov/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov"
  },
  {
    "id": "dCXEowTE",
    "type": "article-journal",
    "abstract": "The Protein Data Bank (PDB; http://www.rcsb.org/pdb/ ) is the single worldwide archive of structural data of biological macromolecules. This paper describes the goals of the PDB, the systems in place for data deposition and access, how to obtain further information, and near-term plans for the future development of the resource.",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/28.1.235",
    "ISSN": "0305-1048",
    "issue": "1",
    "journalAbbreviation": "Nucleic Acids Res",
    "language": "eng",
    "note": "PMID: 10592235\nPMCID: PMC102472\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/10592235",
    "page": "235-242",
    "source": "PubMed",
    "title": "The Protein Data Bank",
    "volume": "28",
    "author": [
      {
        "family": "Berman",
        "given": "H. M."
      },
      {
        "family": "Westbrook",
        "given": "J."
      },
      {
        "family": "Feng",
        "given": "Z."
      },
      {
        "family": "Gilliland",
        "given": "G."
      },
      {
        "family": "Bhat",
        "given": "T. N."
      },
      {
        "family": "Weissig",
        "given": "H."
      },
      {
        "family": "Shindyalov",
        "given": "I. N."
      },
      {
        "family": "Bourne",
        "given": "P. E."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2000",
          1,
          1
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/10592235"
  },
  {
    "id": "18guJvxxN",
    "type": "article-journal",
    "abstract": "We report on the quality of a whole-genome assembly of Drosophila melanogaster and the nature of the computer algorithms that accomplished it. Three independent external data sources essentially agree with and support the assembly's sequence and ordering of contigs across the euchromatic portion of the genome. In addition, there are isolated contigs that we believe represent nonrepetitive pockets within the heterochromatin of the centromeres. Comparison with a previously sequenced 2.9- megabase region indicates that sequencing accuracy within nonrepetitive segments is greater than 99. 99% without manual curation. As such, this initial reconstruction of the Drosophila sequence should be of substantial value to the scientific community.",
    "container-title": "Science (New York, N.Y.)",
    "DOI": "10.1126/science.287.5461.2196",
    "ISSN": "0036-8075",
    "issue": "5461",
    "journalAbbreviation": "Science",
    "language": "eng",
    "note": "PMID: 10731133\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/10731133",
    "page": "2196-2204",
    "source": "PubMed",
    "title": "A whole-genome assembly of Drosophila",
    "volume": "287",
    "author": [
      {
        "family": "Myers",
        "given": "E. W."
      },
      {
        "family": "Sutton",
        "given": "G. G."
      },
      {
        "family": "Delcher",
        "given": "A. L."
      },
      {
        "family": "Dew",
        "given": "I. M."
      },
      {
        "family": "Fasulo",
        "given": "D. P."
      },
      {
        "family": "Flanigan",
        "given": "M. J."
      },
      {
        "family": "Kravitz",
        "given": "S. A."
      },
      {
        "family": "Mobarry",
        "given": "C. M."
      },
      {
        "family": "Reinert",
        "given": "K. H."
      },
      {
        "family": "Remington",
        "given": "K. A."
      },
      {
        "family": "Anson",
        "given": "E. L."
      },
      {
        "family": "Bolanos",
        "given": "R. A."
      },
      {
        "family": "Chou",
        "given": "H. H."
      },
      {
        "family": "Jordan",
        "given": "C. M."
      },
      {
        "family": "Halpern",
        "given": "A. L."
      },
      {
        "family": "Lonardi",
        "given": "S."
      },
      {
        "family": "Beasley",
        "given": "E. M."
      },
      {
        "family": "Brandon",
        "given": "R. C."
      },
      {
        "family": "Chen",
        "given": "L."
      },
      {
        "family": "Dunn",
        "given": "P. J."
      },
      {
        "family": "Lai",
        "given": "Z."
      },
      {
        "family": "Liang",
        "given": "Y."
      },
      {
        "family": "Nusskern",
        "given": "D. R."
      },
      {
        "family": "Zhan",
        "given": "M."
      },
      {
        "family": "Zhang",
        "given": "Q."
      },
      {
        "family": "Zheng",
        "given": "X."
      },
      {
        "family": "Rubin",
        "given": "G. M."
      },
      {
        "family": "Adams",
        "given": "M. D."
      },
      {
        "family": "Venter",
        "given": "J. C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2000",
          3,
          24
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/10731133"
  },
  {
    "id": "7GkBaAWF",
    "type": "article-journal",
    "abstract": "A 2.91-billion base pair (bp) consensus sequence of the euchromatic portion of the human genome was generated by the whole-genome shotgun sequencing method. The 14.8-billion bp DNA sequence was generated over 9 months from 27,271,853 high-quality sequence reads (5.11-fold coverage of the genome) from both ends of plasmid clones made from the DNA of five individuals. Two assembly strategies-a whole-genome assembly and a regional chromosome assembly-were used, each combining sequence data from Celera and the publicly funded genome effort. The public data were shredded into 550-bp segments to create a 2.9-fold coverage of those genome regions that had been sequenced, without including biases inherent in the cloning and assembly procedure used by the publicly funded group. This brought the effective coverage in the assemblies to eightfold, reducing the number and size of gaps in the final assembly over what would be obtained with 5.11-fold coverage. The two assembly strategies yielded very similar results that largely agree with independent mapping data. The assemblies effectively cover the euchromatic regions of the human chromosomes. More than 90% of the genome is in scaffold assemblies of 100,000 bp or more, and 25% of the genome is in scaffolds of 10 million bp or larger. Analysis of the genome sequence revealed 26,588 protein-encoding transcripts for which there was strong corroborating evidence and an additional approximately 12,000 computationally derived genes with mouse matches or other weak supporting evidence. Although gene-dense clusters are obvious, almost half the genes are dispersed in low G+C sequence separated by large tracts of apparently noncoding sequence. Only 1.1% of the genome is spanned by exons, whereas 24% is in introns, with 75% of the genome being intergenic DNA. Duplications of segmental blocks, ranging in size up to chromosomal lengths, are abundant throughout the genome and reveal a complex evolutionary history. Comparative genomic analysis indicates vertebrate expansions of genes associated with neuronal function, with tissue-specific developmental regulation, and with the hemostasis and immune systems. DNA sequence comparisons between the consensus sequence and publicly funded genome data provided locations of 2.1 million single-nucleotide polymorphisms (SNPs). A random pair of human haploid genomes differed at a rate of 1 bp per 1250 on average, but there was marked heterogeneity in the level of polymorphism across the genome. Less than 1% of all SNPs resulted in variation in proteins, but the task of determining which SNPs have functional consequences remains an open challenge.",
    "container-title": "Science (New York, N.Y.)",
    "DOI": "10.1126/science.1058040",
    "ISSN": "0036-8075",
    "issue": "5507",
    "journalAbbreviation": "Science",
    "language": "eng",
    "note": "PMID: 11181995\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/11181995",
    "page": "1304-1351",
    "source": "PubMed",
    "title": "The sequence of the human genome",
    "volume": "291",
    "author": [
      {
        "family": "Venter",
        "given": "J. C."
      },
      {
        "family": "Adams",
        "given": "M. D."
      },
      {
        "family": "Myers",
        "given": "E. W."
      },
      {
        "family": "Li",
        "given": "P. W."
      },
      {
        "family": "Mural",
        "given": "R. J."
      },
      {
        "family": "Sutton",
        "given": "G. G."
      },
      {
        "family": "Smith",
        "given": "H. O."
      },
      {
        "family": "Yandell",
        "given": "M."
      },
      {
        "family": "Evans",
        "given": "C. A."
      },
      {
        "family": "Holt",
        "given": "R. A."
      },
      {
        "family": "Gocayne",
        "given": "J. D."
      },
      {
        "family": "Amanatides",
        "given": "P."
      },
      {
        "family": "Ballew",
        "given": "R. M."
      },
      {
        "family": "Huson",
        "given": "D. H."
      },
      {
        "family": "Wortman",
        "given": "J. R."
      },
      {
        "family": "Zhang",
        "given": "Q."
      },
      {
        "family": "Kodira",
        "given": "C. D."
      },
      {
        "family": "Zheng",
        "given": "X. H."
      },
      {
        "family": "Chen",
        "given": "L."
      },
      {
        "family": "Skupski",
        "given": "M."
      },
      {
        "family": "Subramanian",
        "given": "G."
      },
      {
        "family": "Thomas",
        "given": "P. D."
      },
      {
        "family": "Zhang",
        "given": "J."
      },
      {
        "family": "Gabor Miklos",
        "given": "G. L."
      },
      {
        "family": "Nelson",
        "given": "C."
      },
      {
        "family": "Broder",
        "given": "S."
      },
      {
        "family": "Clark",
        "given": "A. G."
      },
      {
        "family": "Nadeau",
        "given": "J."
      },
      {
        "family": "McKusick",
        "given": "V. A."
      },
      {
        "family": "Zinder",
        "given": "N."
      },
      {
        "family": "Levine",
        "given": "A. J."
      },
      {
        "family": "Roberts",
        "given": "R. J."
      },
      {
        "family": "Simon",
        "given": "M."
      },
      {
        "family": "Slayman",
        "given": "C."
      },
      {
        "family": "Hunkapiller",
        "given": "M."
      },
      {
        "family": "Bolanos",
        "given": "R."
      },
      {
        "family": "Delcher",
        "given": "A."
      },
      {
        "family": "Dew",
        "given": "I."
      },
      {
        "family": "Fasulo",
        "given": "D."
      },
      {
        "family": "Flanigan",
        "given": "M."
      },
      {
        "family": "Florea",
        "given": "L."
      },
      {
        "family": "Halpern",
        "given": "A."
      },
      {
        "family": "Hannenhalli",
        "given": "S."
      },
      {
        "family": "Kravitz",
        "given": "S."
      },
      {
        "family": "Levy",
        "given": "S."
      },
      {
        "family": "Mobarry",
        "given": "C."
      },
      {
        "family": "Reinert",
        "given": "K."
      },
      {
        "family": "Remington",
        "given": "K."
      },
      {
        "family": "Abu-Threideh",
        "given": "J."
      },
      {
        "family": "Beasley",
        "given": "E."
      },
      {
        "family": "Biddick",
        "given": "K."
      },
      {
        "family": "Bonazzi",
        "given": "V."
      },
      {
        "family": "Brandon",
        "given": "R."
      },
      {
        "family": "Cargill",
        "given": "M."
      },
      {
        "family": "Chandramouliswaran",
        "given": "I."
      },
      {
        "family": "Charlab",
        "given": "R."
      },
      {
        "family": "Chaturvedi",
        "given": "K."
      },
      {
        "family": "Deng",
        "given": "Z."
      },
      {
        "family": "Di Francesco",
        "given": "V."
      },
      {
        "family": "Dunn",
        "given": "P."
      },
      {
        "family": "Eilbeck",
        "given": "K."
      },
      {
        "family": "Evangelista",
        "given": "C."
      },
      {
        "family": "Gabrielian",
        "given": "A. E."
      },
      {
        "family": "Gan",
        "given": "W."
      },
      {
        "family": "Ge",
        "given": "W."
      },
      {
        "family": "Gong",
        "given": "F."
      },
      {
        "family": "Gu",
        "given": "Z."
      },
      {
        "family": "Guan",
        "given": "P."
      },
      {
        "family": "Heiman",
        "given": "T. J."
      },
      {
        "family": "Higgins",
        "given": "M. E."
      },
      {
        "family": "Ji",
        "given": "R. R."
      },
      {
        "family": "Ke",
        "given": "Z."
      },
      {
        "family": "Ketchum",
        "given": "K. A."
      },
      {
        "family": "Lai",
        "given": "Z."
      },
      {
        "family": "Lei",
        "given": "Y."
      },
      {
        "family": "Li",
        "given": "Z."
      },
      {
        "family": "Li",
        "given": "J."
      },
      {
        "family": "Liang",
        "given": "Y."
      },
      {
        "family": "Lin",
        "given": "X."
      },
      {
        "family": "Lu",
        "given": "F."
      },
      {
        "family": "Merkulov",
        "given": "G. V."
      },
      {
        "family": "Milshina",
        "given": "N."
      },
      {
        "family": "Moore",
        "given": "H. M."
      },
      {
        "family": "Naik",
        "given": "A. K."
      },
      {
        "family": "Narayan",
        "given": "V. A."
      },
      {
        "family": "Neelam",
        "given": "B."
      },
      {
        "family": "Nusskern",
        "given": "D."
      },
      {
        "family": "Rusch",
        "given": "D. B."
      },
      {
        "family": "Salzberg",
        "given": "S."
      },
      {
        "family": "Shao",
        "given": "W."
      },
      {
        "family": "Shue",
        "given": "B."
      },
      {
        "family": "Sun",
        "given": "J."
      },
      {
        "family": "Wang",
        "given": "Z."
      },
      {
        "family": "Wang",
        "given": "A."
      },
      {
        "family": "Wang",
        "given": "X."
      },
      {
        "family": "Wang",
        "given": "J."
      },
      {
        "family": "Wei",
        "given": "M."
      },
      {
        "family": "Wides",
        "given": "R."
      },
      {
        "family": "Xiao",
        "given": "C."
      },
      {
        "family": "Yan",
        "given": "C."
      },
      {
        "family": "Yao",
        "given": "A."
      },
      {
        "family": "Ye",
        "given": "J."
      },
      {
        "family": "Zhan",
        "given": "M."
      },
      {
        "family": "Zhang",
        "given": "W."
      },
      {
        "family": "Zhang",
        "given": "H."
      },
      {
        "family": "Zhao",
        "given": "Q."
      },
      {
        "family": "Zheng",
        "given": "L."
      },
      {
        "family": "Zhong",
        "given": "F."
      },
      {
        "family": "Zhong",
        "given": "W."
      },
      {
        "family": "Zhu",
        "given": "S."
      },
      {
        "family": "Zhao",
        "given": "S."
      },
      {
        "family": "Gilbert",
        "given": "D."
      },
      {
        "family": "Baumhueter",
        "given": "S."
      },
      {
        "family": "Spier",
        "given": "G."
      },
      {
        "family": "Carter",
        "given": "C."
      },
      {
        "family": "Cravchik",
        "given": "A."
      },
      {
        "family": "Woodage",
        "given": "T."
      },
      {
        "family": "Ali",
        "given": "F."
      },
      {
        "family": "An",
        "given": "H."
      },
      {
        "family": "Awe",
        "given": "A."
      },
      {
        "family": "Baldwin",
        "given": "D."
      },
      {
        "family": "Baden",
        "given": "H."
      },
      {
        "family": "Barnstead",
        "given": "M."
      },
      {
        "family": "Barrow",
        "given": "I."
      },
      {
        "family": "Beeson",
        "given": "K."
      },
      {
        "family": "Busam",
        "given": "D."
      },
      {
        "family": "Carver",
        "given": "A."
      },
      {
        "family": "Center",
        "given": "A."
      },
      {
        "family": "Cheng",
        "given": "M. L."
      },
      {
        "family": "Curry",
        "given": "L."
      },
      {
        "family": "Danaher",
        "given": "S."
      },
      {
        "family": "Davenport",
        "given": "L."
      },
      {
        "family": "Desilets",
        "given": "R."
      },
      {
        "family": "Dietz",
        "given": "S."
      },
      {
        "family": "Dodson",
        "given": "K."
      },
      {
        "family": "Doup",
        "given": "L."
      },
      {
        "family": "Ferriera",
        "given": "S."
      },
      {
        "family": "Garg",
        "given": "N."
      },
      {
        "family": "Gluecksmann",
        "given": "A."
      },
      {
        "family": "Hart",
        "given": "B."
      },
      {
        "family": "Haynes",
        "given": "J."
      },
      {
        "family": "Haynes",
        "given": "C."
      },
      {
        "family": "Heiner",
        "given": "C."
      },
      {
        "family": "Hladun",
        "given": "S."
      },
      {
        "family": "Hostin",
        "given": "D."
      },
      {
        "family": "Houck",
        "given": "J."
      },
      {
        "family": "Howland",
        "given": "T."
      },
      {
        "family": "Ibegwam",
        "given": "C."
      },
      {
        "family": "Johnson",
        "given": "J."
      },
      {
        "family": "Kalush",
        "given": "F."
      },
      {
        "family": "Kline",
        "given": "L."
      },
      {
        "family": "Koduru",
        "given": "S."
      },
      {
        "family": "Love",
        "given": "A."
      },
      {
        "family": "Mann",
        "given": "F."
      },
      {
        "family": "May",
        "given": "D."
      },
      {
        "family": "McCawley",
        "given": "S."
      },
      {
        "family": "McIntosh",
        "given": "T."
      },
      {
        "family": "McMullen",
        "given": "I."
      },
      {
        "family": "Moy",
        "given": "M."
      },
      {
        "family": "Moy",
        "given": "L."
      },
      {
        "family": "Murphy",
        "given": "B."
      },
      {
        "family": "Nelson",
        "given": "K."
      },
      {
        "family": "Pfannkoch",
        "given": "C."
      },
      {
        "family": "Pratts",
        "given": "E."
      },
      {
        "family": "Puri",
        "given": "V."
      },
      {
        "family": "Qureshi",
        "given": "H."
      },
      {
        "family": "Reardon",
        "given": "M."
      },
      {
        "family": "Rodriguez",
        "given": "R."
      },
      {
        "family": "Rogers",
        "given": "Y. H."
      },
      {
        "family": "Romblad",
        "given": "D."
      },
      {
        "family": "Ruhfel",
        "given": "B."
      },
      {
        "family": "Scott",
        "given": "R."
      },
      {
        "family": "Sitter",
        "given": "C."
      },
      {
        "family": "Smallwood",
        "given": "M."
      },
      {
        "family": "Stewart",
        "given": "E."
      },
      {
        "family": "Strong",
        "given": "R."
      },
      {
        "family": "Suh",
        "given": "E."
      },
      {
        "family": "Thomas",
        "given": "R."
      },
      {
        "family": "Tint",
        "given": "N. N."
      },
      {
        "family": "Tse",
        "given": "S."
      },
      {
        "family": "Vech",
        "given": "C."
      },
      {
        "family": "Wang",
        "given": "G."
      },
      {
        "family": "Wetter",
        "given": "J."
      },
      {
        "family": "Williams",
        "given": "S."
      },
      {
        "family": "Williams",
        "given": "M."
      },
      {
        "family": "Windsor",
        "given": "S."
      },
      {
        "family": "Winn-Deen",
        "given": "E."
      },
      {
        "family": "Wolfe",
        "given": "K."
      },
      {
        "family": "Zaveri",
        "given": "J."
      },
      {
        "family": "Zaveri",
        "given": "K."
      },
      {
        "family": "Abril",
        "given": "J. F."
      },
      {
        "family": "Guigó",
        "given": "R."
      },
      {
        "family": "Campbell",
        "given": "M. J."
      },
      {
        "family": "Sjolander",
        "given": "K. V."
      },
      {
        "family": "Karlak",
        "given": "B."
      },
      {
        "family": "Kejariwal",
        "given": "A."
      },
      {
        "family": "Mi",
        "given": "H."
      },
      {
        "family": "Lazareva",
        "given": "B."
      },
      {
        "family": "Hatton",
        "given": "T."
      },
      {
        "family": "Narechania",
        "given": "A."
      },
      {
        "family": "Diemer",
        "given": "K."
      },
      {
        "family": "Muruganujan",
        "given": "A."
      },
      {
        "family": "Guo",
        "given": "N."
      },
      {
        "family": "Sato",
        "given": "S."
      },
      {
        "family": "Bafna",
        "given": "V."
      },
      {
        "family": "Istrail",
        "given": "S."
      },
      {
        "family": "Lippert",
        "given": "R."
      },
      {
        "family": "Schwartz",
        "given": "R."
      },
      {
        "family": "Walenz",
        "given": "B."
      },
      {
        "family": "Yooseph",
        "given": "S."
      },
      {
        "family": "Allen",
        "given": "D."
      },
      {
        "family": "Basu",
        "given": "A."
      },
      {
        "family": "Baxendale",
        "given": "J."
      },
      {
        "family": "Blick",
        "given": "L."
      },
      {
        "family": "Caminha",
        "given": "M."
      },
      {
        "family": "Carnes-Stine",
        "given": "J."
      },
      {
        "family": "Caulk",
        "given": "P."
      },
      {
        "family": "Chiang",
        "given": "Y. H."
      },
      {
        "family": "Coyne",
        "given": "M."
      },
      {
        "family": "Dahlke",
        "given": "C."
      },
      {
        "family": "Deslattes Mays",
        "given": "A."
      },
      {
        "family": "Dombroski",
        "given": "M."
      },
      {
        "family": "Donnelly",
        "given": "M."
      },
      {
        "family": "Ely",
        "given": "D."
      },
      {
        "family": "Esparham",
        "given": "S."
      },
      {
        "family": "Fosler",
        "given": "C."
      },
      {
        "family": "Gire",
        "given": "H."
      },
      {
        "family": "Glanowski",
        "given": "S."
      },
      {
        "family": "Glasser",
        "given": "K."
      },
      {
        "family": "Glodek",
        "given": "A."
      },
      {
        "family": "Gorokhov",
        "given": "M."
      },
      {
        "family": "Graham",
        "given": "K."
      },
      {
        "family": "Gropman",
        "given": "B."
      },
      {
        "family": "Harris",
        "given": "M."
      },
      {
        "family": "Heil",
        "given": "J."
      },
      {
        "family": "Henderson",
        "given": "S."
      },
      {
        "family": "Hoover",
        "given": "J."
      },
      {
        "family": "Jennings",
        "given": "D."
      },
      {
        "family": "Jordan",
        "given": "C."
      },
      {
        "family": "Jordan",
        "given": "J."
      },
      {
        "family": "Kasha",
        "given": "J."
      },
      {
        "family": "Kagan",
        "given": "L."
      },
      {
        "family": "Kraft",
        "given": "C."
      },
      {
        "family": "Levitsky",
        "given": "A."
      },
      {
        "family": "Lewis",
        "given": "M."
      },
      {
        "family": "Liu",
        "given": "X."
      },
      {
        "family": "Lopez",
        "given": "J."
      },
      {
        "family": "Ma",
        "given": "D."
      },
      {
        "family": "Majoros",
        "given": "W."
      },
      {
        "family": "McDaniel",
        "given": "J."
      },
      {
        "family": "Murphy",
        "given": "S."
      },
      {
        "family": "Newman",
        "given": "M."
      },
      {
        "family": "Nguyen",
        "given": "T."
      },
      {
        "family": "Nguyen",
        "given": "N."
      },
      {
        "family": "Nodell",
        "given": "M."
      },
      {
        "family": "Pan",
        "given": "S."
      },
      {
        "family": "Peck",
        "given": "J."
      },
      {
        "family": "Peterson",
        "given": "M."
      },
      {
        "family": "Rowe",
        "given": "W."
      },
      {
        "family": "Sanders",
        "given": "R."
      },
      {
        "family": "Scott",
        "given": "J."
      },
      {
        "family": "Simpson",
        "given": "M."
      },
      {
        "family": "Smith",
        "given": "T."
      },
      {
        "family": "Sprague",
        "given": "A."
      },
      {
        "family": "Stockwell",
        "given": "T."
      },
      {
        "family": "Turner",
        "given": "R."
      },
      {
        "family": "Venter",
        "given": "E."
      },
      {
        "family": "Wang",
        "given": "M."
      },
      {
        "family": "Wen",
        "given": "M."
      },
      {
        "family": "Wu",
        "given": "D."
      },
      {
        "family": "Wu",
        "given": "M."
      },
      {
        "family": "Xia",
        "given": "A."
      },
      {
        "family": "Zandieh",
        "given": "A."
      },
      {
        "family": "Zhu",
        "given": "X."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2001",
          2,
          16
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/11181995"
  },
  {
    "id": "prCR1hpf",
    "type": "article-journal",
    "abstract": "Bioinformatics is often described as being in its infancy, but computers emerged as important tools in molecular biology during the early 1960s. A decade before DNA sequencing became feasible, computational biologists focused on the rapidly accumulating data from protein biochemistry. Without the benefits of super computers or computer networks, these scientists laid important conceptual and technical foundations for bioinformatics today.",
    "container-title": "Nature Reviews. Genetics",
    "DOI": "10.1038/35042090",
    "ISSN": "1471-0056",
    "issue": "3",
    "journalAbbreviation": "Nat Rev Genet",
    "language": "eng",
    "note": "PMID: 11252753\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/11252753",
    "page": "231-236",
    "source": "PubMed",
    "title": "The origins of bioinformatics",
    "volume": "1",
    "author": [
      {
        "family": "Hagen",
        "given": "J. B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2000",
          12
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/11252753"
  },
  {
    "id": "AtEWfwCG",
    "type": "article-journal",
    "container-title": "Nature",
    "DOI": "10.1038/181662a0",
    "ISSN": "0028-0836",
    "issue": "4610",
    "journalAbbreviation": "Nature",
    "language": "eng",
    "note": "PMID: 13517261\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/13517261",
    "page": "662-666",
    "source": "PubMed",
    "title": "A three-dimensional model of the myoglobin molecule obtained by x-ray analysis",
    "volume": "181",
    "author": [
      {
        "family": "Kendrew",
        "given": "J. C."
      },
      {
        "family": "Bodo",
        "given": "G."
      },
      {
        "family": "Dintzis",
        "given": "H. M."
      },
      {
        "family": "Parrish",
        "given": "R. G."
      },
      {
        "family": "Wyckoff",
        "given": "H."
      },
      {
        "family": "Phillips",
        "given": "D. C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1958",
          3,
          8
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/13517261"
  },
  {
    "id": "4gIVWSCR",
    "type": "article-journal",
    "container-title": "Proceedings of the National Academy of Sciences of the United States of America",
    "DOI": "10.1073/pnas.37.11.729",
    "ISSN": "0027-8424",
    "issue": "11",
    "journalAbbreviation": "Proc Natl Acad Sci U S A",
    "language": "eng",
    "note": "PMID: 16578412\nPMCID: PMC1063460\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/16578412",
    "page": "729-740",
    "source": "PubMed",
    "title": "Configurations of Polypeptide Chains With Favored Orientations Around Single Bonds: Two New Pleated Sheets",
    "title-short": "Configurations of Polypeptide Chains With Favored Orientations Around Single Bonds",
    "volume": "37",
    "author": [
      {
        "family": "Pauling",
        "given": "L."
      },
      {
        "family": "Corey",
        "given": "R. B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1951",
          11
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/16578412"
  },
  {
    "id": "Dq9jMQ6v",
    "type": "article-journal",
    "container-title": "Archives of Biochemistry",
    "ISSN": "0096-9621",
    "issue": "3",
    "journalAbbreviation": "Arch Biochem",
    "language": "eng",
    "note": "PMID: 18134557\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/18134557",
    "page": "475",
    "source": "PubMed",
    "title": "A method for the determination of amino acid sequence in peptides",
    "volume": "22",
    "author": [
      {
        "family": "Edman",
        "given": "P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1949",
          7
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/18134557"
  },
  {
    "id": "1DngqA2eE",
    "type": "article-journal",
    "abstract": "Structural proteomics is one of the powerful research areas in the postgenomic era, elucidating structure-function relationships of uncharacterized gene products based on the 3D protein structure. It proposes biochemical and cellular functions of unannotated proteins and thereby identifies potential drug design and protein engineering targets. Recently, a number of pioneering groups in structural proteomics research have achieved proof of structural proteomic theory by predicting the 3D structures of hypothetical proteins that successfully identified the biological functions of those proteins. The pioneering groups made use of a number of techniques, including NMR spectroscopy, which has been applied successfully to structural proteomics studies over the past 10 years. In addition, advances in hardware design, data acquisition methods, sample preparation and automation of data analysis have been developed and successfully applied to high-throughput structure determination techniques. These efforts ensure that NMR spectroscopy will become an important methodology for performing structural proteomics research on a genomic scale. NMR-based structural proteomics together with x-ray crystallography will provide a comprehensive structural database to predict the basic biological functions of hypothetical proteins identified by the genome projects.",
    "container-title": "Expert Review of Proteomics",
    "DOI": "10.1586/14789450.5.4.589",
    "ISSN": "1744-8387",
    "issue": "4",
    "journalAbbreviation": "Expert Rev Proteomics",
    "language": "eng",
    "note": "PMID: 18761469\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/18761469",
    "page": "589-601",
    "source": "PubMed",
    "title": "Structural proteomics by NMR spectroscopy",
    "volume": "5",
    "author": [
      {
        "family": "Shin",
        "given": "Joon"
      },
      {
        "family": "Lee",
        "given": "Woonghee"
      },
      {
        "family": "Lee",
        "given": "Weontae"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2008",
          8
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/18761469"
  },
  {
    "id": "WmbDqOWv",
    "type": "article-journal",
    "container-title": "Nature",
    "DOI": "10.1038/185422a0",
    "ISSN": "0028-0836",
    "issue": "4711",
    "journalAbbreviation": "Nature",
    "language": "eng",
    "note": "PMID: 18990802\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/18990802",
    "page": "422-427",
    "source": "PubMed",
    "title": "Structure of myoglobin: A three-dimensional Fourier synthesis at 2 A. resolution",
    "title-short": "Structure of myoglobin",
    "volume": "185",
    "author": [
      {
        "family": "Kendrew",
        "given": "J. C."
      },
      {
        "family": "Dickerson",
        "given": "R. E."
      },
      {
        "family": "Strandberg",
        "given": "B. E."
      },
      {
        "family": "Hart",
        "given": "R. G."
      },
      {
        "family": "Davies",
        "given": "D. R."
      },
      {
        "family": "Phillips",
        "given": "D. C."
      },
      {
        "family": "Shore",
        "given": "V. C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1960",
          2,
          13
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/18990802"
  },
  {
    "id": "YYcdJkZI",
    "type": "article-journal",
    "abstract": "Collecting, comparing, and computing molecular sequences are among the most prevalent practices in contemporary biological research. They represent a specific way of producing knowledge. This paper explores the historical development of these practices, focusing on the work of Margaret O. Dayhoff, Richard V. Eck, and Robert S. Ledley, who produced the first computer-based collection of protein sequences, published in book format in 1965 as the Atlas of Protein Sequence and Structure. While these practices are generally associated with the rise of molecular evolution in the 1960s, this paper shows that they grew out of research agendas from the previous decade, including the biochemical investigation of the relations between the structures and function of proteins and the theoretical attempt to decipher the genetic code. It also shows how computers became essential for the handling and analysis of sequence data. Finally, this paper reflects on the relationships between experimenting and collecting as two distinct \"ways of knowing\" that were essential for the transformation of the life sciences in the twentieth century.",
    "container-title": "Journal of the History of Biology",
    "DOI": "10.1007/s10739-009-9221-0",
    "ISSN": "0022-5010",
    "issue": "4",
    "journalAbbreviation": "J Hist Biol",
    "language": "eng",
    "note": "PMID: 20665074\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/20665074",
    "page": "623-660",
    "source": "PubMed",
    "title": "Collecting, comparing, and computing sequences: the making of Margaret O. Dayhoff's Atlas of Protein Sequence and Structure, 1954-1965",
    "title-short": "Collecting, comparing, and computing sequences",
    "volume": "43",
    "author": [
      {
        "family": "Strasser",
        "given": "Bruno J."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2010"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/20665074"
  },
  {
    "id": "FWkpcKEh",
    "type": "article-journal",
    "abstract": "Understanding how complex phenotypes arise from individual molecules and their interactions is a primary challenge in biology that computational approaches are poised to tackle. We report a whole-cell computational model of the life cycle of the human pathogen Mycoplasma genitalium that includes all of its molecular components and their interactions. An integrative approach to modeling that combines diverse mathematics enabled the simultaneous inclusion of fundamentally different cellular processes and experimental measurements. Our whole-cell model accounts for all annotated gene functions and was validated against a broad range of data. The model provides insights into many previously unobserved cellular behaviors, including in vivo rates of protein-DNA association and an inverse relationship between the durations of DNA replication initiation and replication. In addition, experimental analysis directed by model predictions identified previously undetected kinetic parameters and biological functions. We conclude that comprehensive whole-cell models can be used to facilitate biological discovery.",
    "container-title": "Cell",
    "DOI": "10.1016/j.cell.2012.05.044",
    "ISSN": "1097-4172",
    "issue": "2",
    "journalAbbreviation": "Cell",
    "language": "eng",
    "note": "PMID: 22817898\nPMCID: PMC3413483\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/22817898",
    "page": "389-401",
    "source": "PubMed",
    "title": "A whole-cell computational model predicts phenotype from genotype",
    "volume": "150",
    "author": [
      {
        "family": "Karr",
        "given": "Jonathan R."
      },
      {
        "family": "Sanghvi",
        "given": "Jayodita C."
      },
      {
        "family": "Macklin",
        "given": "Derek N."
      },
      {
        "family": "Gutschow",
        "given": "Miriam V."
      },
      {
        "family": "Jacobs",
        "given": "Jared M."
      },
      {
        "family": "Bolival",
        "given": "Benjamin"
      },
      {
        "family": "Assad-Garcia",
        "given": "Nacyra"
      },
      {
        "family": "Glass",
        "given": "John I."
      },
      {
        "family": "Covert",
        "given": "Markus W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2012",
          7,
          20
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/22817898"
  },
  {
    "id": "10odPMDxs",
    "type": "article-journal",
    "abstract": "Clustal Omega is a completely rewritten and revised version of the widely used Clustal series of programs for multiple sequence alignment. It can deal with very large numbers (many tens of thousands) of DNA/RNA or protein sequences due to its use of the mBED algorithm for calculating guide trees. This algorithm allows very large alignment problems to be tackled very quickly, even on personal computers. The accuracy of the program has been considerably improved over earlier Clustal programs, through the use of the HHalign method for aligning profile hidden Markov models. The program currently is used from the command line or can be run on line.",
    "container-title": "Methods in Molecular Biology (Clifton, N.J.)",
    "DOI": "10.1007/978-1-62703-646-7_6",
    "ISSN": "1940-6029",
    "journalAbbreviation": "Methods Mol Biol",
    "language": "eng",
    "note": "PMID: 24170397\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/24170397",
    "page": "105-116",
    "source": "PubMed",
    "title": "Clustal Omega, accurate alignment of very large numbers of sequences",
    "volume": "1079",
    "author": [
      {
        "family": "Sievers",
        "given": "Fabian"
      },
      {
        "family": "Higgins",
        "given": "Desmond G."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/24170397"
  },
  {
    "id": "EOFcwWBm",
    "type": "article-journal",
    "abstract": "Integrated whole-cell modeling is poised to make a dramatic impact on molecular and systems biology, bioengineering, and medicine--once certain obstacles are overcome. From our group's experience building a whole-cell model of Mycoplasma genitalium, we identified several significant challenges to building models of more complex cells. Here we review and discuss these challenges in seven areas: first, experimental interrogation; second, data curation; third, model building and integration; fourth, accelerated computation; fifth, analysis and visualization; sixth, model validation; and seventh, collaboration and community development. Surmounting these challenges will require the cooperation of an interdisciplinary group of researchers to create increasingly sophisticated whole-cell models and make data, models, and simulations more accessible to the wider community.",
    "container-title": "Current Opinion in Biotechnology",
    "DOI": "10.1016/j.copbio.2014.01.012",
    "ISSN": "1879-0429",
    "journalAbbreviation": "Curr Opin Biotechnol",
    "language": "eng",
    "note": "PMID: 24556244\nPMCID: PMC4111988\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/24556244",
    "page": "111-115",
    "source": "PubMed",
    "title": "The future of whole-cell modeling",
    "volume": "28",
    "author": [
      {
        "family": "Macklin",
        "given": "Derek N."
      },
      {
        "family": "Ruggero",
        "given": "Nicholas A."
      },
      {
        "family": "Covert",
        "given": "Markus W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2014",
          8
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/24556244"
  },
  {
    "id": "ZSOb6iY1",
    "type": "article-journal",
    "abstract": "Our ability to build computational models that account for all known gene functions in a cell has increased dramatically. But why build whole-cell models, and how can they best be used? In this forum, we enumerate several areas in which whole-cell modeling can significantly impact research and technology.",
    "container-title": "Trends in Cell Biology",
    "DOI": "10.1016/j.tcb.2015.09.004",
    "ISSN": "1879-3088",
    "issue": "12",
    "journalAbbreviation": "Trends Cell Biol",
    "language": "eng",
    "note": "PMID: 26471224\nPMCID: PMC4663153\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/26471224",
    "page": "719-722",
    "source": "PubMed",
    "title": "Why Build Whole-Cell Models?",
    "volume": "25",
    "author": [
      {
        "family": "Carrera",
        "given": "Javier"
      },
      {
        "family": "Covert",
        "given": "Markus W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2015",
          12
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/26471224"
  },
  {
    "id": "NQbXc701",
    "type": "article-journal",
    "abstract": "DNA can be sequenced by a chemical procedure that breaks a terminally labeled DNA molecule partially at each repetition of a base. The lengths of the labeled fragments then identify the positions of that base. We describe reactions that cleave DNA preferentially at guanines, at adenines, at cytosines and thymines equally, and at cytosines alone. When the products of these four reactions are resolved by size, by electrophoresis on a polyacrylamide gel, the DNA sequence can be read from the pattern of radioactive bands. The technique will permit sequencing of at least 100 bases from the point of labeling.",
    "container-title": "Proceedings of the National Academy of Sciences of the United States of America",
    "DOI": "10.1073/pnas.74.2.560",
    "ISSN": "0027-8424",
    "issue": "2",
    "journalAbbreviation": "Proc Natl Acad Sci U S A",
    "language": "eng",
    "note": "PMID: 265521\nPMCID: PMC392330\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/265521",
    "page": "560-564",
    "source": "PubMed",
    "title": "A new method for sequencing DNA",
    "volume": "74",
    "author": [
      {
        "family": "Maxam",
        "given": "A. M."
      },
      {
        "family": "Gilbert",
        "given": "W."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1977",
          2
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/265521"
  },
  {
    "id": "cc2nygki",
    "type": "article-journal",
    "abstract": "Protein secondary structure prediction began in 1951 when Pauling and Corey predicted helical and sheet conformations for protein polypeptide backbone even before the first protein structure was determined. Sixty-five years later, powerful new methods breathe new life into this field. The highest three-state accuracy without relying on structure templates is now at 82-84%, a number unthinkable just a few years ago. These improvements came from increasingly larger databases of protein sequences and structures for training, the use of template secondary structure information and more powerful deep learning techniques. As we are approaching to the theoretical limit of three-state prediction (88-90%), alternative to secondary structure prediction (prediction of backbone torsion angles and Cα-atom-based angles and torsion angles) not only has more room for further improvement but also allows direct prediction of three-dimensional fragment structures with constantly improved accuracy. About 20% of all 40-residue fragments in a database of 1199 non-redundant proteins have <6 Å root-mean-squared distance from the native conformations by SPIDER2. More powerful deep learning methods with improved capability of capturing long-range interactions begin to emerge as the next generation of techniques for secondary structure prediction. The time has come to finish off the final stretch of the long march towards protein secondary structure prediction.",
    "container-title": "Briefings in Bioinformatics",
    "DOI": "10.1093/bib/bbw129",
    "ISSN": "1477-4054",
    "issue": "3",
    "journalAbbreviation": "Brief Bioinform",
    "language": "eng",
    "note": "PMID: 28040746\nPMCID: PMC5952956\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/28040746",
    "page": "482-494",
    "source": "PubMed",
    "title": "Sixty-five years of the long march in protein secondary structure prediction: the final stretch?",
    "title-short": "Sixty-five years of the long march in protein secondary structure prediction",
    "volume": "19",
    "author": [
      {
        "family": "Yang",
        "given": "Yuedong"
      },
      {
        "family": "Gao",
        "given": "Jianzhao"
      },
      {
        "family": "Wang",
        "given": "Jihua"
      },
      {
        "family": "Heffernan",
        "given": "Rhys"
      },
      {
        "family": "Hanson",
        "given": "Jack"
      },
      {
        "family": "Paliwal",
        "given": "Kuldip"
      },
      {
        "family": "Zhou",
        "given": "Yaoqi"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2018",
          5,
          1
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/28040746"
  },
  {
    "id": "17nhyKsKm",
    "type": "article-journal",
    "abstract": "Bayesian methods have become very popular in molecular phylogenetics due to the availability of user-friendly software implementing sophisticated models of evolution. However, Bayesian phylogenetic models are complex, and analyses are often carried out using default settings, which may not be appropriate. Here, we summarize the major features of Bayesian phylogenetic inference and discuss Bayesian computation using Markov chain Monte Carlo (MCMC), the diagnosis of an MCMC run, and ways of summarising the MCMC sample. We discuss the specification of the prior, the choice of the substitution model, and partitioning of the data. Finally, we provide a list of common Bayesian phylogenetic software and provide recommendations as to their use.",
    "container-title": "Nature Ecology & Evolution",
    "DOI": "10.1038/s41559-017-0280-x",
    "ISSN": "2397-334X",
    "issue": "10",
    "journalAbbreviation": "Nat Ecol Evol",
    "language": "eng",
    "note": "PMID: 28983516\nPMCID: PMC5624502\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/28983516",
    "page": "1446-1454",
    "source": "PubMed",
    "title": "A biologist's guide to Bayesian phylogenetic analysis",
    "volume": "1",
    "author": [
      {
        "family": "Nascimento",
        "given": "Fabrícia F."
      },
      {
        "family": "Reis",
        "given": "Mario Dos"
      },
      {
        "family": "Yang",
        "given": "Ziheng"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2017",
          10
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/28983516"
  },
  {
    "id": "9IbNBDhy",
    "type": "article-journal",
    "abstract": "A progressive alignment method is described that utilizes the Needleman and Wunsch pairwise alignment algorithm iteratively to achieve the multiple alignment of a set of protein sequences and to construct an evolutionary tree depicting their relationship. The sequences are assumed a priori to share a common ancestor, and the trees are constructed from difference matrices derived directly from the multiple alignment. The thrust of the method involves putting more trust in the comparison of recently diverged sequences than in those evolved in the distant past. In particular, this rule is followed: \"once a gap, always a gap.\" The method has been applied to three sets of protein sequences: 7 superoxide dismutases, 11 globins, and 9 tyrosine kinase-like sequences. Multiple alignments and phylogenetic trees for these sets of sequences were determined and compared with trees derived by conventional pairwise treatments. In several instances, the progressive method led to trees that appeared to be more in line with biological expectations than were trees obtained by more commonly used methods.",
    "container-title": "Journal of Molecular Evolution",
    "DOI": "10.1007/BF02603120",
    "ISSN": "0022-2844",
    "issue": "4",
    "journalAbbreviation": "J Mol Evol",
    "language": "eng",
    "note": "PMID: 3118049\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/3118049",
    "page": "351-360",
    "source": "PubMed",
    "title": "Progressive sequence alignment as a prerequisite to correct phylogenetic trees",
    "volume": "25",
    "author": [
      {
        "family": "Feng",
        "given": "D. F."
      },
      {
        "family": "Doolittle",
        "given": "R. F."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1987"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/3118049"
  },
  {
    "id": "t5RQ2moY",
    "type": "article-journal",
    "abstract": "CASP (critical assessment of structure prediction) assesses the state of the art in modeling protein structure from amino acid sequence. The most recent experiment (CASP13 held in 2018) saw dramatic progress in structure modeling without use of structural templates (historically \"ab initio\" modeling). Progress was driven by the successful application of deep learning techniques to predict inter-residue distances. In turn, these results drove dramatic improvements in three-dimensional structure accuracy: With the proviso that there are an adequate number of sequences known for the protein family, the new methods essentially solve the long-standing problem of predicting the fold topology of monomeric proteins. Further, the number of sequences required in the alignment has fallen substantially. There is also substantial improvement in the accuracy of template-based models. Other areas-model refinement, accuracy estimation, and the structure of protein assemblies-have again yielded interesting results. CASP13 placed increased emphasis on the use of sparse data together with modeling and chemical crosslinking, SAXS, and NMR all yielded more mature results. This paper summarizes the key outcomes of CASP13. The special issue of PROTEINS contains papers describing the CASP13 assessments in each modeling category and contributions from the participants.",
    "container-title": "Proteins",
    "DOI": "10.1002/prot.25823",
    "ISSN": "1097-0134",
    "issue": "12",
    "journalAbbreviation": "Proteins",
    "language": "eng",
    "note": "PMID: 31589781\nPMCID: PMC6927249\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/31589781",
    "page": "1011-1020",
    "source": "PubMed",
    "title": "Critical assessment of methods of protein structure prediction (CASP)-Round XIII",
    "volume": "87",
    "author": [
      {
        "family": "Kryshtafovych",
        "given": "Andriy"
      },
      {
        "family": "Schwede",
        "given": "Torsten"
      },
      {
        "family": "Topf",
        "given": "Maya"
      },
      {
        "family": "Fidelis",
        "given": "Krzysztof"
      },
      {
        "family": "Moult",
        "given": "John"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2019",
          12
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/31589781"
  },
  {
    "id": "XD0xBDxb",
    "type": "article-journal",
    "abstract": "Typical personal medical data contains sensitive information about individuals. Storing or sharing the personal medical data is thus often risky. For example, a short DNA sequence can provide information that can identify not only an individual, but also his or her relatives. Nonetheless, most countries and researchers agree on the necessity of collecting personal medical data. This stems from the fact that medical data, including genomic data, are an indispensable resource for further research and development regarding disease prevention and treatment. To prevent personal medical data from being misused, techniques to reliably preserve sensitive information should be developed for real world applications. In this paper, we propose a framework called anonymized generative adversarial networks (AnomiGAN), to preserve the privacy of personal medical data, while also maintaining high prediction performance. We compared our method to state-of-the-art techniques and observed that our method preserves the same level of privacy as differential privacy (DP) and provides better prediction results. We also observed that there is a trade-off between privacy and prediction results that depends on the degree of preservation of the original data. Here, we provide a mathematical overview of our proposed model and demonstrate its validation using UCI machine learning repository datasets in order to highlight its utility in practice. The code is available at https://github.com/hobae/AnomiGAN/.",
    "container-title": "Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing",
    "ISSN": "2335-6936",
    "journalAbbreviation": "Pac Symp Biocomput",
    "language": "eng",
    "note": "PMID: 31797628\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/31797628",
    "page": "563-574",
    "source": "PubMed",
    "title": "AnomiGAN: Generative Adversarial Networks for Anonymizing Private Medical Data",
    "title-short": "AnomiGAN",
    "volume": "25",
    "author": [
      {
        "family": "Bae",
        "given": "Ho"
      },
      {
        "family": "Jung",
        "given": "Dahuin"
      },
      {
        "family": "Choi",
        "given": "Hyun-Soo"
      },
      {
        "family": "Yoon",
        "given": "Sungroh"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2020"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/31797628"
  },
  {
    "id": "r9luxm5X",
    "type": "article-journal",
    "abstract": "An approach for performing multiple alignments of large numbers of amino acid or nucleotide sequences is described. The method is based on first deriving a phylogenetic tree from a matrix of all pairwise sequence similarity scores, obtained using a fast pairwise alignment algorithm. Then the multiple alignment is achieved from a series of pairwise alignments of clusters of sequences, following the order of branching in the tree. The method is sufficiently fast and economical with memory to be easily implemented on a microcomputer, and yet the results obtained are comparable to those from packages requiring mainframe computer facilities.",
    "container-title": "Gene",
    "DOI": "10.1016/0378-1119(88)90330-7",
    "ISSN": "0378-1119",
    "issue": "1",
    "journalAbbreviation": "Gene",
    "language": "eng",
    "note": "PMID: 3243435\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/3243435",
    "page": "237-244",
    "source": "PubMed",
    "title": "CLUSTAL: a package for performing multiple sequence alignment on a microcomputer",
    "title-short": "CLUSTAL",
    "volume": "73",
    "author": [
      {
        "family": "Higgins",
        "given": "D. G."
      },
      {
        "family": "Sharp",
        "given": "P. M."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1988",
          12,
          15
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/3243435"
  },
  {
    "id": "1Fjz4nRHW",
    "type": "article-journal",
    "abstract": "MOTIVATION: Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.\nRESULTS: To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.\nAVAILABILITY AND IMPLEMENTATION: The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
    "container-title": "Bioinformatics (Oxford, England)",
    "DOI": "10.1093/bioinformatics/btab083",
    "ISSN": "1367-4811",
    "issue": "15",
    "journalAbbreviation": "Bioinformatics",
    "language": "eng",
    "note": "PMID: 33538820\nPMCID: PMC11025658\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/33538820",
    "page": "2112-2120",
    "source": "PubMed",
    "title": "DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome",
    "title-short": "DNABERT",
    "volume": "37",
    "author": [
      {
        "family": "Ji",
        "given": "Yanrong"
      },
      {
        "family": "Zhou",
        "given": "Zhihan"
      },
      {
        "family": "Liu",
        "given": "Han"
      },
      {
        "family": "Davuluri",
        "given": "Ramana V."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2021",
          8,
          9
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/33538820"
  },
  {
    "id": "19IrNgQEN",
    "type": "article-journal",
    "abstract": "Generative models have shown breakthroughs in a wide spectrum of domains due to recent advancements in machine learning algorithms and increased computational power. Despite these impressive achievements, the ability of generative models to create realistic synthetic data is still under-exploited in genetics and absent from population genetics. Yet a known limitation in the field is the reduced access to many genetic databases due to concerns about violations of individual privacy, although they would provide a rich resource for data mining and integration towards advancing genetic studies. In this study, we demonstrated that deep generative adversarial networks (GANs) and restricted Boltzmann machines (RBMs) can be trained to learn the complex distributions of real genomic datasets and generate novel high-quality artificial genomes (AGs) with none to little privacy loss. We show that our generated AGs replicate characteristics of the source dataset such as allele frequencies, linkage disequilibrium, pairwise haplotype distances and population structure. Moreover, they can also inherit complex features such as signals of selection. To illustrate the promising outcomes of our method, we showed that imputation quality for low frequency alleles can be improved by data augmentation to reference panels with AGs and that the RBM latent space provides a relevant encoding of the data, hence allowing further exploration of the reference dataset and features for solving supervised tasks. Generative models and AGs have the potential to become valuable assets in genetic studies by providing a rich yet compact representation of existing genomes and high-quality, easy-access and anonymous alternatives for private databases.",
    "container-title": "PLoS genetics",
    "DOI": "10.1371/journal.pgen.1009303",
    "ISSN": "1553-7404",
    "issue": "2",
    "journalAbbreviation": "PLoS Genet",
    "language": "eng",
    "note": "PMID: 33539374\nPMCID: PMC7861435\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/33539374",
    "page": "e1009303",
    "source": "PubMed",
    "title": "Creating artificial human genomes using generative neural networks",
    "volume": "17",
    "author": [
      {
        "family": "Yelmen",
        "given": "Burak"
      },
      {
        "family": "Decelle",
        "given": "Aurélien"
      },
      {
        "family": "Ongaro",
        "given": "Linda"
      },
      {
        "family": "Marnetto",
        "given": "Davide"
      },
      {
        "family": "Tallec",
        "given": "Corentin"
      },
      {
        "family": "Montinaro",
        "given": "Francesco"
      },
      {
        "family": "Furtlehner",
        "given": "Cyril"
      },
      {
        "family": "Pagani",
        "given": "Luca"
      },
      {
        "family": "Jay",
        "given": "Flora"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2021",
          2
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/33539374"
  },
  {
    "id": "aTuvh2K",
    "type": "article-journal",
    "abstract": "DeepMind presented notably accurate predictions at the recent 14th Critical Assessment of Structure Prediction (CASP14) conference. We explored network architectures that incorporate related ideas and obtained the best performance with a three-track network in which information at the one-dimensional (1D) sequence level, the 2D distance map level, and the 3D coordinate level is successively transformed and integrated. The three-track network produces structure predictions with accuracies approaching those of DeepMind in CASP14, enables the rapid solution of challenging x-ray crystallography and cryo-electron microscopy structure modeling problems, and provides insights into the functions of proteins of currently unknown structure. The network also enables rapid generation of accurate protein-protein complex models from sequence information alone, short-circuiting traditional approaches that require modeling of individual subunits followed by docking. We make the method available to the scientific community to speed biological research.",
    "container-title": "Science (New York, N.Y.)",
    "DOI": "10.1126/science.abj8754",
    "ISSN": "1095-9203",
    "issue": "6557",
    "journalAbbreviation": "Science",
    "language": "eng",
    "note": "PMID: 34282049\nPMCID: PMC7612213\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/34282049",
    "page": "871-876",
    "source": "PubMed",
    "title": "Accurate prediction of protein structures and interactions using a three-track neural network",
    "volume": "373",
    "author": [
      {
        "family": "Baek",
        "given": "Minkyung"
      },
      {
        "family": "DiMaio",
        "given": "Frank"
      },
      {
        "family": "Anishchenko",
        "given": "Ivan"
      },
      {
        "family": "Dauparas",
        "given": "Justas"
      },
      {
        "family": "Ovchinnikov",
        "given": "Sergey"
      },
      {
        "family": "Lee",
        "given": "Gyu Rie"
      },
      {
        "family": "Wang",
        "given": "Jue"
      },
      {
        "family": "Cong",
        "given": "Qian"
      },
      {
        "family": "Kinch",
        "given": "Lisa N."
      },
      {
        "family": "Schaeffer",
        "given": "R. Dustin"
      },
      {
        "family": "Millán",
        "given": "Claudia"
      },
      {
        "family": "Park",
        "given": "Hahnbeom"
      },
      {
        "family": "Adams",
        "given": "Carson"
      },
      {
        "family": "Glassman",
        "given": "Caleb R."
      },
      {
        "family": "DeGiovanni",
        "given": "Andy"
      },
      {
        "family": "Pereira",
        "given": "Jose H."
      },
      {
        "family": "Rodrigues",
        "given": "Andria V."
      },
      {
        "family": "Dijk",
        "given": "Alberdina A.",
        "non-dropping-particle": "van"
      },
      {
        "family": "Ebrecht",
        "given": "Ana C."
      },
      {
        "family": "Opperman",
        "given": "Diederik J."
      },
      {
        "family": "Sagmeister",
        "given": "Theo"
      },
      {
        "family": "Buhlheller",
        "given": "Christoph"
      },
      {
        "family": "Pavkov-Keller",
        "given": "Tea"
      },
      {
        "family": "Rathinaswamy",
        "given": "Manoj K."
      },
      {
        "family": "Dalwadi",
        "given": "Udit"
      },
      {
        "family": "Yip",
        "given": "Calvin K."
      },
      {
        "family": "Burke",
        "given": "John E."
      },
      {
        "family": "Garcia",
        "given": "K. Christopher"
      },
      {
        "family": "Grishin",
        "given": "Nick V."
      },
      {
        "family": "Adams",
        "given": "Paul D."
      },
      {
        "family": "Read",
        "given": "Randy J."
      },
      {
        "family": "Baker",
        "given": "David"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2021",
          8,
          20
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/34282049"
  },
  {
    "id": "oDgLqpUO",
    "type": "article-journal",
    "abstract": "In order to uncover the meanings of 'book of life', 155 different biological language models (BLMs) for DNA, RNA and protein sequence analysis are discussed in this study, which are able to extract the linguistic properties of 'book of life'. We also extend the BLMs into a system called BioSeq-BLM for automatically representing and analyzing the sequence data. Experimental results show that the predictors generated by BioSeq-BLM achieve comparable or even obviously better performance than the exiting state-of-the-art predictors published in literatures, indicating that BioSeq-BLM will provide new approaches for biological sequence analysis based on natural language processing technologies, and contribute to the development of this very important field. In order to help the readers to use BioSeq-BLM for their own experiments, the corresponding web server and stand-alone package are established and released, which can be freely accessed at http://bliulab.net/BioSeq-BLM/.",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/gkab829",
    "ISSN": "1362-4962",
    "issue": "22",
    "journalAbbreviation": "Nucleic Acids Res",
    "language": "eng",
    "note": "PMID: 34581805\nPMCID: PMC8682797\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/34581805",
    "page": "e129",
    "source": "PubMed",
    "title": "BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models",
    "title-short": "BioSeq-BLM",
    "volume": "49",
    "author": [
      {
        "family": "Li",
        "given": "Hong-Liang"
      },
      {
        "family": "Pang",
        "given": "Yi-He"
      },
      {
        "family": "Liu",
        "given": "Bin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2021",
          12,
          16
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/34581805"
  },
  {
    "id": "16YP71KXq",
    "type": "article-journal",
    "abstract": "How noncoding DNA determines gene expression in different cell types is a major unsolved problem, and critical downstream applications in human genetics depend on improved solutions. Here, we report substantially improved gene expression prediction accuracy from DNA sequences through the use of a deep learning architecture, called Enformer, that is able to integrate information from long-range interactions (up to 100 kb away) in the genome. This improvement yielded more accurate variant effect predictions on gene expression for both natural genetic variants and saturation mutagenesis measured by massively parallel reporter assays. Furthermore, Enformer learned to predict enhancer-promoter interactions directly from the DNA sequence competitively with methods that take direct experimental data as input. We expect that these advances will enable more effective fine-mapping of human disease associations and provide a framework to interpret cis-regulatory evolution.",
    "container-title": "Nature Methods",
    "DOI": "10.1038/s41592-021-01252-x",
    "ISSN": "1548-7105",
    "issue": "10",
    "journalAbbreviation": "Nat Methods",
    "language": "eng",
    "note": "PMID: 34608324\nPMCID: PMC8490152\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/34608324",
    "page": "1196-1203",
    "source": "PubMed",
    "title": "Effective gene expression prediction from sequence by integrating long-range interactions",
    "volume": "18",
    "author": [
      {
        "family": "Avsec",
        "given": "Žiga"
      },
      {
        "family": "Agarwal",
        "given": "Vikram"
      },
      {
        "family": "Visentin",
        "given": "Daniel"
      },
      {
        "family": "Ledsam",
        "given": "Joseph R."
      },
      {
        "family": "Grabska-Barwinska",
        "given": "Agnieszka"
      },
      {
        "family": "Taylor",
        "given": "Kyle R."
      },
      {
        "family": "Assael",
        "given": "Yannis"
      },
      {
        "family": "Jumper",
        "given": "John"
      },
      {
        "family": "Kohli",
        "given": "Pushmeet"
      },
      {
        "family": "Kelley",
        "given": "David R."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2021",
          10
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/34608324"
  },
  {
    "id": "8pwrkuN4",
    "type": "article-journal",
    "abstract": "To accelerate biomedical research process, deep-learning systems are developed to automatically acquire knowledge about molecule entities by reading large-scale biomedical data. Inspired by humans that learn deep molecule knowledge from versatile reading on both molecule structure and biomedical text information, we propose a knowledgeable machine reading system that bridges both types of information in a unified deep-learning framework for comprehensive biomedical research assistance. We solve the problem that existing machine reading models can only process different types of data separately, and thus achieve a comprehensive and thorough understanding of molecule entities. By grasping meta-knowledge in an unsupervised fashion within and across different information sources, our system can facilitate various real-world biomedical applications, including molecular property prediction, biomedical relation extraction and so on. Experimental results show that our system even surpasses human professionals in the capability of molecular property comprehension, and also reveal its promising potential in facilitating automatic drug discovery and documentation in the future.",
    "container-title": "Nature Communications",
    "DOI": "10.1038/s41467-022-28494-3",
    "ISSN": "2041-1723",
    "issue": "1",
    "journalAbbreviation": "Nat Commun",
    "language": "eng",
    "note": "PMID: 35165275\nPMCID: PMC8844428\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/35165275",
    "page": "862",
    "source": "PubMed",
    "title": "A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals",
    "volume": "13",
    "author": [
      {
        "family": "Zeng",
        "given": "Zheni"
      },
      {
        "family": "Yao",
        "given": "Yuan"
      },
      {
        "family": "Liu",
        "given": "Zhiyuan"
      },
      {
        "family": "Sun",
        "given": "Maosong"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022",
          2,
          14
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/35165275"
  },
  {
    "id": "18PNFVXMG",
    "type": "article-journal",
    "abstract": "Proteins perform many essential functions in biological systems and can be successfully developed as bio-therapeutics. It is invaluable to be able to predict their properties based on a proposed sequence and structure. In this study, we developed a novel generalizable deep learning framework, LM-GVP, composed of a protein Language Model (LM) and Graph Neural Network (GNN) to leverage information from both 1D amino acid sequences and 3D structures of proteins. Our approach outperformed the state-of-the-art protein LMs on a variety of property prediction tasks including fluorescence, protease stability, and protein functions from Gene Ontology (GO). We also illustrated insights into how a GNN prediction head can inform the fine-tuning of protein LMs to better leverage structural information. We envision that our deep learning framework will be generalizable to many protein property prediction problems to greatly accelerate protein engineering and drug development.",
    "container-title": "Scientific Reports",
    "DOI": "10.1038/s41598-022-10775-y",
    "ISSN": "2045-2322",
    "issue": "1",
    "journalAbbreviation": "Sci Rep",
    "language": "eng",
    "note": "PMID: 35477726\nPMCID: PMC9046255\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/35477726",
    "page": "6832",
    "source": "PubMed",
    "title": "LM-GVP: an extensible sequence and structure informed deep learning framework for protein property prediction",
    "title-short": "LM-GVP",
    "volume": "12",
    "author": [
      {
        "family": "Wang",
        "given": "Zichen"
      },
      {
        "family": "Combs",
        "given": "Steven A."
      },
      {
        "family": "Brand",
        "given": "Ryan"
      },
      {
        "family": "Calvo",
        "given": "Miguel Romero"
      },
      {
        "family": "Xu",
        "given": "Panpan"
      },
      {
        "family": "Price",
        "given": "George"
      },
      {
        "family": "Golovach",
        "given": "Nataliya"
      },
      {
        "family": "Salawu",
        "given": "Emmanuel O."
      },
      {
        "family": "Wise",
        "given": "Colby J."
      },
      {
        "family": "Ponnapalli",
        "given": "Sri Priya"
      },
      {
        "family": "Clark",
        "given": "Peter M."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022",
          4,
          27
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/35477726"
  },
  {
    "id": "7ikCKv12",
    "type": "article-journal",
    "abstract": "Natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. In this paper, we first distinguish four phases by discussing different levels of NLP and components of Natural Language Generation followed by presenting the history and evolution of NLP. We then discuss in detail the state of the art presenting the various applications of NLP, current trends, and challenges. Finally, we present a discussion on some available datasets, models, and evaluation metrics in NLP.",
    "container-title": "Multimedia Tools and Applications",
    "DOI": "10.1007/s11042-022-13428-4",
    "ISSN": "1380-7501",
    "issue": "3",
    "journalAbbreviation": "Multimed Tools Appl",
    "language": "eng",
    "note": "PMID: 35855771\nPMCID: PMC9281254\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/35855771",
    "page": "3713-3744",
    "source": "PubMed",
    "title": "Natural language processing: state of the art, current trends and challenges",
    "title-short": "Natural language processing",
    "volume": "82",
    "author": [
      {
        "family": "Khurana",
        "given": "Diksha"
      },
      {
        "family": "Koli",
        "given": "Aditya"
      },
      {
        "family": "Khatter",
        "given": "Kiran"
      },
      {
        "family": "Singh",
        "given": "Sukhdev"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/35855771"
  },
  {
    "id": "Ar0akTY8",
    "type": "article-journal",
    "abstract": "Although deep learning has revolutionized protein structure prediction, almost all experimentally characterized de novo protein designs have been generated using physically based approaches such as Rosetta. Here, we describe a deep learning-based protein sequence design method, ProteinMPNN, that has outstanding performance in both in silico and experimental tests. On native protein backbones, ProteinMPNN has a sequence recovery of 52.4% compared with 32.9% for Rosetta. The amino acid sequence at different positions can be coupled between single or multiple chains, enabling application to a wide range of current protein design challenges. We demonstrate the broad utility and high accuracy of ProteinMPNN using x-ray crystallography, cryo-electron microscopy, and functional studies by rescuing previously failed designs, which were made using Rosetta or AlphaFold, of protein monomers, cyclic homo-oligomers, tetrahedral nanoparticles, and target-binding proteins.",
    "container-title": "Science (New York, N.Y.)",
    "DOI": "10.1126/science.add2187",
    "ISSN": "1095-9203",
    "issue": "6615",
    "journalAbbreviation": "Science",
    "language": "eng",
    "note": "PMID: 36108050\nPMCID: PMC9997061\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/36108050",
    "page": "49-56",
    "source": "PubMed",
    "title": "Robust deep learning-based protein sequence design using ProteinMPNN",
    "volume": "378",
    "author": [
      {
        "family": "Dauparas",
        "given": "J."
      },
      {
        "family": "Anishchenko",
        "given": "I."
      },
      {
        "family": "Bennett",
        "given": "N."
      },
      {
        "family": "Bai",
        "given": "H."
      },
      {
        "family": "Ragotte",
        "given": "R. J."
      },
      {
        "family": "Milles",
        "given": "L. F."
      },
      {
        "family": "Wicky",
        "given": "B. I. M."
      },
      {
        "family": "Courbet",
        "given": "A."
      },
      {
        "family": "Haas",
        "given": "R. J.",
        "non-dropping-particle": "de"
      },
      {
        "family": "Bethel",
        "given": "N."
      },
      {
        "family": "Leung",
        "given": "P. J. Y."
      },
      {
        "family": "Huddy",
        "given": "T. F."
      },
      {
        "family": "Pellock",
        "given": "S."
      },
      {
        "family": "Tischer",
        "given": "D."
      },
      {
        "family": "Chan",
        "given": "F."
      },
      {
        "family": "Koepnick",
        "given": "B."
      },
      {
        "family": "Nguyen",
        "given": "H."
      },
      {
        "family": "Kang",
        "given": "A."
      },
      {
        "family": "Sankaran",
        "given": "B."
      },
      {
        "family": "Bera",
        "given": "A. K."
      },
      {
        "family": "King",
        "given": "N. P."
      },
      {
        "family": "Baker",
        "given": "D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022",
          10,
          7
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/36108050"
  },
  {
    "id": "iU5gKzdn",
    "type": "article-journal",
    "abstract": "MOTIVATION: The development of novel compounds targeting proteins of interest is one of the most important tasks in the pharmaceutical industry. Deep generative models have been applied to targeted molecular design and have shown promising results. Recently, target-specific molecule generation has been viewed as a translation between the protein language and the chemical language. However, such a model is limited by the availability of interacting protein-ligand pairs. On the other hand, large amounts of unlabelled protein sequences and chemical compounds are available and have been used to train language models that learn useful representations. In this study, we propose exploiting pretrained biochemical language models to initialize (i.e. warm start) targeted molecule generation models. We investigate two warm start strategies: (i) a one-stage strategy where the initialized model is trained on targeted molecule generation and (ii) a two-stage strategy containing a pre-finetuning on molecular generation followed by target-specific training. We also compare two decoding strategies to generate compounds: beam search and sampling.\nRESULTS: The results show that the warm-started models perform better than a baseline model trained from scratch. The two proposed warm-start strategies achieve similar results to each other with respect to widely used metrics from benchmarks. However, docking evaluation of the generated compounds for a number of novel proteins suggests that the one-stage strategy generalizes better than the two-stage strategy. Additionally, we observe that beam search outperforms sampling in both docking evaluation and benchmark metrics for assessing compound quality.\nAVAILABILITY AND IMPLEMENTATION: The source code is available at https://github.com/boun-tabi/biochemical-lms-for-drug-design and the materials (i.e., data, models, and outputs) are archived in Zenodo at https://doi.org/10.5281/zenodo.6832145.\nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.",
    "container-title": "Bioinformatics (Oxford, England)",
    "DOI": "10.1093/bioinformatics/btac482",
    "ISSN": "1367-4811",
    "issue": "Suppl_2",
    "journalAbbreviation": "Bioinformatics",
    "language": "eng",
    "note": "PMID: 36124801\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/36124801",
    "page": "ii155-ii161",
    "source": "PubMed",
    "title": "Exploiting pretrained biochemical language models for targeted drug design",
    "volume": "38",
    "author": [
      {
        "family": "Uludoğan",
        "given": "Gökçe"
      },
      {
        "family": "Ozkirimli",
        "given": "Elif"
      },
      {
        "family": "Ulgen",
        "given": "Kutlu O."
      },
      {
        "family": "Karalı",
        "given": "Nilgün"
      },
      {
        "family": "Özgür",
        "given": "Arzucan"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2022",
          9,
          16
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/36124801"
  },
  {
    "id": "GcVrid39",
    "type": "article-journal",
    "abstract": "The DNA-protein binding plays a pivotal role in regulating gene expression and evolution, and computational identification of DNA-protein has drawn more and more attention in bioinformatics. Recently, variants of BERT are also used to capture the semantic information of DNA sequences for predicting DNA-protein bindings. In this study, we leverage a task-specific pre-training strategy on BERT using large-scale multi-source DNA-protein binding data and present TFBert. TFBert treats DNA sequences as natural sentences and k-mer nucleotides as words. It can effectively extract upstream and downstream nucleotide context information by pre-training the 690 unlabeled ChIP-seq datasets. Experiments show that the pre-trained model can achieve promising performance on every single dataset in the 690 ChIP-seq datasets after simple fine tuning, especially on small datasets. The average AUC is 94.7%, outperforming existing popular methods. In conclusion, this study provides a variant of BERT based on pre-training and achieved state-of-the-art results in predicting DNA-protein bindings. We believe that TFBert can provide insights into other biological sequence classification problems.",
    "container-title": "Interdisciplinary Sciences, Computational Life Sciences",
    "DOI": "10.1007/s12539-022-00537-9",
    "ISSN": "1867-1462",
    "issue": "1",
    "journalAbbreviation": "Interdiscip Sci",
    "language": "eng",
    "note": "PMID: 36136096\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/36136096",
    "page": "32-43",
    "source": "PubMed",
    "title": "Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training",
    "volume": "15",
    "author": [
      {
        "family": "Luo",
        "given": "Hanyu"
      },
      {
        "family": "Shan",
        "given": "Wenyu"
      },
      {
        "family": "Chen",
        "given": "Cheng"
      },
      {
        "family": "Ding",
        "given": "Pingjian"
      },
      {
        "family": "Luo",
        "given": "Lingyun"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          3
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/36136096"
  },
  {
    "id": "tqicyXuM",
    "type": "article-journal",
    "abstract": "Recent advances in machine learning have leveraged evolutionary information in multiple sequence alignments to predict protein structure. We demonstrate direct inference of full atomic-level protein structure from primary sequence using a large language model. As language models of protein sequences are scaled up to 15 billion parameters, an atomic-resolution picture of protein structure emerges in the learned representations. This results in an order-of-magnitude acceleration of high-resolution structure prediction, which enables large-scale structural characterization of metagenomic proteins. We apply this capability to construct the ESM Metagenomic Atlas by predicting structures for >617 million metagenomic protein sequences, including >225 million that are predicted with high confidence, which gives a view into the vast breadth and diversity of natural proteins.",
    "container-title": "Science (New York, N.Y.)",
    "DOI": "10.1126/science.ade2574",
    "ISSN": "1095-9203",
    "issue": "6637",
    "journalAbbreviation": "Science",
    "language": "eng",
    "note": "PMID: 36927031\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/36927031",
    "page": "1123-1130",
    "source": "PubMed",
    "title": "Evolutionary-scale prediction of atomic-level protein structure with a language model",
    "volume": "379",
    "author": [
      {
        "family": "Lin",
        "given": "Zeming"
      },
      {
        "family": "Akin",
        "given": "Halil"
      },
      {
        "family": "Rao",
        "given": "Roshan"
      },
      {
        "family": "Hie",
        "given": "Brian"
      },
      {
        "family": "Zhu",
        "given": "Zhongkai"
      },
      {
        "family": "Lu",
        "given": "Wenting"
      },
      {
        "family": "Smetanin",
        "given": "Nikita"
      },
      {
        "family": "Verkuil",
        "given": "Robert"
      },
      {
        "family": "Kabeli",
        "given": "Ori"
      },
      {
        "family": "Shmueli",
        "given": "Yaniv"
      },
      {
        "family": "Dos Santos Costa",
        "given": "Allan"
      },
      {
        "family": "Fazel-Zarandi",
        "given": "Maryam"
      },
      {
        "family": "Sercu",
        "given": "Tom"
      },
      {
        "family": "Candido",
        "given": "Salvatore"
      },
      {
        "family": "Rives",
        "given": "Alexander"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          3,
          17
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/36927031"
  },
  {
    "id": "TKv5YqOB",
    "type": "article-journal",
    "abstract": "Large language models (LLMs) can respond to free-text queries without being specifically trained in the task in question, causing excitement and concern about their use in healthcare settings. ChatGPT is a generative artificial intelligence (AI) chatbot produced through sophisticated fine-tuning of an LLM, and other tools are emerging through similar developmental processes. Here we outline how LLM applications such as ChatGPT are developed, and we discuss how they are being leveraged in clinical settings. We consider the strengths and limitations of LLMs and their potential to improve the efficiency and effectiveness of clinical, educational and research work in medicine. LLM chatbots have already been deployed in a range of biomedical contexts, with impressive but mixed results. This review acts as a primer for interested clinicians, who will determine if and how LLM technology is used in healthcare for the benefit of patients and practitioners.",
    "container-title": "Nature Medicine",
    "DOI": "10.1038/s41591-023-02448-8",
    "ISSN": "1546-170X",
    "issue": "8",
    "journalAbbreviation": "Nat Med",
    "language": "eng",
    "note": "PMID: 37460753\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/37460753",
    "page": "1930-1940",
    "source": "PubMed",
    "title": "Large language models in medicine",
    "volume": "29",
    "author": [
      {
        "family": "Thirunavukarasu",
        "given": "Arun James"
      },
      {
        "family": "Ting",
        "given": "Darren Shu Jeng"
      },
      {
        "family": "Elangovan",
        "given": "Kabilan"
      },
      {
        "family": "Gutierrez",
        "given": "Laura"
      },
      {
        "family": "Tan",
        "given": "Ting Fang"
      },
      {
        "family": "Ting",
        "given": "Daniel Shu Wei"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          8
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/37460753"
  },
  {
    "id": "Q6HnLWXu",
    "type": "article-journal",
    "abstract": "BACKGROUND: Artificial intelligence (AI), machine learning and deep learning (including generative AI) are increasingly being investigated in the context of research and management of human infection.\nOBJECTIVES: We summarise recent and potential future applications of AI and its relevance to clinical infection practice.\nMETHODS: 1617 PubMed results were screened, with priority given to clinical trials, systematic reviews and meta-analyses. This narrative review focusses on studies using prospectively collected real-world data with clinical validation, and on research with translational potential, such as novel drug discovery and microbiome-based interventions.\nRESULTS: There is some evidence of clinical utility of AI applied to laboratory diagnostics (e.g. digital culture plate reading, malaria diagnosis, antimicrobial resistance profiling), clinical imaging analysis (e.g. pulmonary tuberculosis diagnosis), clinical decision support tools (e.g. sepsis prediction, antimicrobial prescribing) and public health outbreak management (e.g. COVID-19). Most studies to date lack any real-world validation or clinical utility metrics. Significant heterogeneity in study design and reporting limits comparability. Many practical and ethical issues exist, including algorithm transparency and risk of bias.\nCONCLUSIONS: Interest in and development of AI-based tools for infection research and management are undoubtedly gaining pace, although the real-world clinical utility to date appears much more modest.",
    "container-title": "The Journal of Infection",
    "DOI": "10.1016/j.jinf.2023.07.006",
    "ISSN": "1532-2742",
    "issue": "4",
    "journalAbbreviation": "J Infect",
    "language": "eng",
    "note": "PMID: 37468046\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/37468046",
    "page": "287-294",
    "source": "PubMed",
    "title": "Artificial intelligence, machine learning and deep learning: Potential resources for the infection clinician",
    "title-short": "Artificial intelligence, machine learning and deep learning",
    "volume": "87",
    "author": [
      {
        "family": "Theodosiou",
        "given": "Anastasia A."
      },
      {
        "family": "Read",
        "given": "Robert C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          10
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/37468046"
  },
  {
    "id": "5C9SlgjM",
    "type": "article-journal",
    "abstract": "To replicate inside macrophages and cause tuberculosis, Mycobacterium tuberculosis must scavenge a variety of nutrients from the host1,2. The mammalian cell entry (MCE) proteins are important virulence factors in M. tuberculosis1,3, where they are encoded by large gene clusters and have been implicated in the transport of fatty acids4-7 and cholesterol1,4,8 across the impermeable mycobacterial cell envelope. Very little is known about how cargos are transported across this barrier, and it remains unclear how the approximately ten proteins encoded by a mycobacterial mce gene cluster assemble to transport cargo across the cell envelope. Here we report the cryo-electron microscopy (cryo-EM) structure of the endogenous Mce1 lipid-import machine of Mycobacterium smegmatis-a non-pathogenic relative of M. tuberculosis. The structure reveals how the proteins of the Mce1 system assemble to form an elongated ABC transporter complex that is long enough to span the cell envelope. The Mce1 complex is dominated by a curved, needle-like domain that appears to be unrelated to previously described protein structures, and creates a protected hydrophobic pathway for lipid transport across the periplasm. Our structural data revealed the presence of a subunit of the Mce1 complex, which we identified using a combination of cryo-EM and AlphaFold2, and name LucB. Our data lead to a structural model for Mce1-mediated lipid import across the mycobacterial cell envelope.",
    "container-title": "Nature",
    "DOI": "10.1038/s41586-023-06366-0",
    "ISSN": "1476-4687",
    "issue": "7973",
    "journalAbbreviation": "Nature",
    "language": "eng",
    "note": "PMID: 37495693\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/37495693",
    "page": "445-452",
    "source": "PubMed",
    "title": "Structure of an endogenous mycobacterial MCE lipid transporter",
    "volume": "620",
    "author": [
      {
        "family": "Chen",
        "given": "James"
      },
      {
        "family": "Fruhauf",
        "given": "Alice"
      },
      {
        "family": "Fan",
        "given": "Catherine"
      },
      {
        "family": "Ponce",
        "given": "Jackeline"
      },
      {
        "family": "Ueberheide",
        "given": "Beatrix"
      },
      {
        "family": "Bhabha",
        "given": "Gira"
      },
      {
        "family": "Ekiert",
        "given": "Damian C."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          8
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/37495693"
  },
  {
    "id": "FzEFtq9e",
    "type": "article-journal",
    "abstract": "MOTIVATION: In recent years, the development of natural language process (NLP) technologies and deep learning hardware has led to significant improvement in large language models (LLMs). The ChatGPT, the state-of-the-art LLM built on GPT-3.5 and GPT-4, shows excellent capabilities in general language understanding and reasoning. Researchers also tested the GPTs on a variety of NLP-related tasks and benchmarks and got excellent results. With exciting performance on daily chat, researchers began to explore the capacity of ChatGPT on expertise that requires professional education for human and we are interested in the biomedical domain.\nRESULTS: To evaluate the performance of ChatGPT on biomedical-related tasks, this article presents a comprehensive benchmark study on the use of ChatGPT for biomedical corpus, including article abstracts, clinical trials description, biomedical questions, and so on. Typical NLP tasks like named entity recognization, relation extraction, sentence similarity, question and answering, and document classification are included. Overall, ChatGPT got a BLURB score of 58.50 while the state-of-the-art model had a score of 84.30. Through a series of experiments, we demonstrated the effectiveness and versatility of ChatGPT in biomedical text understanding, reasoning and generation, and the limitation of ChatGPT build on GPT-3.5.\nAVAILABILITY AND IMPLEMENTATION: All the datasets are available from BLURB benchmark https://microsoft.github.io/BLURB/index.html. The prompts are described in the article.",
    "container-title": "Bioinformatics (Oxford, England)",
    "DOI": "10.1093/bioinformatics/btad557",
    "ISSN": "1367-4811",
    "issue": "9",
    "journalAbbreviation": "Bioinformatics",
    "language": "eng",
    "note": "PMID: 37682111\nPMCID: PMC10562950\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/37682111",
    "page": "btad557",
    "source": "PubMed",
    "title": "An extensive benchmark study on biomedical text generation and mining with ChatGPT",
    "volume": "39",
    "author": [
      {
        "family": "Chen",
        "given": "Qijie"
      },
      {
        "family": "Sun",
        "given": "Haotong"
      },
      {
        "family": "Liu",
        "given": "Haoyang"
      },
      {
        "family": "Jiang",
        "given": "Yinghui"
      },
      {
        "family": "Ran",
        "given": "Ting"
      },
      {
        "family": "Jin",
        "given": "Xurui"
      },
      {
        "family": "Xiao",
        "given": "Xianglu"
      },
      {
        "family": "Lin",
        "given": "Zhimin"
      },
      {
        "family": "Chen",
        "given": "Hongming"
      },
      {
        "family": "Niu",
        "given": "Zhangmin"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          9,
          2
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/37682111"
  },
  {
    "id": "Cicp6dTw",
    "type": "article-journal",
    "abstract": "Artificial intelligence (AI) techniques have the potential to revolutionize drug release modeling, optimize therapy for personalized medicine, and minimize side effects. By applying AI algorithms, researchers can predict drug release profiles, incorporate patient-specific factors, and optimize dosage regimens to achieve tailored and effective therapies. This AI-based approach has the potential to improve treatment outcomes, enhance patient satisfaction, and advance the field of pharmaceutical sciences. International collaborations and professional organizations play vital roles in establishing guidelines and best practices for data collection and sharing. Open data initiatives can enhance transparency and scientific progress, facilitating algorithm validation.",
    "container-title": "Cureus",
    "DOI": "10.7759/cureus.47486",
    "ISSN": "2168-8184",
    "issue": "10",
    "journalAbbreviation": "Cureus",
    "language": "eng",
    "note": "PMID: 37881323\nPMCID: PMC10597591\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/37881323",
    "page": "e47486",
    "source": "PubMed",
    "title": "Artificial Intelligence's Impact on Drug Discovery and Development From Bench to Bedside",
    "volume": "15",
    "author": [
      {
        "family": "Vidhya",
        "given": "K. S."
      },
      {
        "family": "Sultana",
        "given": "Ayesha"
      },
      {
        "family": "M",
        "given": "Naveen Kumar"
      },
      {
        "family": "Rangareddy",
        "given": "Harish"
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2023",
          10
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/37881323"
  },
  {
    "id": "XVMV1nZ",
    "type": "article-journal",
    "abstract": "The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2-6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein-ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein-nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody-antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.",
    "container-title": "Nature",
    "DOI": "10.1038/s41586-024-07487-w",
    "ISSN": "1476-4687",
    "issue": "8016",
    "journalAbbreviation": "Nature",
    "language": "eng",
    "note": "PMID: 38718835\nPMCID: PMC11168924\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/38718835",
    "page": "493-500",
    "source": "PubMed",
    "title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
    "volume": "630",
    "author": [
      {
        "family": "Abramson",
        "given": "Josh"
      },
      {
        "family": "Adler",
        "given": "Jonas"
      },
      {
        "family": "Dunger",
        "given": "Jack"
      },
      {
        "family": "Evans",
        "given": "Richard"
      },
      {
        "family": "Green",
        "given": "Tim"
      },
      {
        "family": "Pritzel",
        "given": "Alexander"
      },
      {
        "family": "Ronneberger",
        "given": "Olaf"
      },
      {
        "family": "Willmore",
        "given": "Lindsay"
      },
      {
        "family": "Ballard",
        "given": "Andrew J."
      },
      {
        "family": "Bambrick",
        "given": "Joshua"
      },
      {
        "family": "Bodenstein",
        "given": "Sebastian W."
      },
      {
        "family": "Evans",
        "given": "David A."
      },
      {
        "family": "Hung",
        "given": "Chia-Chun"
      },
      {
        "family": "O'Neill",
        "given": "Michael"
      },
      {
        "family": "Reiman",
        "given": "David"
      },
      {
        "family": "Tunyasuvunakool",
        "given": "Kathryn"
      },
      {
        "family": "Wu",
        "given": "Zachary"
      },
      {
        "family": "Žemgulytė",
        "given": "Akvilė"
      },
      {
        "family": "Arvaniti",
        "given": "Eirini"
      },
      {
        "family": "Beattie",
        "given": "Charles"
      },
      {
        "family": "Bertolli",
        "given": "Ottavia"
      },
      {
        "family": "Bridgland",
        "given": "Alex"
      },
      {
        "family": "Cherepanov",
        "given": "Alexey"
      },
      {
        "family": "Congreve",
        "given": "Miles"
      },
      {
        "family": "Cowen-Rivers",
        "given": "Alexander I."
      },
      {
        "family": "Cowie",
        "given": "Andrew"
      },
      {
        "family": "Figurnov",
        "given": "Michael"
      },
      {
        "family": "Fuchs",
        "given": "Fabian B."
      },
      {
        "family": "Gladman",
        "given": "Hannah"
      },
      {
        "family": "Jain",
        "given": "Rishub"
      },
      {
        "family": "Khan",
        "given": "Yousuf A."
      },
      {
        "family": "Low",
        "given": "Caroline M. R."
      },
      {
        "family": "Perlin",
        "given": "Kuba"
      },
      {
        "family": "Potapenko",
        "given": "Anna"
      },
      {
        "family": "Savy",
        "given": "Pascal"
      },
      {
        "family": "Singh",
        "given": "Sukhdeep"
      },
      {
        "family": "Stecula",
        "given": "Adrian"
      },
      {
        "family": "Thillaisundaram",
        "given": "Ashok"
      },
      {
        "family": "Tong",
        "given": "Catherine"
      },
      {
        "family": "Yakneen",
        "given": "Sergei"
      },
      {
        "family": "Zhong",
        "given": "Ellen D."
      },
      {
        "family": "Zielinski",
        "given": "Michal"
      },
      {
        "family": "Žídek",
        "given": "Augustin"
      },
      {
        "family": "Bapst",
        "given": "Victor"
      },
      {
        "family": "Kohli",
        "given": "Pushmeet"
      },
      {
        "family": "Jaderberg",
        "given": "Max"
      },
      {
        "family": "Hassabis",
        "given": "Demis"
      },
      {
        "family": "Jumper",
        "given": "John M."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "2024",
          6
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/38718835"
  },
  {
    "id": "gldEpQ9",
    "type": "article-journal",
    "container-title": "Science Progress",
    "ISSN": "0036-8504",
    "issue": "218",
    "journalAbbreviation": "Sci Prog",
    "language": "eng",
    "note": "PMID: 4859964\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/4859964",
    "page": "279-292",
    "source": "PubMed",
    "title": "Computers in the study of evolution",
    "volume": "55",
    "author": [
      {
        "family": "Crosby",
        "given": "J. L."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1967"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/4859964"
  },
  {
    "id": "C1oThCRN",
    "type": "article-journal",
    "container-title": "Journal of Molecular Biology",
    "DOI": "10.1016/0022-2836(70)90057-4",
    "ISSN": "0022-2836",
    "issue": "3",
    "journalAbbreviation": "J Mol Biol",
    "language": "eng",
    "note": "PMID: 5420325\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/5420325",
    "page": "443-453",
    "source": "PubMed",
    "title": "A general method applicable to the search for similarities in the amino acid sequence of two proteins",
    "volume": "48",
    "author": [
      {
        "family": "Needleman",
        "given": "S. B."
      },
      {
        "family": "Wunsch",
        "given": "C. D."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1970",
          3
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/5420325"
  },
  {
    "id": "dSNqAXK0",
    "type": "article-journal",
    "abstract": "A collection of PASCAL programs designed for the Apple II microcomputer is presented. These DNA sequence handling and analysis programs are interactive and may be used even by people with no computer experience. The package allows the user to enter a sequence from the keyboard, to modify it, to generate the reverse complement, to create new sequences from parts of other ones, to display or print sequences in various formats. Some analysis tasks are also performed: Translation, searches for restriction sites, for homology with subsequences, either perfect or with an adjustable match percentage. In addition, two programs are also included: The first one allows DNA data sequences generated with a BASIC program under the CP/M operating system to be used with these PASCAL programs. The second one is designed for the automatic assembly of DNA fragments sequences, obtained with the GILBERT-MAXAM or M13 techniques, into a complete sequence.",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/12.1part2.569",
    "ISSN": "0305-1048",
    "issue": "1 Pt 2",
    "journalAbbreviation": "Nucleic Acids Res",
    "language": "eng",
    "note": "PMID: 6320099\nPMCID: PMC321071\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/6320099",
    "page": "569-579",
    "source": "PubMed",
    "title": "Apple II PASCAL programs for molecular biologists",
    "volume": "12",
    "author": [
      {
        "family": "Malthiery",
        "given": "B."
      },
      {
        "family": "Bellon",
        "given": "B."
      },
      {
        "family": "Giorgi",
        "given": "D."
      },
      {
        "family": "Jacq",
        "given": "B."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1984",
          1,
          11
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/6320099"
  },
  {
    "id": "12FKql4zv",
    "type": "article-journal",
    "abstract": "The University of Wisconsin Genetics Computer Group (UWGCG) has been organized to develop computational tools for the analysis and publication of biological sequence data. A group of programs that will interact with each other has been developed for the Digital Equipment Corporation VAX computer using the VMS operating system. The programs available and the conditions for transfer are described.",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/12.1part1.387",
    "ISSN": "0305-1048",
    "issue": "1 Pt 1",
    "journalAbbreviation": "Nucleic Acids Res",
    "language": "eng",
    "note": "PMID: 6546423\nPMCID: PMC321012\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/6546423",
    "page": "387-395",
    "source": "PubMed",
    "title": "A comprehensive set of sequence analysis programs for the VAX",
    "volume": "12",
    "author": [
      {
        "family": "Devereux",
        "given": "J."
      },
      {
        "family": "Haeberli",
        "given": "P."
      },
      {
        "family": "Smithies",
        "given": "O."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1984",
          1,
          11
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/6546423"
  },
  {
    "id": "nzs4mjEF",
    "type": "article-journal",
    "abstract": "The application of maximum likelihood techniques to the estimation of evolutionary trees from nucleic acid sequence data is discussed. A computationally feasible method for finding such maximum likelihood estimates is developed, and a computer program is available. This method has advantages over the traditional parsimony algorithms, which can give misleading results if rates of evolution differ in different lineages. It also allows the testing of hypotheses about the constancy of evolutionary rates by likelihood ratio tests, and gives rough indication of the error of ;the estimate of the tree.",
    "container-title": "Journal of Molecular Evolution",
    "DOI": "10.1007/BF01734359",
    "ISSN": "0022-2844",
    "issue": "6",
    "journalAbbreviation": "J Mol Evol",
    "language": "eng",
    "note": "PMID: 7288891\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/7288891",
    "page": "368-376",
    "source": "PubMed",
    "title": "Evolutionary trees from DNA sequences: a maximum likelihood approach",
    "title-short": "Evolutionary trees from DNA sequences",
    "volume": "17",
    "author": [
      {
        "family": "Felsenstein",
        "given": "J."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1981"
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/7288891"
  },
  {
    "id": "11GoDnHp3",
    "type": "article-journal",
    "abstract": "An approach for genome analysis based on sequencing and assembly of unselected pieces of DNA from the whole chromosome has been applied to obtain the complete nucleotide sequence (1,830,137 base pairs) of the genome from the bacterium Haemophilus influenzae Rd. This approach eliminates the need for initial mapping efforts and is therefore applicable to the vast array of microbial species for which genome maps are unavailable. The H. influenzae Rd genome sequence (Genome Sequence DataBase accession number L42023) represents the only complete genome sequence from a free-living organism.",
    "container-title": "Science (New York, N.Y.)",
    "DOI": "10.1126/science.7542800",
    "ISSN": "0036-8075",
    "issue": "5223",
    "journalAbbreviation": "Science",
    "language": "eng",
    "note": "PMID: 7542800\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/7542800",
    "page": "496-512",
    "source": "PubMed",
    "title": "Whole-genome random sequencing and assembly of Haemophilus influenzae Rd",
    "volume": "269",
    "author": [
      {
        "family": "Fleischmann",
        "given": "R. D."
      },
      {
        "family": "Adams",
        "given": "M. D."
      },
      {
        "family": "White",
        "given": "O."
      },
      {
        "family": "Clayton",
        "given": "R. A."
      },
      {
        "family": "Kirkness",
        "given": "E. F."
      },
      {
        "family": "Kerlavage",
        "given": "A. R."
      },
      {
        "family": "Bult",
        "given": "C. J."
      },
      {
        "family": "Tomb",
        "given": "J. F."
      },
      {
        "family": "Dougherty",
        "given": "B. A."
      },
      {
        "family": "Merrick",
        "given": "J. M."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1995",
          7,
          28
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/7542800"
  },
  {
    "id": "i2bVokif",
    "type": "article-journal",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/21.13.2967",
    "ISSN": "0305-1048",
    "issue": "13",
    "journalAbbreviation": "Nucleic Acids Res",
    "language": "eng",
    "note": "PMID: 8332519\nPMCID: PMC309722\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/8332519",
    "page": "2967-2971",
    "source": "PubMed",
    "title": "The EMBL data library",
    "volume": "21",
    "author": [
      {
        "family": "Rice",
        "given": "C. M."
      },
      {
        "family": "Fuchs",
        "given": "R."
      },
      {
        "family": "Higgins",
        "given": "D. G."
      },
      {
        "family": "Stoehr",
        "given": "P. J."
      },
      {
        "family": "Cameron",
        "given": "G. N."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1993",
          7,
          1
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/8332519"
  },
  {
    "id": "1CARqSM3b",
    "type": "article-journal",
    "abstract": "A new method is presented for inferring evolutionary trees using nucleotide sequence data. The birth-death process is used as a model of speciation and extinction to specify the prior distribution of phylogenies and branching times. Nucleotide substitution is modeled by a continuous-time Markov process. Parameters of the branching model and the substitution model are estimated by maximum likelihood. The posterior probabilities of different phylogenies are calculated and the phylogeny with the highest posterior probability is chosen as the best estimate of the evolutionary relationship among species. We refer to this as the maximum posterior probability (MAP) tree. The posterior probability provides a natural measure of the reliability of the estimated phylogeny. Two example data sets are analyzed to infer the phylogenetic relationship of human, chimpanzee, gorilla, and orangutan. The best trees estimated by the new method are the same as those from the maximum likelihood analysis of separate topologies, but the posterior probabilities are quite different from the bootstrap proportions. The results of the method are found to be insensitive to changes in the rate parameter of the branching process.",
    "container-title": "Journal of Molecular Evolution",
    "DOI": "10.1007/BF02338839",
    "ISSN": "0022-2844",
    "issue": "3",
    "journalAbbreviation": "J Mol Evol",
    "language": "eng",
    "note": "PMID: 8703097\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/8703097",
    "page": "304-311",
    "source": "PubMed",
    "title": "Probability distribution of molecular evolutionary trees: a new method of phylogenetic inference",
    "title-short": "Probability distribution of molecular evolutionary trees",
    "volume": "43",
    "author": [
      {
        "family": "Rannala",
        "given": "B."
      },
      {
        "family": "Yang",
        "given": "Z."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1996",
          9
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/8703097"
  },
  {
    "id": "Ero6Muzo",
    "type": "article-journal",
    "abstract": "Sequencing of large clones or small genomes is generally done by the shotgun approach (Anderson et al. 1982). This has two phases: (1) a shotgun phase in which a number of reads are generated from random subclones and assembled into contigs, followed by (2) a directed, or finishing phase in which the assembly is inspected for correctness and for various kinds of data anomalies (such as contaminant reads, unremoved vector sequence, and chimeric or deleted reads), additional data are collected to close gaps and resolve low quality regions, and editing is performed to correct assembly or base-calling errors. Finishing is currently a bottleneck in large-scale sequencing efforts, and throughput gains will depend both on reducing the need for human intervention and making it as efficient as possible. We have developed a finishing tool, consed, which attempts to implement these principles. A distinguishing feature relative to other programs is the use of error probabilities from our programs phred and phrap as an objective criterion to guide the entire finishing process. More information is available at http:// www.genome.washington.edu/consed/consed. html.",
    "container-title": "Genome Research",
    "DOI": "10.1101/gr.8.3.195",
    "ISSN": "1088-9051",
    "issue": "3",
    "journalAbbreviation": "Genome Res",
    "language": "eng",
    "note": "PMID: 9521923\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubmed.ncbi.nlm.nih.gov/9521923",
    "page": "195-202",
    "source": "PubMed",
    "title": "Consed: a graphical tool for sequence finishing",
    "title-short": "Consed",
    "volume": "8",
    "author": [
      {
        "family": "Gordon",
        "given": "D."
      },
      {
        "family": "Abajian",
        "given": "C."
      },
      {
        "family": "Green",
        "given": "P."
      }
    ],
    "issued": {
      "date-parts": [
        [
          "1998",
          3
        ]
      ]
    },
    "URL": "https://pubmed.ncbi.nlm.nih.gov/9521923"
  },
  {
    "id": "1865CJ7gK",
    "type": "article-journal",
    "container-title": "Journal of Chemical Information and Modeling",
    "DOI": "10.1021/acs.jcim.1c00600",
    "ISSN": "1549-9596, 1549-960X",
    "issue": "9",
    "journalAbbreviation": "J. Chem. Inf. Model.",
    "language": "en",
    "page": "2064-2076",
    "source": "DOI.org (Crossref)",
    "title": "MolGPT: Molecular Generation Using a Transformer-Decoder Model",
    "title-short": "MolGPT",
    "URL": "https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600",
    "volume": "62",
    "author": [
      {
        "family": "Bagal",
        "given": "Viraj"
      },
      {
        "family": "Aggarwal",
        "given": "Rishal"
      },
      {
        "family": "Vinod",
        "given": "P. K."
      },
      {
        "family": "Priyakumar",
        "given": "U. Deva"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          5,
          9
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600"
  },
  {
    "id": "Ho9yh6th",
    "type": "article-journal",
    "container-title": "ACS Central Science",
    "DOI": "10.1021/acscentsci.9b00576",
    "ISSN": "2374-7943, 2374-7951",
    "issue": "9",
    "journalAbbreviation": "ACS Cent. Sci.",
    "language": "en",
    "page": "1572-1583",
    "source": "DOI.org (Crossref)",
    "title": "Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction",
    "title-short": "Molecular Transformer",
    "URL": "https://pubs.acs.org/doi/10.1021/acscentsci.9b00576",
    "volume": "5",
    "author": [
      {
        "family": "Schwaller",
        "given": "Philippe"
      },
      {
        "family": "Laino",
        "given": "Teodoro"
      },
      {
        "family": "Gaudin",
        "given": "Théophile"
      },
      {
        "family": "Bolgar",
        "given": "Peter"
      },
      {
        "family": "Hunter",
        "given": "Christopher A."
      },
      {
        "family": "Bekas",
        "given": "Costas"
      },
      {
        "family": "Lee",
        "given": "Alpha A."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          25
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://pubs.acs.org/doi/10.1021/acscentsci.9b00576"
  },
  {
    "id": "UK6rEYVY",
    "type": "article-journal",
    "abstract": "It is suggested that a system of chemical substances, called morphogens, reacting together and diffusing through a tissue, is adequate to account for the main phenomena of morphogenesis. Such a system, although it may originally be quite homogeneous, may later develop a pattern or structure due to an instability of the homogeneous equilibrium, which is triggered off by random disturbances. Such reaction-diffusion systems are considered in some detail in the case of an isolated ring of cells, a mathematically convenient, though biologically unusual system. The investigation is chiefly concerned with the onset of instability. It is found that there are six essentially different forms which this may take. In the most interesting form stationary waves appear on the ring. It is suggested that this might account, for instance, for the tentacle patterns on\r\n              Hydra\r\n              and for whorled leaves. A system of reactions and diffusion on a sphere is also considered. Such a system appears to account for gastrulation. Another reaction system in two dimensions gives rise to patterns reminiscent of dappling. It is also suggested that stationary waves in two dimensions could account for the phenomena of phyllotaxis. The purpose of this paper is to discuss a possible mechanism by which the genes of a zygote may determine the anatomical structure of the resulting organism. The theory does not make any new hypotheses; it merely suggests that certain well-known physical laws are sufficient to account for many of the facts. The full understanding of the paper requires a good knowledge of mathematics, some biology, and some elementary chemistry. Since readers cannot be expected to be experts in all of these subjects, a number of elementary facts are explained, which can be found in text-books, but whose omission would make the paper difficult reading.",
    "container-title": "Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences",
    "DOI": "10.1098/rstb.1952.0012",
    "ISSN": "2054-0280",
    "issue": "641",
    "journalAbbreviation": "Phil. Trans. R. Soc. Lond. B",
    "language": "en",
    "page": "37-72",
    "source": "DOI.org (Crossref)",
    "title": "The chemical basis of morphogenesis",
    "URL": "https://royalsocietypublishing.org/doi/10.1098/rstb.1952.0012",
    "volume": "237",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1952",
          8,
          14
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://royalsocietypublishing.org/doi/10.1098/rstb.1952.0012"
  },
  {
    "URL": "https://stacks.stanford.edu/file/druid:pj337tr4694/pj337tr4694.pdf",
    "type": "webpage",
    "id": "TjcFIKAL",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://stacks.stanford.edu/file/druid:pj337tr4694/pj337tr4694.pdf"
  },
  {
    "id": "IyaaHAPt",
    "type": "webpage",
    "abstract": "SuperGLUE is a new benchmark styled after original GLUE benchmark with a set of more difficult language understanding tasks, improved resources, and a new public leaderboard.",
    "container-title": "SuperGLUE Benchmark",
    "language": "en",
    "title": "SuperGLUE Benchmark",
    "URL": "https://super.gluebenchmark.com/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://super.gluebenchmark.com"
  },
  {
    "id": "ye7UmZPY",
    "type": "webpage",
    "abstract": "Discover the profound impact of Ethical AI on Life Sciences. Uncover ethical considerations, navigate healthcare ethics, and explore AI's transformative influence. Dive into life sciences' digital transformation amidst AI-driven challenges and ethical guidelines.",
    "language": "en",
    "title": "Ethical AI in Life Sciences: Impact & Guidelines",
    "title-short": "Ethical AI in Life Sciences",
    "URL": "https://www.aciinfotech.com/blogs/ethical-ai-in-life-sciences-impact-guidelines",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.aciinfotech.com/blogs/ethical-ai-in-life-sciences-impact-guidelines"
  },
  {
    "id": "iWYRgRGv",
    "type": "webpage",
    "abstract": "bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution",
    "title": "bioRxiv.org - the preprint server for Biology",
    "URL": "https://www.biorxiv.org/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org"
  },
  {
    "id": "mG5bRSDU",
    "type": "article",
    "abstract": "Recent breakthroughs have used deep learning to exploit evolutionary information in multiple sequence alignments (MSAs) to accurately predict protein structures. However, MSAs of homologous proteins are not always available, such as with orphan proteins or fast-evolving proteins like antibodies, and a protein typically folds in a natural setting from its primary amino acid sequence into its three-dimensional structure, suggesting that evolutionary information and MSAs should not be necessary to predict a protein’s folded form. Here, we introduce OmegaFold, the first computational method to successfully predict high-resolution protein structure from a single primary sequence alone. Using a new combination of a protein language model that allows us to make predictions from single sequences and a geometry-inspired transformer model trained on protein structures, OmegaFold outperforms RoseTTAFold and achieves similar prediction accuracy to AlphaFold2 on recently released structures. OmegaFold enables accurate predictions on orphan proteins that do not belong to any functionally characterized protein family and antibodies that tend to have noisy MSAs due to fast evolution. Our study fills a much-encountered gap in structure prediction and brings us a step closer to understanding protein folding in nature.",
    "DOI": "10.1101/2022.07.21.500999",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "High-resolution de novo structure prediction from primary sequence",
    "URL": "https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1",
    "author": [
      {
        "family": "Wu",
        "given": "Ruidong"
      },
      {
        "family": "Ding",
        "given": "Fan"
      },
      {
        "family": "Wang",
        "given": "Rui"
      },
      {
        "family": "Shen",
        "given": "Rui"
      },
      {
        "family": "Zhang",
        "given": "Xiwen"
      },
      {
        "family": "Luo",
        "given": "Shitong"
      },
      {
        "family": "Su",
        "given": "Chenpeng"
      },
      {
        "family": "Wu",
        "given": "Zuofan"
      },
      {
        "family": "Xie",
        "given": "Qi"
      },
      {
        "family": "Berger",
        "given": "Bonnie"
      },
      {
        "family": "Ma",
        "given": "Jianzhu"
      },
      {
        "family": "Peng",
        "given": "Jian"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          7,
          22
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1"
  },
  {
    "id": "6VSmIaGf",
    "type": "article",
    "abstract": "Our work seeks to transform how new and emergent variants of pandemic causing viruses, specially SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences, and then finetuning a SARS-CoV-2 specific model on 1.5 million genomes, we show that GenSLM can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLM represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate the scaling of GenSLMs on both GPU-based supercomputers and AI-hardware accelerators, achieving over 1.54 zettaflops in training runs. We present initial scientific insights gleaned from examining GenSLMs in tracking the evolutionary dynamics of SARS-CoV-2, noting that its full potential on large biological data is yet to be realized.",
    "DOI": "10.1101/2022.10.10.511571",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics",
    "title-short": "GenSLMs",
    "URL": "https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1",
    "author": [
      {
        "family": "Zvyagin",
        "given": "Maxim"
      },
      {
        "family": "Brace",
        "given": "Alexander"
      },
      {
        "family": "Hippe",
        "given": "Kyle"
      },
      {
        "family": "Deng",
        "given": "Yuntian"
      },
      {
        "family": "Zhang",
        "given": "Bin"
      },
      {
        "family": "Bohorquez",
        "given": "Cindy Orozco"
      },
      {
        "family": "Clyde",
        "given": "Austin"
      },
      {
        "family": "Kale",
        "given": "Bharat"
      },
      {
        "family": "Perez-Rivera",
        "given": "Danilo"
      },
      {
        "family": "Ma",
        "given": "Heng"
      },
      {
        "family": "Mann",
        "given": "Carla M."
      },
      {
        "family": "Irvin",
        "given": "Michael"
      },
      {
        "family": "Pauloski",
        "given": "J. Gregory"
      },
      {
        "family": "Ward",
        "given": "Logan"
      },
      {
        "family": "Hayot",
        "given": "Valerie"
      },
      {
        "family": "Emani",
        "given": "Murali"
      },
      {
        "family": "Foreman",
        "given": "Sam"
      },
      {
        "family": "Xie",
        "given": "Zhen"
      },
      {
        "family": "Lin",
        "given": "Diangen"
      },
      {
        "family": "Shukla",
        "given": "Maulik"
      },
      {
        "family": "Nie",
        "given": "Weili"
      },
      {
        "family": "Romero",
        "given": "Josh"
      },
      {
        "family": "Dallago",
        "given": "Christian"
      },
      {
        "family": "Vahdat",
        "given": "Arash"
      },
      {
        "family": "Xiao",
        "given": "Chaowei"
      },
      {
        "family": "Gibbs",
        "given": "Thomas"
      },
      {
        "family": "Foster",
        "given": "Ian"
      },
      {
        "family": "Davis",
        "given": "James J."
      },
      {
        "family": "Papka",
        "given": "Michael E."
      },
      {
        "family": "Brettin",
        "given": "Thomas"
      },
      {
        "family": "Stevens",
        "given": "Rick"
      },
      {
        "family": "Anandkumar",
        "given": "Anima"
      },
      {
        "family": "Vishwanath",
        "given": "Venkatram"
      },
      {
        "family": "Ramanathan",
        "given": "Arvind"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          10,
          11
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1"
  },
  {
    "id": "iwz5DRLF",
    "type": "article",
    "abstract": "There has been considerable recent progress in designing new proteins using deep learning methods1–9. Despite this progress, a general deep learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modeling but limited success when applied to protein modeling, likely due to the complexity of protein backbone geometry and sequence-structure relationships. Here we show that by fine tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding, and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold Diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of new designs. In a manner analogous to networks which produce images from user-specified inputs, RFdiffusion enables the design of diverse, complex, functional proteins from simple molecular specifications.",
    "DOI": "10.1101/2022.12.09.519842",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models",
    "URL": "https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1",
    "author": [
      {
        "family": "Watson",
        "given": "Joseph L."
      },
      {
        "family": "Juergens",
        "given": "David"
      },
      {
        "family": "Bennett",
        "given": "Nathaniel R."
      },
      {
        "family": "Trippe",
        "given": "Brian L."
      },
      {
        "family": "Yim",
        "given": "Jason"
      },
      {
        "family": "Eisenach",
        "given": "Helen E."
      },
      {
        "family": "Ahern",
        "given": "Woody"
      },
      {
        "family": "Borst",
        "given": "Andrew J."
      },
      {
        "family": "Ragotte",
        "given": "Robert J."
      },
      {
        "family": "Milles",
        "given": "Lukas F."
      },
      {
        "family": "Wicky",
        "given": "Basile I. M."
      },
      {
        "family": "Hanikel",
        "given": "Nikita"
      },
      {
        "family": "Pellock",
        "given": "Samuel J."
      },
      {
        "family": "Courbet",
        "given": "Alexis"
      },
      {
        "family": "Sheffler",
        "given": "William"
      },
      {
        "family": "Wang",
        "given": "Jue"
      },
      {
        "family": "Venkatesh",
        "given": "Preetham"
      },
      {
        "family": "Sappington",
        "given": "Isaac"
      },
      {
        "family": "Torres",
        "given": "Susana Vázquez"
      },
      {
        "family": "Lauko",
        "given": "Anna"
      },
      {
        "family": "Bortoli",
        "given": "Valentin De"
      },
      {
        "family": "Mathieu",
        "given": "Emile"
      },
      {
        "family": "Barzilay",
        "given": "Regina"
      },
      {
        "family": "Jaakkola",
        "given": "Tommi S."
      },
      {
        "family": "DiMaio",
        "given": "Frank"
      },
      {
        "family": "Baek",
        "given": "Minkyung"
      },
      {
        "family": "Baker",
        "given": "David"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          12,
          10
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1"
  },
  {
    "id": "34PAkn0s",
    "type": "article",
    "abstract": "Closing the gap between measurable genetic information and observable traits is a longstanding challenge in genomics. Yet, the prediction of molecular phenotypes from DNA sequences alone remains limited and inaccurate, often driven by the scarcity of annotated data and the inability to transfer learnings between prediction tasks. Here, we present an extensive study of foundation models pre-trained on DNA sequences, named the Nucleotide Transformer, integrating information from 3,202 diverse human genomes, as well as 850 genomes from a wide range of species, including model and non-model organisms. These transformer models yield transferable, context-specific representations of nucleotide sequences, which allow for accurate molecular phenotype prediction even in low-data settings. We show that the representations alone match or outperform specialized methods on 11 of 18 prediction tasks, and up to 15 after fine-tuning. Despite no supervision, the transformer models learnt to focus attention on key genomic elements, including those that regulate gene expression, such as enhancers. Lastly, we demonstrate that utilizing model representations alone can improve the prioritization of functional genetic variants. The training and application of foundational models in genomics explored in this study provide a widely applicable stepping stone to bridge the gap of accurate molecular phenotype prediction from DNA sequence alone.",
    "DOI": "10.1101/2023.01.11.523679",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics",
    "title-short": "The Nucleotide Transformer",
    "URL": "https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1",
    "author": [
      {
        "family": "Dalla-Torre",
        "given": "Hugo"
      },
      {
        "family": "Gonzalez",
        "given": "Liam"
      },
      {
        "family": "Revilla",
        "given": "Javier Mendoza"
      },
      {
        "family": "Carranza",
        "given": "Nicolas Lopez"
      },
      {
        "family": "Grzywaczewski",
        "given": "Adam Henryk"
      },
      {
        "family": "Oteri",
        "given": "Francesco"
      },
      {
        "family": "Dallago",
        "given": "Christian"
      },
      {
        "family": "Trop",
        "given": "Evan"
      },
      {
        "family": "Sirelkhatim",
        "given": "Hassan"
      },
      {
        "family": "Richard",
        "given": "Guillaume"
      },
      {
        "family": "Skwark",
        "given": "Marcin"
      },
      {
        "family": "Beguir",
        "given": "Karim"
      },
      {
        "family": "Lopez",
        "given": "Marie"
      },
      {
        "family": "Pierrot",
        "given": "Thomas"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          1,
          15
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1"
  },
  {
    "id": "xvk2JF2w",
    "type": "article",
    "abstract": "The field of genomics has seen substantial advancements through the application of artificial intelligence (AI), with machine learning revealing the potential to interpret genomic sequences without necessitating an exhaustive experimental analysis of all the intricate and interconnected molecular processes involved in DNA functioning. However, precise decoding of genomic sequences demands the comprehension of rich contextual information spread over thousands of nucleotides. Presently, only a few architectures exist that can process such extensive inputs, and they require exceptional computational resources. To address this need, we introduce GENA-LM, a suite of transformer-based foundational DNA language models capable of handling input lengths up to 36 thousands base pairs. We offer pre-trained versions of GENA-LM and demonstrate their capacity for fine-tuning to address complex biological questions with modest computational requirements. We also illustrate diverse applications of GENA-LM for various downstream genomic tasks, showcasing its performance in either matching or exceeding that of prior models, whether task-specific or universal. All models are publicly accessible on GitHub https://github.com/AIRI-Institute/GENA_LM and as pre-trained models with gena-lm-prefix on HuggingFace https://huggingface.co/AIRI-Institute.\nContacts minja-f{at}ya.ru, kardymon{at}airi.net, am{at}lims.ac.uk",
    "DOI": "10.1101/2023.06.12.544594",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "GENA-LM: A Family of Open-Source Foundational Models for Long DNA Sequences",
    "title-short": "GENA-LM",
    "URL": "https://www.biorxiv.org/content/10.1101/2023.06.12.544594v1",
    "author": [
      {
        "family": "Fishman",
        "given": "Veniamin"
      },
      {
        "family": "Kuratov",
        "given": "Yuri"
      },
      {
        "family": "Petrov",
        "given": "Maxim"
      },
      {
        "family": "Shmelev",
        "given": "Aleksei"
      },
      {
        "family": "Shepelin",
        "given": "Denis"
      },
      {
        "family": "Chekanov",
        "given": "Nikolay"
      },
      {
        "family": "Kardymon",
        "given": "Olga"
      },
      {
        "family": "Burtsev",
        "given": "Mikhail"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          6,
          13
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2023.06.12.544594v1.full"
  },
  {
    "id": "LZbja2NS",
    "type": "article",
    "abstract": "RNA molecules play a crucial role as intermediaries in diverse biological processes. Attaining a profound understanding of their function can substantially enhance our comprehension of life’s activities and facilitate drug development for numerous diseases. The advent of high-throughput sequencing technologies makes vast amounts of RNA sequence data accessible, which contains invaluable information and knowledge. However, deriving insights for further application from such an immense volume of data poses a significant challenge. Fortunately, recent advancements in pre-trained models have surfaced as a revolutionary solution for addressing such challenges owing to their exceptional ability to automatically mine and extract hidden knowledge from massive datasets. Inspired by the past successes, we developed a novel context-aware deep learning model named Uni-RNA that performs pre-training on the largest dataset of RNA sequences at the unprecedented scale to date. During this process, our model autonomously unraveled the obscured evolutionary and structural information embedded within the RNA sequences. As a result, through fine-tuning, our model achieved the state-of-the-art (SOTA) performances in a spectrum of downstream tasks, including both structural and functional predictions. Overall, Uni-RNA established a new research paradigm empowered by the large pre-trained model in the field of RNA, enabling the community to unlock the power of AI at a whole new level to significantly expedite the pace of research and foster groundbreaking discoveries.",
    "DOI": "10.1101/2023.07.11.548588",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "Uni-Rna: Universal Pre-Trained Models Revolutionize Rna Research",
    "title-short": "Uni-Rna",
    "URL": "https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1",
    "author": [
      {
        "family": "Wang",
        "given": "Xi"
      },
      {
        "family": "Gu",
        "given": "Ruichu"
      },
      {
        "family": "Chen",
        "given": "Zhiyuan"
      },
      {
        "family": "Li",
        "given": "Yongge"
      },
      {
        "family": "Ji",
        "given": "Xiaohong"
      },
      {
        "family": "Ke",
        "given": "Guolin"
      },
      {
        "family": "Wen",
        "given": "Han"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          7,
          12
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1"
  },
  {
    "id": "OSbr1Vwp",
    "type": "article",
    "abstract": "The success of the GPT series proves that GPT can extract general information from sequences, thereby benefiting all downstream tasks. This motivates us to use pre-trained models to explore the hidden information in DNA sequences. However, data and task requirements in DNA sequence analysis are complexity and diversity as DNA relevant data includes different types of information, such as sequences, expression levels, etc, while there is currently no model specifically designed for these characteristics. Hereby, we present DNAGPT, a generalized foundation model pre-trained on over 10 billion base pairs from 9 species which can be fine-tuned for any DNA sequence analysis task. Our model can simultaneously process or output DNA sequences and numbers. In addition, our unique token design allows users to design prompts according to their own task requirements, making it applicable to any type of task. We have evaluated our model on classification, regression, and generation tasks. We demonstrate that DNAGPT benefits from pre-training, and therefore can bring performance gains to any downstream task. Our model is not only a new attempt in the field of genomes analysis, but also provides a new direction for the application of foundation models in biology.",
    "DOI": "10.1101/2023.07.11.548628",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks",
    "title-short": "DNAGPT",
    "URL": "https://www.biorxiv.org/content/10.1101/2023.07.11.548628v1",
    "author": [
      {
        "family": "Zhang",
        "given": "Daoan"
      },
      {
        "family": "Zhang",
        "given": "Weitong"
      },
      {
        "family": "He",
        "given": "Bing"
      },
      {
        "family": "Zhang",
        "given": "Jianguo"
      },
      {
        "family": "Qin",
        "given": "Chenchen"
      },
      {
        "family": "Yao",
        "given": "Jianhua"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          7,
          12
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2023.07.11.548628v1"
  },
  {
    "id": "L387tcBg",
    "type": "article",
    "abstract": "Adapting large language models (LLMs) to protein sequences spawned the development of powerful protein language models (pLMs). Concurrently, AlphaFold2 broke through in protein structure prediction. Now we can systematically and comprehensively explore the dual nature of proteins that act and exist as three-dimensional (3D) machines and evolve as linear strings of one-dimensional (1D) sequences. Here, we leverage pLMs to simultaneously model both modalities by combining 1D sequences with 3D structure in a single model. We encode protein structures as token sequences using the 3Di-alphabet introduced by the 3D-alignment method Foldseek. This new foundation pLM extracts the features and patterns of the resulting “structure-sequence” representation. Toward this end, we built a non-redundant dataset from AlphaFoldDB and fine-tuned an existing pLM (ProtT5) to translate between 3Di and amino acid sequences. As a proof-of-concept for our novel approach, dubbed Protein structure-sequence T5 (ProstT5), we showed improved performance for subsequent prediction tasks, and for “inverse folding”, namely the generation of novel protein sequences adopting a given structural scaffold (“fold”). Our work showcased the potential of pLMs to tap into the information-rich protein structure revolution fueled by AlphaFold2. ProstT5 paves the way to develop new tools integrating the vast resource of 3D predictions, and opens new research avenues in the post-AlphaFold2 era. Our model is freely available for all at https://github.com/mheinzinger/ProstT5.",
    "DOI": "10.1101/2023.07.23.550085",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "Bilingual Language Model for Protein Sequence and Structure",
    "URL": "https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2",
    "author": [
      {
        "family": "Heinzinger",
        "given": "Michael"
      },
      {
        "family": "Weissenow",
        "given": "Konstantin"
      },
      {
        "family": "Sanchez",
        "given": "Joaquin Gomez"
      },
      {
        "family": "Henkel",
        "given": "Adrian"
      },
      {
        "family": "Mirdita",
        "given": "Milot"
      },
      {
        "family": "Steinegger",
        "given": "Martin"
      },
      {
        "family": "Rost",
        "given": "Burkhard"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          3,
          24
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2"
  },
  {
    "id": "RQ40o1d0",
    "type": "article",
    "abstract": "Protein design holds immense potential for optimizing naturally occurring proteins, with broad applications in drug discovery, material design, and sustainability. How-ever, computational methods for protein engineering are confronted with significant challenges, such as an expansive design space, sparse functional regions, and a scarcity of available labels. These issues are further exacerbated in practice by the fact most real-life design scenarios necessitate the simultaneous optimization of multiple properties. In this work, we introduce ProteinNPT, a non-parametric trans-former variant tailored to protein sequences and particularly suited to label-scarce and multi-task learning settings. We first focus on the supervised fitness prediction setting and develop several cross-validation schemes which support robust perfor-mance assessment. We subsequently reimplement prior top-performing baselines, introduce several extensions of these baselines by integrating diverse branches of the protein engineering literature, and demonstrate that ProteinNPT consistently outperforms all of them across a diverse set of protein property prediction tasks. Finally, we demonstrate the value of our approach for iterative protein design across extensive in silico Bayesian optimization and conditional sampling experiments.",
    "DOI": "10.1101/2023.12.06.570473",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers",
    "title-short": "ProteinNPT",
    "URL": "https://www.biorxiv.org/content/10.1101/2023.12.06.570473v1",
    "author": [
      {
        "family": "Notin",
        "given": "Pascal"
      },
      {
        "family": "Weitzman",
        "given": "Ruben"
      },
      {
        "family": "Marks",
        "given": "Debora S."
      },
      {
        "family": "Gal",
        "given": "Yarin"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          12,
          7
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/2023.12.06.570473v1.full"
  },
  {
    "id": "OvkIs2vK",
    "type": "article",
    "abstract": "Recent advances in deep learning, particularly unsupervised approaches, have shown promise for furthering our biological knowledge through their application to gene expression datasets, though applications to epigenomic data are lacking. Here, we employ an unsupervised deep learning framework with variational autoencoders (VAEs) to learn latent representations of the DNA methylation landscape from three independent breast tumor datasets. Through interrogation of methylation-based learned latent dimension activation values, we demonstrate the feasibility of VAEs to track representative differential methylation patterns among clinical subtypes of tumors. CpGs whose methylation was most correlated VAE latent dimension activation values were significantly enriched for CpG sparse regulatory regions of the genome including enhancer regions. In addition, through comparison with LASSO, we show the utility of the VAE approach for revealing novel information about CpG DNA methylation patterns in breast cancer.",
    "DOI": "10.1101/433763",
    "language": "en",
    "publisher": "bioRxiv",
    "source": "bioRxiv",
    "title": "Unsupervised deep learning with variational autoencoders applied to breast tumor genome-wide DNA methylation data with biologic feature extraction",
    "URL": "https://www.biorxiv.org/content/10.1101/433763v5",
    "author": [
      {
        "family": "Titus",
        "given": "Alexander J."
      },
      {
        "family": "Wilkins",
        "given": "Owen M."
      },
      {
        "family": "Bobak",
        "given": "Carly A."
      },
      {
        "family": "Christensen",
        "given": "Brock C."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018",
          11,
          7
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.biorxiv.org/content/10.1101/433763v5"
  },
  {
    "id": "e2upB1EQ",
    "type": "article-journal",
    "abstract": "The threat landscape of biological hazards with the evolution of AI presents challenges. While AI promises innovative solutions, concerns arise about its misuse in the creation of biological weapons. The convergence of AI and genetic editing raises questions about biosecurity, potentially accelerating the development of dangerous pathogens. The mapping conducted highlights the critical intersection between AI and biological threats, underscoring emerging risks in the criminal manipulation of pathogens. Technological advancement in biology requires preventative and regulatory measures. Expert recommendations emphasize the need for solid regulations and responsibility of creators, demanding a proactive, ethical approach and governance to ensure global safety.",
    "container-title": "Frontiers in Artificial Intelligence",
    "ISSN": "2624-8212",
    "source": "Frontiers",
    "title": "Artificial intelligence challenges in the face of biological threats: emerging catastrophic risks for public health",
    "title-short": "Artificial intelligence challenges in the face of biological threats",
    "URL": "https://www.frontiersin.org/articles/10.3389/frai.2024.1382356",
    "volume": "7",
    "author": [
      {
        "family": "Lima",
        "given": "Renan Chaves",
        "non-dropping-particle": "de"
      },
      {
        "family": "Sinclair",
        "given": "Lucas"
      },
      {
        "family": "Megger",
        "given": "Ricardo"
      },
      {
        "family": "Maciel",
        "given": "Magno Alessandro Guedes"
      },
      {
        "family": "Vasconcelos",
        "given": "Pedro Fernando da Costa"
      },
      {
        "family": "Quaresma",
        "given": "Juarez Antônio Simões"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.frontiersin.org/articles/10.3389/frai.2024.1382356/full"
  },
  {
    "id": "17gPkdn86",
    "type": "webpage",
    "title": "The GNU Manifesto - GNU Project - Free Software Foundation",
    "URL": "https://www.gnu.org/gnu/manifesto.en.html",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.gnu.org/gnu/manifesto.en.html"
  },
  {
    "id": "ntSixaJH",
    "type": "book",
    "abstract": "Biotechnology Industry 5.0 is advancing with the integration of cutting-edge technologies such as Machine Learning (ML), the Internet of Things (IoT), and cloud computing. It is no surprise that an industry that utilizes data from customers and can alter their lives is a target of a variety of attacks. This chapter provides a perspective on how Machine Learning Security Operations (MLSecOps) can help secure the biotechnology Industry 5.0. The chapter provides an analysis of the threats in the biotechnology Industry 5.0 and how ML algorithms can help secure with industry best practices. This chapter explores the scope of MLSecOps in the biotechnology Industry 5.0, highlighting how crucial it is to comply with current regulatory frameworks. With biotechnology Industry 5.0 developing innovative solutions in healthcare, supply chain management, biomanufacturing, pharmaceutical sectors, and more, the chapter also discusses the MLSecOps best practices that industry and enterprises should follow while also considering ethical responsibilities. Overall, the chapter provides a discussion of how to integrate MLSecOps into the design, deployment, and regulation of the processes in the biotechnology Industry 5.0.",
    "ISBN": "9780850144840",
    "language": "en",
    "note": "DOI: 10.5772/intechopen.114972\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.intechopen.com/online-first/89417",
    "publisher": "IntechOpen",
    "source": "www.intechopen.com",
    "title": "Integrating MLSecOps in the Biotechnology Industry 5.0",
    "URL": "https://www.intechopen.com/online-first/89417",
    "author": [
      {
        "family": "Pervez",
        "given": "Naseela"
      },
      {
        "family": "Titus",
        "given": "Alexander J."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          5,
          10
        ]
      ]
    }
  },
  {
    "id": "MNG16ntb",
    "type": "article-journal",
    "abstract": "Embedding a deep-learning model in the known structure of cellular systems yields DCell, a ‘visible’ neural network that can be used to mechanistically interpret genotype–phenotype relationships.",
    "container-title": "Nature Methods",
    "DOI": "10.1038/nmeth.4627",
    "ISSN": "1548-7105",
    "issue": "4",
    "journalAbbreviation": "Nat Methods",
    "language": "en",
    "page": "290-298",
    "source": "www.nature.com",
    "title": "Using deep learning to model the hierarchical structure and function of a cell",
    "URL": "https://www.nature.com/articles/nmeth.4627",
    "volume": "15",
    "author": [
      {
        "family": "Ma",
        "given": "Jianzhu"
      },
      {
        "family": "Yu",
        "given": "Michael Ku"
      },
      {
        "family": "Fong",
        "given": "Samson"
      },
      {
        "family": "Ono",
        "given": "Keiichiro"
      },
      {
        "family": "Sage",
        "given": "Eric"
      },
      {
        "family": "Demchak",
        "given": "Barry"
      },
      {
        "family": "Sharan",
        "given": "Roded"
      },
      {
        "family": "Ideker",
        "given": "Trey"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018",
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.nature.com/articles/nmeth.4627"
  },
  {
    "id": "qdpFhm5d",
    "type": "article-journal",
    "abstract": "Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",
    "container-title": "Nature",
    "DOI": "10.1038/s41586-021-03819-2",
    "ISSN": "1476-4687",
    "issue": "7873",
    "language": "en",
    "page": "583-589",
    "source": "www.nature.com",
    "title": "Highly accurate protein structure prediction with AlphaFold",
    "URL": "https://www.nature.com/articles/s41586-021-03819-2",
    "volume": "596",
    "author": [
      {
        "family": "Jumper",
        "given": "John"
      },
      {
        "family": "Evans",
        "given": "Richard"
      },
      {
        "family": "Pritzel",
        "given": "Alexander"
      },
      {
        "family": "Green",
        "given": "Tim"
      },
      {
        "family": "Figurnov",
        "given": "Michael"
      },
      {
        "family": "Ronneberger",
        "given": "Olaf"
      },
      {
        "family": "Tunyasuvunakool",
        "given": "Kathryn"
      },
      {
        "family": "Bates",
        "given": "Russ"
      },
      {
        "family": "Žídek",
        "given": "Augustin"
      },
      {
        "family": "Potapenko",
        "given": "Anna"
      },
      {
        "family": "Bridgland",
        "given": "Alex"
      },
      {
        "family": "Meyer",
        "given": "Clemens"
      },
      {
        "family": "Kohl",
        "given": "Simon A. A."
      },
      {
        "family": "Ballard",
        "given": "Andrew J."
      },
      {
        "family": "Cowie",
        "given": "Andrew"
      },
      {
        "family": "Romera-Paredes",
        "given": "Bernardino"
      },
      {
        "family": "Nikolov",
        "given": "Stanislav"
      },
      {
        "family": "Jain",
        "given": "Rishub"
      },
      {
        "family": "Adler",
        "given": "Jonas"
      },
      {
        "family": "Back",
        "given": "Trevor"
      },
      {
        "family": "Petersen",
        "given": "Stig"
      },
      {
        "family": "Reiman",
        "given": "David"
      },
      {
        "family": "Clancy",
        "given": "Ellen"
      },
      {
        "family": "Zielinski",
        "given": "Michal"
      },
      {
        "family": "Steinegger",
        "given": "Martin"
      },
      {
        "family": "Pacholska",
        "given": "Michalina"
      },
      {
        "family": "Berghammer",
        "given": "Tamas"
      },
      {
        "family": "Bodenstein",
        "given": "Sebastian"
      },
      {
        "family": "Silver",
        "given": "David"
      },
      {
        "family": "Vinyals",
        "given": "Oriol"
      },
      {
        "family": "Senior",
        "given": "Andrew W."
      },
      {
        "family": "Kavukcuoglu",
        "given": "Koray"
      },
      {
        "family": "Kohli",
        "given": "Pushmeet"
      },
      {
        "family": "Hassabis",
        "given": "Demis"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          8
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.nature.com/articles/s41586-021-03819-2"
  },
  {
    "id": "1Dw3zwV6h",
    "type": "article-journal",
    "abstract": "The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2–6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.",
    "container-title": "Nature",
    "DOI": "10.1038/s41586-024-07487-w",
    "ISSN": "1476-4687",
    "issue": "8016",
    "language": "en",
    "page": "493-500",
    "source": "www.nature.com",
    "title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
    "URL": "https://www.nature.com/articles/s41586-024-07487-w",
    "volume": "630",
    "author": [
      {
        "family": "Abramson",
        "given": "Josh"
      },
      {
        "family": "Adler",
        "given": "Jonas"
      },
      {
        "family": "Dunger",
        "given": "Jack"
      },
      {
        "family": "Evans",
        "given": "Richard"
      },
      {
        "family": "Green",
        "given": "Tim"
      },
      {
        "family": "Pritzel",
        "given": "Alexander"
      },
      {
        "family": "Ronneberger",
        "given": "Olaf"
      },
      {
        "family": "Willmore",
        "given": "Lindsay"
      },
      {
        "family": "Ballard",
        "given": "Andrew J."
      },
      {
        "family": "Bambrick",
        "given": "Joshua"
      },
      {
        "family": "Bodenstein",
        "given": "Sebastian W."
      },
      {
        "family": "Evans",
        "given": "David A."
      },
      {
        "family": "Hung",
        "given": "Chia-Chun"
      },
      {
        "family": "O’Neill",
        "given": "Michael"
      },
      {
        "family": "Reiman",
        "given": "David"
      },
      {
        "family": "Tunyasuvunakool",
        "given": "Kathryn"
      },
      {
        "family": "Wu",
        "given": "Zachary"
      },
      {
        "family": "Žemgulytė",
        "given": "Akvilė"
      },
      {
        "family": "Arvaniti",
        "given": "Eirini"
      },
      {
        "family": "Beattie",
        "given": "Charles"
      },
      {
        "family": "Bertolli",
        "given": "Ottavia"
      },
      {
        "family": "Bridgland",
        "given": "Alex"
      },
      {
        "family": "Cherepanov",
        "given": "Alexey"
      },
      {
        "family": "Congreve",
        "given": "Miles"
      },
      {
        "family": "Cowen-Rivers",
        "given": "Alexander I."
      },
      {
        "family": "Cowie",
        "given": "Andrew"
      },
      {
        "family": "Figurnov",
        "given": "Michael"
      },
      {
        "family": "Fuchs",
        "given": "Fabian B."
      },
      {
        "family": "Gladman",
        "given": "Hannah"
      },
      {
        "family": "Jain",
        "given": "Rishub"
      },
      {
        "family": "Khan",
        "given": "Yousuf A."
      },
      {
        "family": "Low",
        "given": "Caroline M. R."
      },
      {
        "family": "Perlin",
        "given": "Kuba"
      },
      {
        "family": "Potapenko",
        "given": "Anna"
      },
      {
        "family": "Savy",
        "given": "Pascal"
      },
      {
        "family": "Singh",
        "given": "Sukhdeep"
      },
      {
        "family": "Stecula",
        "given": "Adrian"
      },
      {
        "family": "Thillaisundaram",
        "given": "Ashok"
      },
      {
        "family": "Tong",
        "given": "Catherine"
      },
      {
        "family": "Yakneen",
        "given": "Sergei"
      },
      {
        "family": "Zhong",
        "given": "Ellen D."
      },
      {
        "family": "Zielinski",
        "given": "Michal"
      },
      {
        "family": "Žídek",
        "given": "Augustin"
      },
      {
        "family": "Bapst",
        "given": "Victor"
      },
      {
        "family": "Kohli",
        "given": "Pushmeet"
      },
      {
        "family": "Jaderberg",
        "given": "Max"
      },
      {
        "family": "Hassabis",
        "given": "Demis"
      },
      {
        "family": "Jumper",
        "given": "John M."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          6
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.nature.com/articles/s41586-024-07487-w"
  },
  {
    "id": "SnkCHfpX",
    "type": "article-journal",
    "abstract": "Protein engineers are drawing on rapidly evolving machine learning tools, deep reservoirs of data, and the structure-predicting firepower of AlphaFold2 to pursue more sophisticated de novo protein designs.",
    "container-title": "Nature Biotechnology",
    "DOI": "10.1038/s41587-023-01705-y",
    "ISSN": "1546-1696",
    "issue": "3",
    "language": "en",
    "page": "303-305",
    "source": "www.nature.com",
    "title": "AI-enhanced protein design makes proteins that have never existed",
    "URL": "https://www.nature.com/articles/s41587-023-01705-y",
    "volume": "41",
    "author": [
      {
        "family": "Eisenstein",
        "given": "Michael"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          3,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.nature.com/articles/s41587-023-01705-y"
  },
  {
    "id": "zszw9syp",
    "type": "article-journal",
    "abstract": "The inherent similarities between natural language and biological sequences have given rise to great interest in adapting the transformer-based large language models (LLMs) underlying recent breakthroughs in natural language processing (references), for applications in genomics. However, current LLMs for genomics suffer from several limitations such as the inability to include chromatin interactions in the training data, and the inability to make prediction in new cellular contexts not represented in the training data. To mitigate these problems, we propose EpiGePT, a transformer-based pretrained language model for predicting context-specific epigenomic signals and chromatin contacts. By taking the context-specific activities of transcription factors (TFs) and 3D genome interactions into consideration, EpiGePT offers wider applicability and deeper biological insights than models trained on DNA sequence only. In a series of experiments, EpiGePT demonstrates superior performance in a diverse set of epigenomic signals prediction tasks when compared to existing methods. In particular, our model enables cross-cell-type prediction of long-range interactions and offers insight on the functional impact of genetic variants under different cellular contexts. These new capabilities will enhance the usefulness of LLM in the study of gene regulatory mechanisms. We provide free online prediction service of EpiGePT through http://health.tsinghua.edu.cn/epigept/.",
    "container-title": "bioRxiv",
    "DOI": "10.1101/2023.07.15.549134",
    "journalAbbreviation": "bioRxiv",
    "note": "PMID: 37502861\nPMCID: PMC10370089\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370089",
    "page": "2023.07.15.549134",
    "source": "PubMed Central",
    "title": "EpiGePT: a Pretrained Transformer model for epigenomics",
    "title-short": "EpiGePT",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370089/",
    "author": [
      {
        "family": "Gao",
        "given": "Zijing"
      },
      {
        "family": "Liu",
        "given": "Qiao"
      },
      {
        "family": "Zeng",
        "given": "Wanwen"
      },
      {
        "family": "Jiang",
        "given": "Rui"
      },
      {
        "family": "Wong",
        "given": "Wing Hung"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          2,
          3
        ]
      ]
    }
  },
  {
    "id": "AXwYjitz",
    "type": "article-journal",
    "abstract": "Whereas protein language models have demonstrated remarkable efficacy in predicting the effects of missense variants, DNA counterparts have not yet achieved a similar competitive edge for genome-wide variant effect predictions, especially in complex genomes such as that of humans. To address this challenge, we here introduce GPN-MSA, a novel framework for DNA language models that leverages whole-genome sequence alignments across multiple species and takes only a few hours to train. Across several benchmarks on clinical databases (ClinVar, COSMIC, OMIM), experimental functional assays (DMS, DepMap), and population genomic data (gnomAD), our model for the human genome achieves outstanding performance on deleteriousness prediction for both coding and non-coding variants.",
    "container-title": "bioRxiv",
    "DOI": "10.1101/2023.10.10.561776",
    "journalAbbreviation": "bioRxiv",
    "note": "PMID: 37873118\nPMCID: PMC10592768\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10592768",
    "page": "2023.10.10.561776",
    "source": "PubMed Central",
    "title": "GPN-MSA: an alignment-based DNA language model for genome-wide variant effect prediction",
    "title-short": "GPN-MSA",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10592768/",
    "author": [
      {
        "family": "Benegas",
        "given": "Gonzalo"
      },
      {
        "family": "Albors",
        "given": "Carlos"
      },
      {
        "family": "Aw",
        "given": "Alan J."
      },
      {
        "family": "Ye",
        "given": "Chengzhong"
      },
      {
        "family": "Song",
        "given": "Yun S."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          4,
          6
        ]
      ]
    }
  },
  {
    "id": "3t4YHcSd",
    "type": "article-journal",
    "container-title": "Biochemical Journal",
    "ISSN": "0264-6021",
    "issue": "3",
    "journalAbbreviation": "Biochem J",
    "note": "PMID: 13032078\nPMCID: PMC1198157\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198157",
    "page": "353-366",
    "source": "PubMed Central",
    "title": "The amino-acid sequence in the glycyl chain of insulin. 1. The identification of lower peptides from partial hydrolysates",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198157/",
    "volume": "53",
    "author": [
      {
        "family": "Sanger",
        "given": "F."
      },
      {
        "family": "Thompson",
        "given": "E. O. P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1953",
          2
        ]
      ]
    }
  },
  {
    "id": "Htmw8erk",
    "type": "article-journal",
    "container-title": "Biochemical Journal",
    "ISSN": "0264-6021",
    "issue": "3",
    "journalAbbreviation": "Biochem J",
    "note": "PMID: 13032079\nPMCID: PMC1198158\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198158",
    "page": "366-374",
    "source": "PubMed Central",
    "title": "The amino-acid sequence in the glycyl chain of insulin. 2. The investigation of peptides from enzymic hydrolysates",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198158/",
    "volume": "53",
    "author": [
      {
        "family": "Sanger",
        "given": "F."
      },
      {
        "family": "Thompson",
        "given": "E. O. P."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1953",
          2
        ]
      ]
    }
  },
  {
    "id": "14cnpLsxs",
    "type": "article-journal",
    "abstract": "GOLD is a comprehensive resource for accessing information related\n to completed and ongoing genome projects world-wide. The database\n currently provides information on 350 genome projects, of which\n 48 have been completely sequenced and their analysis published.\n GOLD was created in 1997 and since April 2000 it has been licensed\n to Integrated Genomics. The database is freely available through the\n URL: http://igweb.integratedgenomics.com/GOLD/.",
    "container-title": "Nucleic Acids Research",
    "ISSN": "0305-1048",
    "issue": "1",
    "journalAbbreviation": "Nucleic Acids Res",
    "note": "PMID: 11125068\nPMCID: PMC29859\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC29859",
    "page": "126-127",
    "source": "PubMed Central",
    "title": "Genomes OnLine Database (GOLD):\n a monitor of genome projects world-wide",
    "title-short": "Genomes OnLine Database (GOLD)",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC29859/",
    "volume": "29",
    "author": [
      {
        "family": "Bernal",
        "given": "Axel"
      },
      {
        "family": "Ear",
        "given": "Uy"
      },
      {
        "family": "Kyrpides",
        "given": "Nikos"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2001",
          1,
          1
        ]
      ]
    }
  },
  {
    "id": "JkRFkNmt",
    "type": "article-journal",
    "abstract": "Experimental studies of protein folding processes are frequently hampered by the fact that only low resolution structural data can be obtained with sufficient temporal resolution. Molecular dynamics simulations offer a complementary approach, providing extremely high resolution spatial and temporal data on folding processes. The effectiveness of such simulations is currently hampered by continuing questions regarding the ability of molecular dynamics force fields to reproduce the true potential energy surfaces of proteins, and ongoing difficulties with obtaining sufficient sampling to meaningfully comment on folding mechanisms. We review recent progress in the simulation of three common model systems for protein folding, and discuss how recent advances in technology and theory are allowing protein folding simulations to address their current shortcomings.",
    "container-title": "Nature physics",
    "DOI": "10.1038/nphys1713",
    "ISSN": "1745-2473",
    "issue": "10",
    "journalAbbreviation": "Nat Phys",
    "note": "PMID: 21297873\nPMCID: PMC3032381\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3032381",
    "page": "751-758",
    "source": "PubMed Central",
    "title": "Challenges in protein folding simulations: Timescale, representation, and analysis",
    "title-short": "Challenges in protein folding simulations",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3032381/",
    "volume": "6",
    "author": [
      {
        "family": "Freddolino",
        "given": "Peter L."
      },
      {
        "family": "Harrison",
        "given": "Christopher B."
      },
      {
        "family": "Liu",
        "given": "Yanxin"
      },
      {
        "family": "Schulten",
        "given": "Klaus"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2010",
          10,
          1
        ]
      ]
    }
  },
  {
    "id": "M8aKGlSe",
    "type": "article-journal",
    "abstract": "With modern fast sequencing techniques and suitable computer programs it is now possible to sequence whole genomes without the need of restriction maps. This paper describes computer programs that can be used to order both sequence gel readings and clones. A method of coding for uncertainties in gel readings is described. These programs are available on request.",
    "container-title": "Nucleic Acids Research",
    "ISSN": "0305-1048",
    "issue": "7",
    "journalAbbreviation": "Nucleic Acids Res",
    "note": "PMID: 461197\nPMCID: PMC327874\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC327874",
    "page": "2601-2610",
    "source": "PubMed Central",
    "title": "A strategy of DNA sequencing employing computer programs.",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC327874/",
    "volume": "6",
    "author": [
      {
        "family": "Staden",
        "given": "R"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1979",
          6,
          11
        ]
      ]
    }
  },
  {
    "id": "Tx5CQPm8",
    "type": "article-journal",
    "abstract": "GenBank® (http://www.ncbi.nlm.nih.gov) is a comprehensive database that contains publicly available nucleotide sequences for almost 260 000 formally described species. These sequences are obtained primarily through submissions from individual laboratories and batch submissions from large-scale sequencing projects, including whole-genome shotgun (WGS) and environmental sampling projects. Most submissions are made using the web-based BankIt or standalone Sequin programs, and GenBank staff assigns accession numbers upon data receipt. Daily data exchange with the European Nucleotide Archive (ENA) and the DNA Data Bank of Japan (DDBJ) ensures worldwide coverage. GenBank is accessible through the NCBI Entrez retrieval system, which integrates data from the major DNA and protein sequence databases along with taxonomy, genome, mapping, protein structure and domain information, and the biomedical journal literature via PubMed. BLAST provides sequence similarity searches of GenBank and other sequence databases. Complete bimonthly releases and daily updates of the GenBank database are available by FTP. To access GenBank and its related retrieval and analysis services, begin at the NCBI home page: www.ncbi.nlm.nih.gov.",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/gks1195",
    "ISSN": "0305-1048",
    "issue": "Database issue",
    "journalAbbreviation": "Nucleic Acids Res",
    "note": "PMID: 23193287\nPMCID: PMC3531190\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531190",
    "page": "D36-D42",
    "source": "PubMed Central",
    "title": "GenBank",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531190/",
    "volume": "41",
    "author": [
      {
        "family": "Benson",
        "given": "Dennis A."
      },
      {
        "family": "Cavanaugh",
        "given": "Mark"
      },
      {
        "family": "Clark",
        "given": "Karen"
      },
      {
        "family": "Karsch-Mizrachi",
        "given": "Ilene"
      },
      {
        "family": "Lipman",
        "given": "David J."
      },
      {
        "family": "Ostell",
        "given": "James"
      },
      {
        "family": "Sayers",
        "given": "Eric W."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2013",
          1
        ]
      ]
    }
  },
  {
    "id": "sT4CEJLS",
    "type": "article-journal",
    "abstract": "Quantitatively accurate all-atom molecular dynamics (MD) simulations of protein folding have long been considered a holy grail of computational biology. Due to the large system sizes and long timescales involved, such a pursuit was for many years computationally intractable. Further, sufficiently accurate forcefields needed to be developed in order to realistically model folding. This decade, however, saw the first reports of folding simulations describing kinetics on the order of milliseconds, placing many proteins firmly within reach of these methods. Progress in sampling and forcefield accuracy, however, presents a new challenge: how to turn huge MD datasets into scientific understanding. Here, we review recent progress in MD simulation techniques and show how the vast datasets generated by such techniques present new challenges for analysis. We critically discuss the state of the art, including reaction coordinate and Markov state model (MSM) methods, and provide a perspective for the future.",
    "container-title": "Current opinion in structural biology",
    "DOI": "10.1016/j.sbi.2012.11.002",
    "ISSN": "0959-440X",
    "issue": "1",
    "journalAbbreviation": "Curr Opin Struct Biol",
    "note": "PMID: 23237705\nPMCID: PMC3673555\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673555",
    "page": "58-65",
    "source": "PubMed Central",
    "title": "To Milliseconds and Beyond: Challenges in the Simulation of Protein Folding",
    "title-short": "To Milliseconds and Beyond",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673555/",
    "volume": "23",
    "author": [
      {
        "family": "Lane",
        "given": "Thomas J."
      },
      {
        "family": "Shukla",
        "given": "Diwakar"
      },
      {
        "family": "Beauchamp",
        "given": "Kyle A."
      },
      {
        "family": "Pande",
        "given": "Vijay S."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2013",
          2
        ]
      ]
    }
  },
  {
    "id": "16yino3ku",
    "type": "article-journal",
    "abstract": "Whole-cell modeling promises to facilitate scientific inquiry by prioritizing future experiments based on existing datasets. To test this promise, we compared simulated growth rates with new measurements for all viable single-gene disruption strains in Mycoplasma genitalium. The discrepancies between simulations and experiments led to novel model predictions about specific kinetic parameters that we subsequently validated. These findings represent the first application of whole-cell modeling to accelerate biological discovery.",
    "container-title": "Nature methods",
    "DOI": "10.1038/nmeth.2724",
    "ISSN": "1548-7091",
    "issue": "12",
    "journalAbbreviation": "Nat Methods",
    "note": "PMID: 24185838\nPMCID: PMC3856890\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3856890",
    "page": "1192-1195",
    "source": "PubMed Central",
    "title": "Accelerated discovery via a whole-cell model",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3856890/",
    "volume": "10",
    "author": [
      {
        "family": "Sanghvi",
        "given": "Jayodita C."
      },
      {
        "family": "Regot",
        "given": "Sergi"
      },
      {
        "family": "Carrasco",
        "given": "Silvia"
      },
      {
        "family": "Karr",
        "given": "Jonathan R."
      },
      {
        "family": "Gutschow",
        "given": "Miriam V."
      },
      {
        "family": "Bolival",
        "given": "Benjamin"
      },
      {
        "family": "Covert",
        "given": "Markus W."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2013",
          12
        ]
      ]
    }
  },
  {
    "id": "10pibmvLC",
    "type": "article-journal",
    "container-title": "Proceedings of the National Academy of Sciences of the United States of America",
    "ISSN": "0027-8424",
    "issue": "6",
    "journalAbbreviation": "Proc Natl Acad Sci U S A",
    "note": "PMID: 806076\nPMCID: PMC432675\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432675",
    "page": "1981-1984",
    "source": "PubMed Central",
    "title": "Summary statement of the Asilomar conference on recombinant DNA molecules.",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432675/",
    "volume": "72",
    "author": [
      {
        "family": "Berg",
        "given": "P"
      },
      {
        "family": "Baltimore",
        "given": "D"
      },
      {
        "family": "Brenner",
        "given": "S"
      },
      {
        "family": "Roblin",
        "given": "R O"
      },
      {
        "family": "Singer",
        "given": "M F"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1975",
          6
        ]
      ]
    }
  },
  {
    "id": "qPG24gP6",
    "type": "article-journal",
    "abstract": "Basic Local Alignment Search Tool (BLAST) is one of the most heavily used sequence analysis tools available in the public domain. There is now a wide choice of BLAST algorithms that can be used to search many different sequence databases via the BLAST web pages (http://www.ncbi.nlm.nih.gov/BLAST/). All the algorithm–database combinations can be executed with default parameters or with customized settings, and the results can be viewed in a variety of ways. A new online resource, the BLAST Program Selection Guide, has been created to assist in the definition of search strategies. This article discusses optimal search strategies and highlights some BLAST features that can make your searches more powerful.",
    "container-title": "Nucleic Acids Research",
    "DOI": "10.1093/nar/gkh435",
    "ISSN": "0305-1048",
    "issue": "Web Server issue",
    "journalAbbreviation": "Nucleic Acids Res",
    "note": "PMID: 15215342\nPMCID: PMC441573\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC441573",
    "page": "W20-W25",
    "source": "PubMed Central",
    "title": "BLAST: at the core of a powerful and diverse set of sequence analysis tools",
    "title-short": "BLAST",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC441573/",
    "volume": "32",
    "author": [
      {
        "family": "McGinnis",
        "given": "Scott"
      },
      {
        "family": "Madden",
        "given": "Thomas L."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2004",
          7,
          1
        ]
      ]
    }
  },
  {
    "id": "Z7e2T8go",
    "type": "article-journal",
    "abstract": "Molecular dynamics simulations have evolved into a mature technique that can be used effectively to understand macromolecular structure-to-function relationships. Present simulation times are close to biologically relevant ones. Information gathered about the dynamic properties of macromolecules is rich enough to shift the usual paradigm of structural bioinformatics from studying single structures to analyze conformational ensembles. Here, we describe the foundations of molecular dynamics and the improvements made in the direction of getting such ensemble. Specific application of the technique to three main issues (allosteric regulation, docking, and structure refinement) is discussed.",
    "container-title": "Advances and Applications in Bioinformatics and Chemistry : AABC",
    "DOI": "10.2147/AABC.S70333",
    "ISSN": "1178-6949",
    "journalAbbreviation": "Adv Appl Bioinform Chem",
    "note": "PMID: 26604800\nPMCID: PMC4655909\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4655909",
    "page": "37-47",
    "source": "PubMed Central",
    "title": "Molecular dynamics simulations: advances and applications",
    "title-short": "Molecular dynamics simulations",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4655909/",
    "volume": "8",
    "author": [
      {
        "family": "Hospital",
        "given": "Adam"
      },
      {
        "family": "Goñi",
        "given": "Josep Ramon"
      },
      {
        "family": "Orozco",
        "given": "Modesto"
      },
      {
        "family": "Gelpí",
        "given": "Josep L"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2015",
          11,
          19
        ]
      ]
    }
  },
  {
    "id": "DWZxgqvi",
    "type": "article-journal",
    "abstract": "By introducing the methods of machine learning into the density functional theory, we made a detour for the construction of the most probable density function, which can be estimated by learning relevant features from the system of interest. Using the properties of universal functional, the vital core of density functional theory, the most probable cluster numbers and the corresponding cluster boundaries in a studying system can be simultaneously and automatically determined and the plausibility is erected on the Hohenberg-Kohn theorems. For the method validation and pragmatic applications, interdisciplinary problems from physical to biological systems were enumerated. The amalgamation of uncharged atomic clusters validated the unsupervised searching process of the cluster numbers and the corresponding cluster boundaries were exhibited likewise. High accurate clustering results of the Fisher’s iris dataset showed the feasibility and the flexibility of the proposed scheme. Brain tumor detections from low-dimensional magnetic resonance imaging datasets and segmentations of high-dimensional neural network imageries in the Brainbow system were also used to inspect the method practicality. The experimental results exhibit the successful connection between the physical theory and the machine learning methods and will benefit the clinical diagnoses.",
    "container-title": "Scientific Reports",
    "DOI": "10.1038/s41598-017-18931-5",
    "ISSN": "2045-2322",
    "journalAbbreviation": "Sci Rep",
    "note": "PMID: 29323205\nPMCID: PMC5765025\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5765025",
    "page": "557",
    "source": "PubMed Central",
    "title": "Unsupervised Learning and Pattern Recognition of Biological Data Structures with Density Functional Theory and Machine Learning",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5765025/",
    "volume": "8",
    "author": [
      {
        "family": "Chen",
        "given": "Chien-Chang"
      },
      {
        "family": "Juan",
        "given": "Hung-Hui"
      },
      {
        "family": "Tsai",
        "given": "Meng-Yuan"
      },
      {
        "family": "Lu",
        "given": "Henry Horng-Shing"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018",
          1,
          11
        ]
      ]
    }
  },
  {
    "id": "MgRWKCpM",
    "type": "article-journal",
    "abstract": "Whole-cell dynamical models of human cells are a central goal of systems\nbiology. Such models could help researchers understand cell biology and help\nphysicians treat disease. Despite significant challenges, we believe that human\nwhole-cell models are rapidly becoming feasible. To develop a plan for achieving\nhuman whole-cell models, we analyzed the existing models of individual cellular\npathways, surveyed the biomodeling community, and reflected on our experience\ndeveloping whole-cell models of bacteria. Based on these analyses, we propose a\nplan for a project, termed the Human Whole-Cell Modeling\nProject, to achieve human whole-cell models. The foundations of the\nplan include technology development, standards development, and\ninterdisciplinary collaboration.,",
    "container-title": "Current opinion in systems biology",
    "DOI": "10.1016/j.coisb.2017.10.005",
    "ISSN": "2452-3100",
    "journalAbbreviation": "Curr Opin Syst Biol",
    "note": "PMID: 29806041\nPMCID: PMC5966287\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966287",
    "page": "8-15",
    "source": "PubMed Central",
    "title": "A blueprint for human whole-cell modeling",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966287/",
    "volume": "7",
    "author": [
      {
        "family": "Szigeti",
        "given": "Balázs"
      },
      {
        "family": "Roth",
        "given": "Yosef D."
      },
      {
        "family": "Sekar",
        "given": "John A. P."
      },
      {
        "family": "Goldberg",
        "given": "Arthur P."
      },
      {
        "family": "Pochiraju",
        "given": "Saahith C."
      },
      {
        "family": "Karr",
        "given": "Jonathan R."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018",
          2
        ]
      ]
    }
  },
  {
    "id": "16JwdMPBH",
    "type": "article-journal",
    "abstract": "Next-generation sequencing has the ability to revolutionize almost all fields of biological science. It has drastically reduced the cost of sequencing. This allows us to study the whole genome or part of the genome to understand how the cellular functions are governed by the genetic code. The data obtained in huge quantity from sequencing upon analysis gives an insight to understand the mechanism of pathogen biology, virulence, and phenomenon of bacterial resistance, which helps in investigating the outbreak. This ultimately helps in the development of therapies for public health welfare against human pathogen and diagnostic reagents for the screening. This chapter includes the basic of Sanger’s method of DNA sequencing and next-generation sequencing, different available platforms for sequencing with their advantages, and limitations and their chemistry with an overview of downstream data analysis. Furthermore, the breadth of applications of high-throughput NGS technology for human health has been discussed.",
    "container-title": "Microbial Technology for the Welfare of Society",
    "DOI": "10.1007/978-981-13-8844-6_15",
    "journalAbbreviation": "Microbial Technology for the Welfare of Society",
    "note": "PMID: null\nPMCID: PMC7122948\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7122948",
    "page": "313-341",
    "source": "PubMed Central",
    "title": "Next-Generation Sequencing and Its Application: Empowering in Public Health Beyond Reality",
    "title-short": "Next-Generation Sequencing and Its Application",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7122948/",
    "volume": "17",
    "author": [
      {
        "family": "Gupta",
        "given": "Nidhi"
      },
      {
        "family": "Verma",
        "given": "Vijay K."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          13
        ]
      ]
    }
  },
  {
    "id": "OHHpDn0z",
    "type": "article-journal",
    "abstract": "Computer-aided design (CAD) for synthetic biology promises to accelerate the rational and robust engineering of biological systems. It requires both detailed and quantitative mathematical and experimental models of the processes to (re)design biology, and software and tools for genetic engineering and DNA assembly. Ultimately, the increased precision in the design phase will have a dramatic impact on the production of designer cells and organisms with bespoke functions and increased modularity. CAD strategies require quantitative models of cells that can capture multiscale processes and link genotypes to phenotypes. Here, we present a perspective on how whole-cell, multiscale models could transform design-build-test-learn cycles in synthetic biology. We show how these models could significantly aid in the design and learn phases while reducing experimental testing by presenting case studies spanning from genome minimization to cell-free systems. We also discuss several challenges for the realization of our vision. The possibility to describe and build whole-cells in silico offers an opportunity to develop increasingly automatized, precise and accessible CAD tools and strategies.",
    "container-title": "Frontiers in Bioengineering and Biotechnology",
    "DOI": "10.3389/fbioe.2020.00942",
    "ISSN": "2296-4185",
    "journalAbbreviation": "Front Bioeng Biotechnol",
    "note": "PMID: 32850764\nPMCID: PMC7426639\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426639",
    "page": "942",
    "source": "PubMed Central",
    "title": "Computer-Aided Whole-Cell Design: Taking a Holistic Approach by Integrating Synthetic With Systems Biology",
    "title-short": "Computer-Aided Whole-Cell Design",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426639/",
    "volume": "8",
    "author": [
      {
        "family": "Marucci",
        "given": "Lucia"
      },
      {
        "family": "Barberis",
        "given": "Matteo"
      },
      {
        "family": "Karr",
        "given": "Jonathan"
      },
      {
        "family": "Ray",
        "given": "Oliver"
      },
      {
        "family": "Race",
        "given": "Paul R."
      },
      {
        "family": "Souza Andrade",
        "given": "Miguel",
        "non-dropping-particle": "de"
      },
      {
        "family": "Grierson",
        "given": "Claire"
      },
      {
        "family": "Hoffmann",
        "given": "Stefan Andreas"
      },
      {
        "family": "Landon",
        "given": "Sophie"
      },
      {
        "family": "Rech",
        "given": "Elibio"
      },
      {
        "family": "Rees-Garbutt",
        "given": "Joshua"
      },
      {
        "family": "Seabrook",
        "given": "Richard"
      },
      {
        "family": "Shaw",
        "given": "William"
      },
      {
        "family": "Woods",
        "given": "Christopher"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2020",
          8,
          7
        ]
      ]
    }
  },
  {
    "id": "17K5Ty7Fj",
    "type": "article-journal",
    "container-title": "Iranian Journal of Public Health",
    "DOI": "10.18502/ijph.v50i11.7600",
    "ISSN": "2251-6085",
    "issue": "11",
    "journalAbbreviation": "Iran J Public Health",
    "note": "PMID: 35223619\nPMCID: PMC8826344\nThis CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8826344",
    "page": "i-v",
    "source": "PubMed Central",
    "title": "Ethical Issues of Artificial Intelligence in Medicine and Healthcare",
    "URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8826344/",
    "volume": "50",
    "author": [
      {
        "family": "Farhud",
        "given": "Dariush D."
      },
      {
        "family": "Zokaei",
        "given": "Shaghayegh"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          11
        ]
      ]
    }
  },
  {
    "id": "Q1QW0BcS",
    "type": "article-journal",
    "container-title": "NIST",
    "language": "en",
    "source": "www.nist.gov",
    "title": "Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence",
    "URL": "https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2023",
          10,
          10
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence"
  },
  {
    "id": "CvVpaf4a",
    "type": "webpage",
    "abstract": "The Nobel Prize in Chemistry 1993 was awarded \"for contributions to the developments of methods within DNA-based chemistry\" jointly with one half to Kary B. Mullis \"for his invention of the polymerase chain reaction (PCR) method\" and with one half to Michael Smith \"for his fundamental contributions to the establishment of oligonucleotide-based, site-directed mutagenesis and its development for protein studies\"",
    "container-title": "NobelPrize.org",
    "language": "en-US",
    "title": "The Nobel Prize in Chemistry 1993",
    "URL": "https://www.nobelprize.org/prizes/chemistry/1993/mullis/lecture/",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.nobelprize.org/prizes/chemistry/1993/mullis/lecture"
  },
  {
    "URL": "https://www.researchgate.net/publication/221307757_The_Free_Software_Movement_and_the_GNULinux_Operating_System",
    "type": "webpage",
    "id": "WD5AIwcy",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/221307757_The_Free_Software_Movement_and_the_GNULinux_Operating_System"
  },
  {
    "URL": "https://www.researchgate.net/publication/223020409_Chen_CM_Intelligent_Web-based_Learning_System_with_Personalized_Learning_Path_Guidance_Computers_Education_512_787-814",
    "type": "webpage",
    "id": "Gz18T5lX",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/223020409_Chen_CM_Intelligent_Web-based_Learning_System_with_Personalized_Learning_Path_Guidance_Computers_Education_512_787-814"
  },
  {
    "URL": "https://www.researchgate.net/publication/235028224_The_Applicability_and_Limitations_of_Expert_System_Shells",
    "type": "webpage",
    "id": "om2dJrZ8",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/235028224_The_Applicability_and_Limitations_of_Expert_System_Shells"
  },
  {
    "URL": "https://www.researchgate.net/publication/242400296_A_Revision_of_Bloom",
    "type": "webpage",
    "id": "10FhlvaO",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/242400296_A_Revision_of_Bloom"
  },
  {
    "URL": "https://www.researchgate.net/publication/30874496_Artificial_Intelligence_A_Modern_Approach",
    "type": "webpage",
    "id": "AviPiwRn",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/30874496_Artificial_Intelligence_A_Modern_Approach"
  },
  {
    "URL": "https://www.researchgate.net/publication/322870935_A_New_Dimension_of_Breast_Cancer_Epigenetics_-_Applications_of_Variational_Autoencoders_with_DNA_Methylation",
    "type": "webpage",
    "id": "Miw01DVK",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/322870935_A_New_Dimension_of_Breast_Cancer_Epigenetics_-_Applications_of_Variational_Autoencoders_with_DNA_Methylation"
  },
  {
    "URL": "https://www.researchgate.net/publication/354105080_LOGO_a_contextualized_pre-trained_language_model_of_human_genome_flexibly_adapts_to_various_downstream_tasks_by_fine-tuning",
    "type": "webpage",
    "id": "VQVJDKnn",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/354105080_LOGO_a_contextualized_pre-trained_language_model_of_human_genome_flexibly_adapts_to_various_downstream_tasks_by_fine-tuning"
  },
  {
    "URL": "https://www.researchgate.net/publication/362540943_MoDNA_motif-oriented_pre-training_for_DNA_language_model",
    "type": "webpage",
    "id": "boBvfMUu",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/362540943_MoDNA_motif-oriented_pre-training_for_DNA_language_model"
  },
  {
    "URL": "https://www.researchgate.net/publication/372592054_Violet_Teaming_AI_in_the_Life_Sciences_A_Preprint",
    "type": "webpage",
    "id": "9ODUtCAc",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/372592054_Violet_Teaming_AI_in_the_Life_Sciences_A_Preprint"
  },
  {
    "URL": "https://www.researchgate.net/publication/377955158_Autoencoders_and_their_applications_in_machine_learning_a_survey",
    "type": "webpage",
    "id": "z4zN2ueG",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.researchgate.net/publication/377955158_Autoencoders_and_their_applications_in_machine_learning_a_survey"
  },
  {
    "id": "hXLTkVKX",
    "type": "webpage",
    "abstract": "Part 1: When AI Bots do Genetic Engineering",
    "language": "en",
    "title": "DNAI - The Artificial Intelligence / Artificial Life convergence",
    "URL": "https://www.scanthehorizon.org/p/dnai-the-artificial-intelligence",
    "author": [
      {
        "family": "Thomas",
        "given": "Jim"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.scanthehorizon.org/p/dnai-the-artificial-intelligence"
  },
  {
    "URL": "https://www.schumer.senate.gov/imo/media/doc/Alexander%20Titus%20",
    "type": "webpage",
    "id": "6sn3T6Xp",
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.schumer.senate.gov/imo/media/doc/Alexander%20Titus%20"
  },
  {
    "id": "JVovM7kx",
    "type": "article-journal",
    "container-title": "Science",
    "DOI": "10.1126/science.add2210",
    "ISSN": "0036-8075, 1095-9203",
    "issue": "6598",
    "journalAbbreviation": "Science",
    "language": "en",
    "page": "1172-1173",
    "source": "DOI.org (Crossref)",
    "title": "Building the nuclear pore complex",
    "URL": "https://www.science.org/doi/10.1126/science.add2210",
    "volume": "376",
    "author": [
      {
        "family": "Jiang",
        "given": "Di"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2022",
          6,
          10
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.science.org/doi/10.1126/science.add2210"
  },
  {
    "id": "rTpWDKdy",
    "type": "webpage",
    "abstract": "Developing life-saving medicines can take billions of dollars and decades of time, but researchers are aiming to speed up this process with a new artificial intelligence-based drug screening process they've developed. Using a method that models drug and target protein interactions using natural language processing techniques, the researchers achieved up to 97% accuracy in identifying promising drug candidates.",
    "container-title": "ScienceDaily",
    "language": "en",
    "title": "AI-based screening method could boost speed of new drug discovery",
    "URL": "https://www.sciencedaily.com/releases/2022/09/220923090832.htm",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.sciencedaily.com/releases/2022/09/220923090832.htm"
  },
  {
    "id": "DOfI6ZOm",
    "type": "article-journal",
    "abstract": "MYCIN is a computer-based consultation system designed to assist physicians in the diagnosis of and therapy selection for patients with bacterial infections. In addition to the consultation system itself, MYCIN contains an explanation system which can answer simple English questions in order to justify its advice or educate the user. The system's knowledge is encoded in the form of some 350 production rules which embody the clinical decision criteria of infectious disease experts. Much of MYCIN's power derives from the modular, highly stylized nature of these decision rules, enabling the system to dissect its own reasoning and allowing easy modification of the knowledge base.",
    "container-title": "International Journal of Man-Machine Studies",
    "DOI": "10.1016/S0020-7373(78)80049-2",
    "ISSN": "0020-7373",
    "issue": "3",
    "journalAbbreviation": "International Journal of Man-Machine Studies",
    "page": "313-322",
    "source": "ScienceDirect",
    "title": "MYCIN: a knowledge-based consultation program for infectious disease diagnosis",
    "title-short": "MYCIN",
    "URL": "https://www.sciencedirect.com/science/article/pii/S0020737378800492",
    "volume": "10",
    "author": [
      {
        "family": "Melle",
        "given": "William",
        "non-dropping-particle": "van"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1978",
          5,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.sciencedirect.com/science/article/abs/pii/S0020737378800492"
  },
  {
    "id": "1J5MgQt2",
    "type": "article-journal",
    "abstract": "Generative artificial intelligence (GAI) is a rapidly growing field with a wide range of applications. In this paper, a thorough examination of the research landscape in GAI is presented, encompassing a comprehensive overview of the prevailing themes and topics within the field. The study analyzes a corpus of 1319 records from Scopus spanning from 1985 to 2023 and comprises journal articles, books, book chapters, conference papers, and selected working papers. The analysis revealed seven distinct clusters of topics in GAI research: image processing and content analysis, content generation, emerging use cases, engineering, cognitive inference and planning, data privacy and security, and Generative Pre-Trained Transformer (GPT) academic applications. The paper discusses the findings of the analysis and identifies some of the key challenges and opportunities in GAI research. The paper concludes by calling for further research in GAI, particularly in the areas of explainability, robustness, cross-modal and multi-modal generation, and interactive co-creation. The paper also highlights the importance of addressing the challenges of data privacy and security in GAI and responsible use of GAI.",
    "collection-title": "Systematic Review and Meta-analysis in Information Management Research",
    "container-title": "Data and Information Management",
    "DOI": "10.1016/j.dim.2024.100066",
    "ISSN": "2543-9251",
    "issue": "2",
    "journalAbbreviation": "Data and Information Management",
    "page": "100066",
    "source": "ScienceDirect",
    "title": "Generative AI: A systematic review using topic modelling techniques",
    "title-short": "Generative AI",
    "URL": "https://www.sciencedirect.com/science/article/pii/S2543925124000020",
    "volume": "8",
    "author": [
      {
        "family": "Gupta",
        "given": "Priyanka"
      },
      {
        "family": "Ding",
        "given": "Bosheng"
      },
      {
        "family": "Guan",
        "given": "Chong"
      },
      {
        "family": "Ding",
        "given": "Ding"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2024",
          6,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.sciencedirect.com/science/article/pii/S2543925124000020"
  },
  {
    "id": "13wKjJvoL",
    "type": "article-journal",
    "abstract": "The development of artificial intelligence technology is currently bringing about new opportunities in construction. Machine learning is a major area of interest within the field of artificial intelligence, playing a pivotal role in the process of making construction “smart”. The application of machine learning in construction has the potential to open up an array of opportunities such as site supervision, automatic detection, and intelligent maintenance. However, the implementation of machine learning faces a range of challenges due to the difficulties in acquiring labeled data, especially when applied in a highly complex construction site environment. This paper reviews the history of machine learning development from shallow to deep learning and its applications in construction. The strengths and weaknesses of machine learning technology in construction have been analyzed in order to foresee the future direction of machine learning applications in this sphere. Furthermore, this paper presents suggestions which may benefit researchers in terms of combining specific knowledge domains in construction with machine learning algorithms so as to develop dedicated deep network models for the industry.",
    "container-title": "Developments in the Built Environment",
    "DOI": "10.1016/j.dibe.2021.100045",
    "ISSN": "2666-1659",
    "journalAbbreviation": "Developments in the Built Environment",
    "page": "100045",
    "source": "ScienceDirect",
    "title": "Machine learning in construction: From shallow to deep learning",
    "title-short": "Machine learning in construction",
    "URL": "https://www.sciencedirect.com/science/article/pii/S2666165921000041",
    "volume": "6",
    "author": [
      {
        "family": "Xu",
        "given": "Yayin"
      },
      {
        "family": "Zhou",
        "given": "Ying"
      },
      {
        "family": "Sekula",
        "given": "Przemyslaw"
      },
      {
        "family": "Ding",
        "given": "Lieyun"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2021",
          5,
          1
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.sciencedirect.com/science/article/pii/S2666165921000041"
  },
  {
    "id": "YKj6eC6u",
    "type": "webpage",
    "abstract": "Scientific Research Publishing is an academic publisher of open access journals. It also publishes academic books and conference proceedings. SCIRP currently has more than 200 open access journals in the areas of science, technology and medicine.",
    "title": "Article Citations - References - Scientific Research Publishing",
    "URL": "https://www.scirp.org/reference/referencespapers?referenceid",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.scirp.org/reference/referencespapers?referenceid"
  },
  {
    "id": "1Bxyvu5tQ",
    "type": "paper-conference",
    "abstract": "Among the main chemical constituents of the human body--and, in fact, of all living things--are proteins. In addition to serving as component structural parts of many types of living tissues, the proteins are &lt;u&gt;enzymes&lt;/u&gt; that are necessary in order that the chemical reactions which comprise the life processes may occur. The protein enzymes act to \"decode\" the message of the genes, interpreting this message in terms of specific chemical reactions which determine the physical and functional characteristics of the organism. Thus proteins play a uniquely vital role in the evolution, ontogeny, and maintenance of living organisms. It therefore becomes important when studying the basis of life processes to know the structure of the proteins themselves.",
    "container-title": "Proceedings of the December 4-6, 1962, fall joint computer conference on - AFIPS '62 (Fall)",
    "DOI": "10.1145/1461518.1461546",
    "event-place": "Philadelphia, Pennsylvania",
    "event-title": "the December 4-6, 1962, fall joint computer conference",
    "language": "en",
    "page": "262-274",
    "publisher": "ACM Press",
    "publisher-place": "Philadelphia, Pennsylvania",
    "source": "Semantic Scholar",
    "title": "Comprotein: a computer program to aid primary protein structure determination",
    "title-short": "Comprotein",
    "URL": "http://portal.acm.org/citation.cfm?doid=1461518.1461546",
    "author": [
      {
        "family": "Dayhoff",
        "given": "Margaret Oakley"
      },
      {
        "family": "Ledley",
        "given": "Robert S."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1962"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.semanticscholar.org/paper/Comprotein%3A-a-computer-program-to-aid-primary-Dayhoff-Ledley/5c73ddc7e6e3e142736210c8a2e71cc6e647bc41"
  },
  {
    "id": "4RZT0xlw",
    "type": "paper-conference",
    "abstract": "Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).",
    "source": "Semantic Scholar",
    "title": "Improving Language Understanding by Generative Pre-Training",
    "URL": "https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035",
    "author": [
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Narasimhan",
        "given": "Karthik"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2018"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035"
  },
  {
    "id": "BUVSzC66",
    "type": "paper-conference",
    "abstract": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
    "source": "Semantic Scholar",
    "title": "Language Models are Unsupervised Multitask Learners",
    "URL": "https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe",
    "author": [
      {
        "family": "Radford",
        "given": "Alec"
      },
      {
        "family": "Wu",
        "given": "Jeff"
      },
      {
        "family": "Child",
        "given": "R."
      },
      {
        "family": "Luan",
        "given": "D."
      },
      {
        "family": "Amodei",
        "given": "Dario"
      },
      {
        "family": "Sutskever",
        "given": "I."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe"
  },
  {
    "id": "bUBwlqml",
    "type": "article-journal",
    "abstract": "Suggestions for the behavior of expert systems and the responsibility of designers to their users are proposed and Simplicity is highly recommended. The user interface to an expert system shares many design objectives and methods with the interface to a computer system of any sort. Nevertheless, significant aspects of behavior and user expectation are peculiar to expert systems and their users. These considerations are discussed here with examples from an actual system. Guidelines for the behavior of expert systems and the responsibility of designers to their users are proposed. Simplicity is highly recommended. Entia non sunt multiplicanda praete necessitatem.",
    "container-title": "The AI Magazine",
    "language": "en",
    "source": "www.semanticscholar.org",
    "title": "On Interface Requirements for Expert Systems",
    "URL": "https://www.semanticscholar.org/paper/On-Interface-Requirements-for-Expert-Systems-Wexelblat/291bffa7fec4fafff62462d015dd86c466273d4c",
    "author": [
      {
        "family": "Wexelblat",
        "given": "R."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1989"
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.semanticscholar.org/paper/On-Interface-Requirements-for-Expert-Systems-Wexelblat/291bffa7fec4fafff62462d015dd86c466273d4c"
  },
  {
    "id": "17px28BEq",
    "type": "article-journal",
    "abstract": "With the rapid progress of AI in both academia and industry, Deep Learning has been widely introduced into various areas in drug discovery to accelerate its pace and cut R&D costs. Among all the problems in drug discovery, molecular property prediction has been one of the most important problems. Unlike general Deep Learning applications, the scale of labeled data is limited in molecular property prediction. To better solve this problem, Deep Learning methods have started focusing on how to utilize tremendous unlabeled data to improve the prediction performance on small-scale labeled data. In this paper, we propose a semi-supervised model named SMILES-BERT, which consists of attention mechanism based Transformer Layer. A large-scale unlabeled data has been used to pre-train the model through a Masked SMILES Recovery task. Then the pre-trained model could easily be generalized into different molecular property prediction tasks via fine-tuning. In the experiments, the proposed SMILES-BERT outperforms the state-of-the-art methods on all three datasets, showing the effectiveness of our unsupervised pre-training and great generalization capability of the pre-trained model.",
    "container-title": "Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics",
    "DOI": "10.1145/3307339.3342186",
    "language": "en",
    "page": "429-436",
    "source": "Semantic Scholar",
    "title": "SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction",
    "title-short": "SMILES-BERT",
    "URL": "https://dl.acm.org/doi/10.1145/3307339.3342186",
    "author": [
      {
        "family": "Wang",
        "given": "Sheng"
      },
      {
        "family": "Guo",
        "given": "Yuzhi"
      },
      {
        "family": "Wang",
        "given": "Yuhong"
      },
      {
        "family": "Sun",
        "given": "Hongmao"
      },
      {
        "family": "Huang",
        "given": "Junzhou"
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "2019",
          9,
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.semanticscholar.org/paper/SMILES-BERT%3A-Large-Scale-Unsupervised-Pre-Training-Wang-Guo/3d99747cc3e13d22f21e02c35e82b57d2e351e2a"
  },
  {
    "id": "7LBeQ0qG",
    "type": "webpage",
    "abstract": "Enhancers are small segments of DNA that bind to proteins (transcription factors) and the transcription of a gene is strengthened after binding to the protein, thus playing an essential role in gene expression. Recently, machine learning-based …",
    "container-title": "springerprofessional.de",
    "language": "en",
    "title": "iEnhancer-BERT: A Novel Transfer Learning Architecture Based on DNA-Language Model for Identifying Enhancers and Their Strength",
    "title-short": "iEnhancer-BERT",
    "URL": "https://www.springerprofessional.de/en/ienhancer-bert-a-novel-transfer-learning-architecture-based-on-d/23365796",
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.springerprofessional.de/en/ienhancer-bert-a-novel-transfer-learning-architecture-based-on-d/23365796"
  },
  {
    "id": "UglDtG3f",
    "type": "article-journal",
    "container-title": "Ergonomics",
    "DOI": "10.1080/00140138208924954",
    "ISSN": "0014-0139, 1366-5847",
    "issue": "4",
    "journalAbbreviation": "Ergonomics",
    "language": "en",
    "page": "269-284",
    "source": "DOI.org (Crossref)",
    "title": "The BLEND system Programme for the study of some ‘electronic journals’∗",
    "URL": "http://www.tandfonline.com/doi/abs/10.1080/00140138208924954",
    "volume": "25",
    "author": [
      {
        "family": "Shackel",
        "given": "B."
      }
    ],
    "accessed": {
      "date-parts": [
        [
          "2024",
          6,
          16
        ]
      ]
    },
    "issued": {
      "date-parts": [
        [
          "1982",
          4
        ]
      ]
    },
    "note": "This CSL Item was generated by Manubot v0.5.5 from its persistent identifier (standard_id).\nstandard_id: url:https://www.tandfonline.com/doi/abs/10.1080/00140138208924954"
  }
]
