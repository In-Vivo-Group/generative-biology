<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Samuel A. Donkor" />
  <meta name="author" content="Matthew E. Walsh" />
  <meta name="author" content="Alexander J. Titus" />
  <meta name="dcterms.date" content="2024-06-19" />
  <meta name="keywords" content="Artificial Intelligence, Generative AI, Machine Learning, Biology" />
  <title>Computing in the Life Sciences: From Early Algorithms to Modern AI</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta property="og:type" content="article" />
  <meta name="dc.title" content="Computing in the Life Sciences: From Early Algorithms to Modern AI" />
  <meta name="citation_title" content="Computing in the Life Sciences: From Early Algorithms to Modern AI" />
  <meta property="og:title" content="Computing in the Life Sciences: From Early Algorithms to Modern AI" />
  <meta property="twitter:title" content="Computing in the Life Sciences: From Early Algorithms to Modern AI" />
  <meta name="dc.date" content="2024-06-19" />
  <meta name="citation_publication_date" content="2024-06-19" />
  <meta property="article:published_time" content="2024-06-19" />
  <meta name="dc.modified" content="2024-06-19T03:10:11+00:00" />
  <meta property="article:modified_time" content="2024-06-19T03:10:11+00:00" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Samuel A. Donkor" />
  <meta name="citation_author_institution" content="In Vivo Group" />
  <meta name="citation_author" content="Matthew E. Walsh" />
  <meta name="citation_author_institution" content="U.S. National Security Commission on Emerging Biotechnology" />
  <meta name="citation_author_institution" content="Department of Environmental Health and Engineering, Johns Hopkins Bloomberg School of Public Health" />
  <meta name="citation_author_orcid" content="0000-0003-1514-7761" />
  <meta name="citation_author" content="Alexander J. Titus" />
  <meta name="citation_author_institution" content="In Vivo Group" />
  <meta name="citation_author_institution" content="U.S. National Security Commission on Emerging Biotechnology" />
  <meta name="citation_author_institution" content="Information Sciences Institute &amp; Iovine and Young Academy, University of Southern California" />
  <meta name="citation_author_orcid" content="0000-0002-0145-9564" />
  <link rel="canonical" href="https://In-Vivo-Group.github.io/generative-biology/" />
  <meta property="og:url" content="https://In-Vivo-Group.github.io/generative-biology/" />
  <meta property="twitter:url" content="https://In-Vivo-Group.github.io/generative-biology/" />
  <meta name="citation_fulltext_html_url" content="https://In-Vivo-Group.github.io/generative-biology/" />
  <meta name="citation_pdf_url" content="https://In-Vivo-Group.github.io/generative-biology/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://In-Vivo-Group.github.io/generative-biology/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://In-Vivo-Group.github.io/generative-biology/v/af774ed427133155fffd197dc3c27a291ad71d4a/" />
  <meta name="manubot_html_url_versioned" content="https://In-Vivo-Group.github.io/generative-biology/v/af774ed427133155fffd197dc3c27a291ad71d4a/" />
  <meta name="manubot_pdf_url_versioned" content="https://In-Vivo-Group.github.io/generative-biology/v/af774ed427133155fffd197dc3c27a291ad71d4a/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Computing in the Life Sciences: From Early Algorithms to Modern AI</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://In-Vivo-Group.github.io/generative-biology/v/af774ed427133155fffd197dc3c27a291ad71d4a/">permalink</a>)
was automatically generated
from <a href="https://github.com/In-Vivo-Group/generative-biology/tree/af774ed427133155fffd197dc3c27a291ad71d4a">In-Vivo-Group/generative-biology@af774ed</a>
on June 19, 2024.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Samuel A. Donkor</strong>
<br>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/samadon1">samadon1</a>
<br>
<small>
In Vivo Group
</small></p></li>
<li><p><strong>Matthew E. Walsh</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0003-1514-7761">0000-0003-1514-7761</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/mwalsh52">mwalsh52</a>
<br>
<small>
U.S. National Security Commission on Emerging Biotechnology; Department of Environmental Health and Engineering, Johns Hopkins Bloomberg School of Public Health
</small></p></li>
<li><p><strong>Alexander J. Titus</strong>
<sup><a href="#correspondence">✉</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0002-0145-9564">0000-0002-0145-9564</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/alexandertitus">alexandertitus</a>
<br>
<small>
In Vivo Group; U.S. National Security Commission on Emerging Biotechnology; Information Sciences Institute &amp; Iovine and Young Academy, University of Southern California
</small></p></li>
</ul>
<div id="correspondence">
<p>✉ — Correspondence possible via <a href="https://github.com/In-Vivo-Group/generative-biology/issues">GitHub Issues</a>
or email to
Alexander J. Titus &lt;publications@theinvivogroup.com&gt;.</p>
</div>
<h2 id="abstract">Abstract</h2>
<p>Computing in the life sciences has undergone a transformative evolution, from early computational models in the 1950s to the applications of artificial intelligence (AI) and machine learning (ML) seen today. This paper highlights key milestones and technological advancements through the historical development of computing in the life sciences. The discussion includes the inception of computational models for biological processes, the advent of bioinformatics tools, and the integration of AI/ML in modern life sciences research. Attention is given to AI-enabled tools used in the life sciences, such as scientific large language models and bio-AI tools, examining their capabilities, limitations, and impact to biological risk. This paper seeks to clarify and establish essential terminology and concepts to ensure informed decision-making and effective communication across disciplines.</p>
<p><em>The views and opinions expressed within this manuscript are those of the authors and do not necessarily reflect the views and opinions of any organization the authors are affiliated with.</em></p>
<h1 id="executive-summary">Executive Summary</h1>
<p>The integration of computing technologies into the life sciences has revolutionized the field, enabling unprecedented advancements in biological research and applications. This manuscript traces the historical milestones and technological advancements that have shaped this transformative journey.</p>
<p>The early days of computing in the life sciences saw the use of primitive computers for population genetics calculations and biological modeling in the 1950s. This period marked the rise of computational biology, with computers becoming indispensable for protein crystallography and the determination of three-dimensional protein structures.</p>
<p>The 1960s and 1970s witnessed significant developments, including the shift from protein to DNA analysis, driven by the advent of DNA sequencing methods. Dynamic programming algorithms for sequence alignment and pioneering methods for inferring phylogenetic trees from DNA sequences emerged during this time.</p>
<p>The 1980s and 1990s were pivotal, characterized by parallel advancements in molecular biology and computing. Gene targeting techniques, the polymerase chain reaction (PCR), and the emergence of bioinformatics software suites propelled the field forward. The completion of the Haemophilus influenzae genome in the mid-1990s ushered in the genomic era, culminating in the publication of the human genome at the turn of the century.</p>
<p>The last two decades have seen the integration of artificial intelligence (AI) and machine learning (ML) into the life sciences, revolutionizing data analysis, drug discovery, and personalized medicine. AI models, from early expert systems to modern deep learning architectures, have enhanced our ability to predict protein structures, analyze genomic data, and design novel biological entities.</p>
<p>The manuscript delves into the various categories of AI-enabled tools used in the life sciences, focusing on large language models (LLMs) and biological design tools (BDTs). LLMs, such as GPT and BERT, have been adapted for the life sciences domain, giving rise to specialized models like scientific LLMs (Sci-LLMs), protein LLMs (Prot-LLMs), and genomic LLMs (Gene-LLMs). These models excel at tasks such as processing scientific literature, predicting protein structures and functions, and analyzing genomic data.</p>
<p>BDTs, on the other hand, aid in the design of proteins, viral vectors, and other biological agents. Protein structure prediction tools, like AlphaFold and RoseTTAFold, have revolutionized the field by drastically reducing the time required to determine protein structures. Other subcategories of BDTs include protein sequence design tools, small molecule design tools, vaccine design tools, and genetic modification tools, each serving specific purposes in biological research and applications.</p>
<p>The manuscript also highlights the importance of benchmarking and evaluating AI models in the life sciences. Bloom’s taxonomy and frameworks like SciEval and KnowEval are used to assess the capabilities of LLMs across different cognitive levels and scientific knowledge domains. Specific benchmarks for Sci-LLMs, Prot-LLMs, Gene-LLMs, and multimodal Sci-LLMs are discussed, emphasizing the need for rigorous evaluation to ensure the reliability and effectiveness of these tools.</p>
<p>While the integration of AI in the life sciences has enabled rapid progress, it also presents potential risks and limitations. Inaccurate outputs from AI models, stemming from biased or incomplete training data, can misguide researchers and waste valuable resources. The potential misuse of AI in creating harmful biological agents raises significant biosecurity concerns. Ethical considerations, such as data privacy, informed consent, and algorithmic bias, must be addressed to ensure responsible and beneficial use of AI in the life sciences.</p>
<p>Looking ahead, the manuscript underscores the need for more comprehensive benchmarks that assess AI models’ performance in real-world applications and their ability to adapt to evolving scientific knowledge. Techniques like red teaming, blue teaming, and violet teaming are proposed to build resilient AI systems that minimize harm and maximize benefit. The integration of Machine Learning Security Operations (MLSecOps) is also highlighted as a crucial step in ensuring the safety and security of AI models in the life sciences.</p>
<p>In conclusion, the integration of computing technologies into the life sciences has transformed the field, enabling unprecedented advancements in biological research and applications. From the early days of computational modeling to the sophisticated AI-driven tools of today, this journey has been marked by historical milestones and technological breakthroughs. As we move forward, harnessing the power of AI, cloud computing, and other emerging technologies will continue to drive innovation, offering new solutions to complex biological problems and improving human health. However, navigating the challenges and ethical considerations associated with AI in the life sciences will be crucial to ensure its responsible and beneficial use.</p>
<h1 id="introduction">Introduction</h1>
<p>Computing technologies have become indispensable to life scientists, changing how research is conducted and expanding the scope of scientific discovery. The history of computing in the life sciences is marked by significant milestones that have advanced research, including early algorithmic approaches to the application of artificial intelligence (AI) and machine learning (ML). Early uses of computers in the 1950s for population genetics calculations and the pioneering work of Alan Turing in biological morphogenesis set the stage for subsequent developments. Over the following decades, computational biology evolved from basic protein structure analysis to complex genomic studies, driven by advancements in DNA sequencing and computing.</p>
<p>Today, the terms AI, ML, deep learning, and large language models (LLMs) are often used interchangeably in the life sciences. Although these terms are related, they each have distinct meanings (Figure 2). AI broadly refers to machines designed to mimic human intelligence. ML is a subset of AI focused on algorithms that improve through experience. Deep learning is a subset of ML involving neural networks with many layers that can learn from vast amounts of data, and LLMs, such as GPT (Generative Pre-trained Transformer) models like ChatGPT, are a specific type of deep learning that excel in understanding and generating human-like text. By processing and analyzing biological data at unprecedented scale and speeds, these technologies have advanced fields such as bioinformatics, structural biology, and genomics.
Understanding distinctions among AI-related nomenclature is crucial as technology development accelerates. Decisions about funding, regulation, new product development, and the implementation of new technologies rely on an accurate understanding of what these technologies can and cannot do. A nuanced understanding of the capabilities and limitations of AI, ML, LLMs and other computational tools can help to correctly estimate their potential and effectively utilize valuable resources.</p>
<p>This paper provides an overview of historical context, current applications, and future directions of computing in the life sciences. By explaining key terms, concepts, and timelines, we aim to bridge the knowledge gap between practitioners and stakeholders, fostering an environment for progress that supports scientific innovation and public benefit outcomes.</p>
<h1 id="computers-algorithms-and-the-internet">Computers, Algorithms and the Internet</h1>
<h2 id="s-and-1960s-early-computers-and-algorithms">1950s and 1960s: Early computers and algorithms</h2>
<p>Computers were used in the early 1950s for population genetics calculations <span class="citation" data-cites="gldEpQ9">[<a href="#ref-gldEpQ9" role="doc-biblioref">1</a>]</span>. The inception of computational modeling in biology coincides with the origins of computer science itself. British mathematician and logician Alan Turing, often referred to as “the father of computing”, used primitive computers to implement a model of biological morphogenesis (the emergence of pattern and shape in living organisms) in 1952 <span class="citation" data-cites="UK6rEYVY">[<a href="#ref-UK6rEYVY" role="doc-biblioref">2</a>]</span>. At about the same time, a computer called MANIAC was used for measuring speculative genetic codes; it was originally built for weaponry research at the Los Alamos National Laboratory in New Mexico <span class="citation" data-cites="RqzQpRL0">[<a href="#ref-RqzQpRL0" role="doc-biblioref">3</a>]</span>.</p>
<p>Computers were used for the study of protein structure by the 1960s, and other increasingly diverse analyses. These developments marked the rise of the computational biology field, stemming from research focused on protein crystallography, in which scientists found computers indispensable for carrying out laborious Fourier analyses to determine the three-dimensional structure of proteins <span class="citation" data-cites="AtEWfwCG WmbDqOWv">[<a href="#ref-AtEWfwCG" role="doc-biblioref">4</a>,<a href="#ref-WmbDqOWv" role="doc-biblioref">5</a>]</span>.</p>
<p>In addition to advances in determination of protein structures through crystallography, the first sequence of protein, insulin, was published <span class="citation" data-cites="3t4YHcSd Htmw8erk">[<a href="#ref-3t4YHcSd" role="doc-biblioref">6</a>,<a href="#ref-Htmw8erk" role="doc-biblioref">7</a>]</span>. More efficient protein sequencing methods, such as the Edman degradation technique <span class="citation" data-cites="Dq9jMQ6v">[<a href="#ref-Dq9jMQ6v" role="doc-biblioref">8</a>]</span>, enabled sequencing 15 different proteins over a decade <span class="citation" data-cites="prCR1hpf">[<a href="#ref-prCR1hpf" role="doc-biblioref">9</a>]</span>. COMPROTEIN, one of the first bioinformatics softwares developed in the early 1960s, was designed to overcome the limitations of Edman sequencing <span class="citation" data-cites="1Bxyvu5tQ">[<a href="#ref-1Bxyvu5tQ" role="doc-biblioref">10</a>]</span>. In an effort to simplify the handling of protein sequence data for the COMPROTEIN software, a one-letter amino acid code was developed <span class="citation" data-cites="ibvKtnrl">[<a href="#ref-ibvKtnrl" role="doc-biblioref">11</a>]</span>. This one-letter code was first used in the Atlas of Protein Sequence and Structure <span class="citation" data-cites="YYcdJkZI">[<a href="#ref-YYcdJkZI" role="doc-biblioref">12</a>]</span>, the first biological sequence database, laying the groundwork for paleogenetic studies.</p>
<p>Development of methods to compare protein sequences followed. The Needleman-Wunsch algorithm <span class="citation" data-cites="C1oThCRN">[<a href="#ref-C1oThCRN" role="doc-biblioref">13</a>]</span>, the first dynamic programming algorithm developed for pairwise protein sequence alignments, was introduced in the 1970s. Multiple sequence alignment (MSA) algorithms followed in the early 1980s. Progressive sequence alignment was introduced by Feng and Doolittle in 1987 <span class="citation" data-cites="9IbNBDhy">[<a href="#ref-9IbNBDhy" role="doc-biblioref">14</a>]</span>. The MSA software CLUSTAL, a simplification of the Feng-Doolittle algorithm <span class="citation" data-cites="r9luxm5X">[<a href="#ref-r9luxm5X" role="doc-biblioref">15</a>]</span> was developed in 1988. It is still used and maintained to this day <span class="citation" data-cites="10odPMDxs">[<a href="#ref-10odPMDxs" role="doc-biblioref">16</a>]</span>.</p>
<h2 id="s-from-protein-to-dna-analysis">1970s: From protein to DNA analysis</h2>
<p>The deciphering of all 64 triplet codons of the genetic code in 196817 fueled a desire to efficiently determine the sequence of DNA that existed into the 1970s. This desire led to the development of cost-efficient DNA sequencing methods, such as the Maxam-Gilbert and Sanger sequencing techniques in the mid-1970s <span class="citation" data-cites="NQbXc701 3t4YHcSd Htmw8erk">[<a href="#ref-3t4YHcSd" role="doc-biblioref">6</a>,<a href="#ref-Htmw8erk" role="doc-biblioref">7</a>,<a href="#ref-NQbXc701" role="doc-biblioref">17</a>]</span>. With this new ability to generate DNA sequence data, a paradigm shift from protein analysis to DNA analysis occurred in the late 1970s. Concurrently, concerns over recombinant DNA research led to safety protocols established during the 1975 Asilomar conference <span class="citation" data-cites="10pibmvLC">[<a href="#ref-10pibmvLC" role="doc-biblioref">18</a>]</span>.</p>
<p>New DNA sequencing techniques resulted in significantly more data to be analyzed, a task at which computation could help. The first software dedicated to analyzing Sanger sequencing reads was published in 1979 <span class="citation" data-cites="M8aKGlSe">[<a href="#ref-M8aKGlSe" role="doc-biblioref">19</a>]</span>. DNA sequences began to be utilized in phylogenetic inference with pioneering methods like maximum likelihood for inferring phylogenetic trees from DNA sequences <span class="citation" data-cites="nzs4mjEF">[<a href="#ref-nzs4mjEF" role="doc-biblioref">20</a>]</span>. Several bioinformatics tools and statistical methods were developed following this work. The adoption of Bayesian statistics in molecular phylogeny in the 1990s was inspired by this <span class="citation" data-cites="1CARqSM3b">[<a href="#ref-1CARqSM3b" role="doc-biblioref">21</a>]</span> and is still commonly used in biology today <span class="citation" data-cites="17nhyKsKm">[<a href="#ref-17nhyKsKm" role="doc-biblioref">22</a>]</span>. Yet, numerous computational limitations needed to be overcome during the latter half of the 1970s to expand the utilization of computing in the life sciences, especially in DNA analysis. The subsequent decade proved instrumental in addressing these challenges.</p>
<figure>
<img src="https://github.com/samadon1/generative-biology/assets/56901167/4d765e41-8a46-4eb8-9b45-c206e0ab22e3" alt="Figure 1: The history of parallel advancements in computing and the life sciences: A timeline of major milestones." />
<figcaption aria-hidden="true"><strong>Figure 1</strong>: The history of parallel advancements in computing and the life sciences: A timeline of major milestones.</figcaption>
</figure>
<h2 id="s-simultaneous-advances-in-computing-and-biology">1980s: Simultaneous advances in computing and biology</h2>
<p>Parallel advancements in biology and computing propelled bioinformatics forward during the 1980s and 1990s. Molecular techniques like gene targeting and amplification, using enzymes like restriction endonucleases and DNA ligases, laid the groundwork for genetic engineering <span class="citation" data-cites="10pibmvLC">[<a href="#ref-10pibmvLC" role="doc-biblioref">18</a>]</span>. The polymerase chain reaction (PCR) transformed gene amplification, while innovations like Taq polymerase and thermal cyclers optimized the process <span class="citation" data-cites="CvVpaf4a">[<a href="#ref-CvVpaf4a" role="doc-biblioref">23</a>]</span>.</p>
<p>Computing accessibility surged with microcomputers like the Commodore PET, Apple II, and Tandy TRS-80, along with bioinformatics software like the GCG software suite <span class="citation" data-cites="12FKql4zv">[<a href="#ref-12FKql4zv" role="doc-biblioref">24</a>]</span> and DNASTAR <span class="citation" data-cites="17EunIaqv">[<a href="#ref-17EunIaqv" role="doc-biblioref">25</a>]</span>, another sequence manipulation suite capable of assembling and analyzing Sanger sequencing data. Other sequence manipulation suites were developed to run on CP/M, Apple II, and Macintosh computers <span class="citation" data-cites="dSNqAXK0">[<a href="#ref-dSNqAXK0" role="doc-biblioref">26</a>]</span> in the years 1984 and 1985. Free code copies of this software were offered on demand by some developers. This propelled an upcoming software-sharing movement in the programming world <span class="citation" data-cites="17gPkdn86 WD5AIwcy">[<a href="#ref-17gPkdn86" role="doc-biblioref">27</a>,<a href="#ref-WD5AIwcy" role="doc-biblioref">28</a>]</span>.</p>
<p>The free software movement, led by the GNU project, promoted open-source bioinformatics tools. Major sequence databases (EMBL, GenBank, DDBJ) standardized data formatting and enabled global sharing. Bioinformatics journals, like CABIOS, which is now known as Bioinformatics (Oxford, England) accentuated computational methods’ importance. Desktop workstations with Unix-like systems and scripting languages aided bioinformatics analyses, and scripting languages simplified tool development.</p>
<h2 id="s-the-genomics-era-and-web-based-bioinformatics">1990s: The genomics era and web-based bioinformatics</h2>
<p>The genomics era began in the mid-1990s with the complete sequencing of the Haemophilus influenzae genome <span class="citation" data-cites="11GoDnHp3">[<a href="#ref-11GoDnHp3" role="doc-biblioref">29</a>]</span>, initiating genome-scale analyses. This milestone was followed by the publication of the human genome at the beginning of the 21st century, which served as the definitive catalyst for the genomic era <span class="citation" data-cites="7GkBaAWF">[<a href="#ref-7GkBaAWF" role="doc-biblioref">30</a>]</span>. This transformative event spurred the design and development of several specialized Perl-based software to assemble whole-genome sequencing reads: PHRAP <span class="citation" data-cites="Ero6Muzo">[<a href="#ref-Ero6Muzo" role="doc-biblioref">31</a>]</span>, Celera Assembler <span class="citation" data-cites="18guJvxxN">[<a href="#ref-18guJvxxN" role="doc-biblioref">32</a>]</span> among others.</p>
<p>Tim Berners-Lee’s pioneering work at CERN in the early 1990s resulted in the World Wide Web, transforming global communication and ushering in an era of unprecedented access to information. With the advent of the internet, researchers gained a powerful platform to share and access vast amounts of biological data efficiently. This facilitated collaborative efforts in biology and genomics, leading to the establishment of foundational databases such as the EMBL Nucleotide Sequence Data Library <span class="citation" data-cites="i2bVokif">[<a href="#ref-i2bVokif" role="doc-biblioref">33</a>]</span> and the GenBank database became the responsibility of the NCBI <span class="citation" data-cites="Tx5CQPm8">[<a href="#ref-Tx5CQPm8" role="doc-biblioref">34</a>]</span> in 1992. Also, the famous NCBI website came online in 1994, featuring the efficient pairwise alignment tool BLAST <span class="citation" data-cites="qPG24gP6">[<a href="#ref-qPG24gP6" role="doc-biblioref">35</a>]</span>. After that, the world saw the birth of major databases we still rely on today: Genomes (1995), PubMed (1997), and Human Genome (1999) <span class="citation" data-cites="14cnpLsxs 1DNwvIpHH yOdF9F1J">[<a href="#ref-14cnpLsxs" role="doc-biblioref">36</a>,<a href="#ref-1DNwvIpHH" role="doc-biblioref">37</a>,<a href="#ref-yOdF9F1J" role="doc-biblioref">38</a>]</span>.</p>
<p>The proliferation of web-based resources transformed access to bioinformatics tools, democratizing their availability and usability for researchers worldwide. Through the development of web platforms, bioinformatics tools became more user-friendly and accessible. This shift enabled researchers to interact with sophisticated analytical tools without needing extensive computational expertise or access to specialized hardware. Consequently, the widespread adoption of web-based bioinformatics resources facilitated broader participation in genomic and molecular research, accelerating scientific discovery and collaboration on a global scale. Graphical web servers emerged as a convenient alternative to traditional UNIX-based systems, simplifying data analysis without the need for complex installations. The continued relevance of servers for scientific purposes is exemplified by the AlphaFold Server which uses the latest AlphaFold 3 model <span class="citation" data-cites="XVMV1nZ">[<a href="#ref-XVMV1nZ" role="doc-biblioref">39</a>]</span>, released in 2024, to provide highly accurate biomolecular structure predictions in a unified platform.</p>
<p>The internet facilitated the dissemination of scientific research through online publications, challenging traditional print-based methods. Early initiatives like BLEND <span class="citation" data-cites="UglDtG3f">[<a href="#ref-UglDtG3f" role="doc-biblioref">40</a>]</span> paved the way for internet-based scientific publishing by shedding insights into the potentials and obstacles associated with using the internet for scientific publications. This study paved the way for leveraging the Internet for both data set storage and dissemination, leading up to the establishment of preprint servers like arXiv (est. 1991) <span class="citation" data-cites="jMSKwR4o">[<a href="#ref-jMSKwR4o" role="doc-biblioref">41</a>]</span> and bioRxiv (est. 2013) <span class="citation" data-cites="iWYRgRGv">[<a href="#ref-iWYRgRGv" role="doc-biblioref">42</a>]</span> which changed the way scientific findings are shared and accessed. These platforms democratized access to scientific knowledge by enabling researchers to share their work rapidly and openly, facilitating interdisciplinary collaborations and the cross-pollination of ideas.</p>
<p>The experimental determination of the first three-dimensional structure of a protein, specifically, myoglobin, occurred in 1958 via X-ray diffraction <span class="citation" data-cites="AtEWfwCG">[<a href="#ref-AtEWfwCG" role="doc-biblioref">4</a>]</span>. However, earlier groundwork by Pauling and Corey with the publication of two articles in 1951 that reported the prediction of α-helices and β-sheets <span class="citation" data-cites="4gIVWSCR">[<a href="#ref-4gIVWSCR" role="doc-biblioref">43</a>]</span> laid the foundation for predicting protein structures. Similar to advances in other biological sciences, the utilization of computers has made it feasible to conduct calculations aimed at predicting the secondary and tertiary structure of proteins, with varying levels of confidence. This capability has been notably enhanced by the development of fold recognition algorithms, also known as threading algorithms <span class="citation" data-cites="cc2nygki eM1kmLmc">[<a href="#ref-cc2nygki" role="doc-biblioref">44</a>,<a href="#ref-eM1kmLmc" role="doc-biblioref">45</a>]</span>. However, proteins are dynamic entities, requiring advanced biophysical models to describe their interactions and movements accurately. Force fields have been formulated to describe the interactions among atoms, enabling the introduction of tools for modeling the molecular dynamics of proteins during the 1990s <span class="citation" data-cites="Z7e2T8go">[<a href="#ref-Z7e2T8go" role="doc-biblioref">46</a>]</span>. Used to study the behavior and interactions of atoms and molecules over time, molecular dynamics simulations calculate the positions and velocities of atoms based on physical principles. Despite the theoretical advancements and availability of tools, executing molecular dynamics simulations remained challenging in practice due to the substantial computational resources they demanded.</p>
<p>Graphical processing Units (GPUs) have made molecular dynamics more accessible <span class="citation" data-cites="sT4CEJLS">[<a href="#ref-sT4CEJLS" role="doc-biblioref">47</a>]</span>, with applications extending to other bioinformatics fields requiring intensive computation. However, the internet’s role in data dissemination, coupled with increasing computational power, has led to the proliferation of ‘Big Data’ in bioinformatics.</p>
<h2 id="s-high-throughput-sequencing-and-big-data">2000s: High-throughput sequencing and big data</h2>
<p>Second-generation sequencing technologies democratized high-throughput bioinformatics. For example ‘454’ pyrosequencing, a high-throughput DNA sequencing technique played a significant role in advancing genomics research by enabling rapid and cost-effective sequencing of DNA samples, particularly for applications such as whole-genome sequencing <span class="citation" data-cites="16JwdMPBH">[<a href="#ref-16JwdMPBH" role="doc-biblioref">48</a>]</span>, but computational challenges arose with increased data volumes. Decreasing sequencing costs resulted in more data being generated, emphasizing data organization and accessibility. Specialized repositories and standardization efforts were needed to ensure data interoperability. High-performance computing adaptation became vital to address the increased amounts of data within bioinformatics projects. The surge in bioinformatics projects, accompanied by a vast influx of data, prompted adjustments from funding bodies to accommodate the demand for high-performance computing resources and collaborative initiatives.</p>
<p>While basic computer setups suffice for some projects, others demand complex infrastructures and substantial expertise. Government-sponsored entities like <a href="https://computecanada.ca/">Compute Canada</a>, <a href="https://esd.ny.gov/doing-business-ny/new-york-state-high-performance-computing-program">New York State’s High-Performance Computing Program</a>, <a href="https://www.etp4hpc.eu/">The European Technology Platform for High-Performance Computing</a>, and <a href="https://www.nchc.org.tw/?langid=2">National Center for High-Performance Computing</a> served researchers’ computational needs. Companies like Amazon, Microsoft, and Google, among many others, offer bioinformatics and life sciences services, emphasizing the field’s importance.</p>
<h3 id="table-1.-organizations-providing-high-performance-computing-resources-for-bioinformatics-and-life-sciences"><strong>Table 1.</strong> Organizations providing High-Performance Computing Resources for Bioinformatics and Life Sciences</h3>
<table>
<colgroup>
<col style="width: 40%" />
<col style="width: 60%" />
</colgroup>
<thead>
<tr class="header">
<th>Organization</th>
<th>Computing Resources</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Compute Canada</strong></td>
<td>Provides high-performance computing resources and support services to researchers and innovators across Canada. They offer supercomputers, cloud platforms, data storage, and training programs to advance scientific research and innovation in various fields.</td>
</tr>
<tr class="even">
<td><strong>New York State’s High-Performance Computing Program</strong></td>
<td>Provides researchers, businesses, and educational institutions with access to high-performance computing (HPC) resources and expertise to support their computational research and development efforts.</td>
</tr>
<tr class="odd">
<td><strong>The European Technology Platform for High-Performance Computing</strong></td>
<td>Fosters collaboration among industry, research, and academic stakeholders to advance high-performance computing (HPC) technology in Europe.</td>
</tr>
<tr class="even">
<td><strong>National Center for High-Performance Computing</strong></td>
<td>Facility for high-performance computing (HPC) resources including large-scale computational science and engineering, cluster and grid computing, middleware development, visualization and virtual reality, data storage, networking, and HPC-related training.</td>
</tr>
<tr class="odd">
<td><strong>National Center for Supercomputing Applications</strong></td>
<td>Offers high-performance computing resources such as the Blue Waters supercomputer, provides advanced data storage solutions, data analysis, and visualization tools, and supports interdisciplinary research in fields such as astrophysics, climate modeling, and genomics.</td>
</tr>
<tr class="even">
<td><strong>Oak Ridge Leadership Computing Facility</strong></td>
<td>Provides supercomputing resources, such as the Summit supercomputer, for scientific research, offers support services including software development, data storage, and visualization, and facilitates research in various fields including climate science, biology, and materials science.</td>
</tr>
<tr class="odd">
<td><strong>Swiss National Supercomputing Centre</strong></td>
<td>Provides high-performance computing systems including the Piz Daint supercomputer, offers cloud computing services, data management, and user support, and facilitates scientific research in areas such as climate modeling, physics, and life sciences.</td>
</tr>
<tr class="even">
<td><strong>Barcelona Supercomputing Center</strong></td>
<td>Provides access to MareNostrum, one of the most powerful supercomputers in Europe, offers resources for high-performance computing, data storage, and computational sciences, and supports research in fields including bioinformatics, computational biology, and engineering.</td>
</tr>
<tr class="odd">
<td><strong>Japan’s RIKEN Center for Computational Science</strong></td>
<td>Houses the Fugaku supercomputer, one of the world’s fastest supercomputers, provides resources for computational science, data processing, and artificial intelligence, and supports research in fields such as life sciences, materials science, and disaster prevention.</td>
</tr>
<tr class="even">
<td><strong>National Supercomputing Centre Singapore</strong></td>
<td>Provides high-performance computing resources and support services, offers data storage, cloud computing, and software development services, and supports research in fields including bioinformatics, environmental modeling, and smart cities.</td>
</tr>
</tbody>
</table>
<p>Community computing platforms democratized participation and expanded bioinformatics research’s reach. Platforms like BOINC enabled broad participation in bioinformatics. Experts can submit computing tasks to BOINC, while non-experts and science enthusiasts can volunteer their computer resources to process these tasks. Several life sciences projects are available through BOINC, including protein-ligand docking, malaria simulations, and protein folding <span class="citation" data-cites="1BWjfloX3">[<a href="#ref-1BWjfloX3" role="doc-biblioref">49</a>]</span>.</p>
<h2 id="the-present-and-future">2010+: The present and future</h2>
<p>The integration of computers into biology has ushered in a new era of research possibilities, allowing for increasingly complex studies. While before, the focus was on individual genes or proteins, advancements today enable the analysis of entire genomes or proteomes <span class="citation" data-cites="1DngqA2eE">[<a href="#ref-1DngqA2eE" role="doc-biblioref">50</a>]</span>. This shift toward a holistic approach in biology is evident in disciplines like genomics, proteomics, and glycomics, which have limited interconnection between them.</p>
<p>The next leap at the intersection of computing and the life sciences lies in modeling entire living organisms and their environments simultaneously, integrating all molecular categories. This has already been achieved in a whole cell model of Mycoplasma genitalium, in which all its genes, products and their known metabolic interactions have been reconstructed <span class="citation" data-cites="FWkpcKEh">[<a href="#ref-FWkpcKEh" role="doc-biblioref">51</a>]</span>. Driven by advancements in measurement techniques, improved computational performance and artificial intelligence (AI) techniques, whole-cell modeling is increasingly becoming realistic and feasible. In contrast to traditional bottom-up approaches relying on molecular interaction networks, a predictive model has been developed for genome-wide phenotypes of budding yeast using deep learning <span class="citation" data-cites="MNG16ntb">[<a href="#ref-MNG16ntb" role="doc-biblioref">52</a>]</span>. The main applications of whole-cell modeling have been in producing useful substances and discovering drugs, such as antimicrobials <span class="citation" data-cites="ZSOb6iY1 EOFcwWBm OHHpDn0z 16yino3ku">[<a href="#ref-ZSOb6iY1" role="doc-biblioref">53</a>,<a href="#ref-EOFcwWBm" role="doc-biblioref">54</a>,<a href="#ref-OHHpDn0z" role="doc-biblioref">55</a>,<a href="#ref-16yino3ku" role="doc-biblioref">56</a>]</span> since whole-cell modeling was first directed toward unicellular organisms. Meanwhile, models of cultured human cells have also been developed, which have found applications in cell differentiation and medical research <span class="citation" data-cites="MgRWKCpM">[<a href="#ref-MgRWKCpM" role="doc-biblioref">57</a>]</span>. The possibility of modeling entire multicellular organisms may not be far off, considering the rapid pace of technological and computational advancements like artificial intelligence (AI) .</p>
<h1 id="artificial-intelligence-ai">Artificial Intelligence (AI)</h1>
<p>Artificial intelligence (AI) refers to a set of tools, techniques and paradigms that enable computers to mimic human behavior and either replicate the decision-making process typically performed by humans or exceed human performance in solving complex tasks independently or with minimal human intervention <span class="citation" data-cites="AviPiwRn">[<a href="#ref-AviPiwRn" role="doc-biblioref">58</a>]</span>. AI is concerned with a variety of central problems, including knowledge representation, reasoning, learning, planning, perception, and communication. It also refers to a variety of tools and methods, including case-based reasoning, rule-based systems, genetic algorithms, fuzzy models, and multi-agent systems <span class="citation" data-cites="Gz18T5lX">[<a href="#ref-Gz18T5lX" role="doc-biblioref">59</a>]</span>. Early AI research focused primarily on hard-coded statements in formal languages, which a computer can then automatically reason about based on logical inference rules. These computer systems known as expert systems, excelled in specific domains but lacked adaptability. Over time, AI has evolved to include a variety of approaches, each with its own strengths and weaknesses. For instance, expert systems are highly accurate within narrow fields but struggle with tasks outside their programmed knowledge. In contrast, machine learning algorithms can generalize from data and adapt to new situations, though they require large datasets and extensive training. Other AI techniques, such as deep learning, neural networks, and natural language processing also offer their own unique advantages and challenges.</p>
<h2 id="expert-systems">Expert systems</h2>
<p>Expert systems are a type of artificial intelligence (AI) that aims to replicate the decision-making capabilities of human experts in specific domains. They are made of a knowledge base containing domain-specific facts, rules, and heuristics, and an inference engine that applies logical reasoning to this knowledge to draw conclusions or make decisions <span class="citation" data-cites="dae8L6vr">[<a href="#ref-dae8L6vr" role="doc-biblioref">60</a>]</span>. Users are typically able to input queries and receive advice or recommendations through a simplified user interface. The primary user action, which involves pointing and clicking, is known as selecting <span class="citation" data-cites="bUBwlqml">[<a href="#ref-bUBwlqml" role="doc-biblioref">61</a>]</span>.</p>
<p>An expert system for chemical analysis was developed in 1965 by AI researcher Edward Feigenbaum and geneticist Joshua Lederberg. This system was originally known as Heuristic DENDRAL and later as DENDRAL <span class="citation" data-cites="TjcFIKAL">[<a href="#ref-TjcFIKAL" role="doc-biblioref">62</a>]</span>. DENDRAL was developed to analyze molecular structures, particularly those containing elements like carbon, hydrogen, and nitrogen, based on spectrographic data. It proposed molecular structures for the compounds, with accuracy comparable to that of expert chemists.</p>
<p>Edward Shortliffe’s work on MYCIN <span class="citation" data-cites="DOfI6ZOm">[<a href="#ref-DOfI6ZOm" role="doc-biblioref">63</a>]</span> began in 1972 at Stanford University. MYCIN, an expert system, was designed to assist physicians in diagnosing and selecting therapies for patients with bacterial infections, particularly patients with meningitis. It used a rule-based system that analyzed patient symptoms and medical history to suggest appropriate antibiotic treatments. MYCIN exhibited proficiency equivalent to infectious disease doctors.</p>
<p>However, despite their capabilities, the paradigm faces several limitations as humans generally struggle to explicitly articulate all their tacit knowledge that is required to perform complex tasks <span class="citation" data-cites="om2dJrZ8">[<a href="#ref-om2dJrZ8" role="doc-biblioref">64</a>]</span>, leading to challenges such as difficulty in extrapolation, handling out-of-distribution data, managing uncertainty, and addressing biases. These limitations arise because expert systems heavily rely on predefined rules and knowledge encoded by humans. Consequently, the involvement of humans in specifying these parameters is essential but can also introduce limitations due to human cognitive constraints and biases. In contrast, machine learning algorithms overcome some of these limitations by learning from data, and making them more adaptable without relying heavily on explicit human guidance.</p>
<h2 id="machine-learning-and-deep-learning">Machine learning and Deep learning</h2>
<p>Machine learning (ML) is a subset of AI that focuses on the development of algorithms and statistical models that enable computers to perform tasks without being explicitly programmed to do so <span class="citation" data-cites="Q6HnLWXu">[<a href="#ref-Q6HnLWXu" role="doc-biblioref">65</a>]</span>. It involves the use of data and algorithms to imitate the way humans learn, gradually improving the system’s performance on a specific task over time through iterative learning processes. Machine learning is effective for tasks such as classification, regression, and clustering, particularly when they involve high-dimensional data. These algorithms analyze data, identify patterns, and make predictions or decisions without being explicitly programmed for each task.</p>
<p>Based on the given problem and the available data, there are many potential model and training paradigms, three of the most prominent types of ML being: supervised learning <span class="citation" data-cites="117qAPYe7">[<a href="#ref-117qAPYe7" role="doc-biblioref">66</a>]</span>, unsupervised learning <span class="citation" data-cites="DWZxgqvi bLNWjb9u">[<a href="#ref-DWZxgqvi" role="doc-biblioref">67</a>,<a href="#ref-bLNWjb9u" role="doc-biblioref">68</a>]</span>, and reinforcement learning <span class="citation" data-cites="IWQdHNWs">[<a href="#ref-IWQdHNWs" role="doc-biblioref">69</a>]</span>. The goal of machine learning is to develop an output model that can make predictions or decisions based on input data. In supervised learning, the model is trained on a labeled dataset, where each training example is paired with an output label. A label is the desired output or result for a given piece of data. For example, in an image recognition task, labels could be the names of objects in the images (e.g., “cat,” “dog,” “car”). In a spam detection task, emails could be labeled as “spam” or “not spam.”. The goal is to learn a mapping from inputs to outputs. Unsupervised learning involves training a model on data without labeled responses. The goal is to uncover patterns or structures within the data. In reinforcement learning, an agent learns to make decisions by interacting with an environment. The agent receives feedback in the form of rewards or penalties based on its actions and learns to maximize cumulative rewards over time.</p>
<p>Depending on the learning task, the field offers various classes of ML algorithms, each of them coming in multiple specifications and variants, including regression models, instance-based algorithms, decision trees, Bayesian methods, and artificial neural networks, among others.</p>
<p>Artificial neural networks (ANNs) span all three major types of machine learning. ANNs are inspired by biological systems and consist of interconnected processing units called neurons, with connections akin to synapses in the human brain. Signals are processed based on thresholds set by activation functions, and organized into layers for input, hidden, and output layers. Shallow machine learning encompasses simpler ANNs and other algorithms, often being more interpretable than deep neural networks. Deep neural networks, which have multiple hidden layers, perform complex calculations to automatically discover patterns in data. This ability is known as deep learning64. Deep learning excels with large, high-dimensional data like text, images, and videos, while shallow learning may outperform with low-dimensional data or limited training data. Time series, image, and text data present various application domains.</p>
<figure>
<img src="https://github.com/samadon1/generative-biology/assets/56901167/e0b9341b-b745-4511-af3f-13466f8a96b9" alt="Figure 2: Relationship between statistics, artificial intelligence, expert systems, machine learning, deep learning and large language models." />
<figcaption aria-hidden="true"><strong>Figure 2</strong>: Relationship between statistics, artificial intelligence, expert systems, machine learning, deep learning and large language models.</figcaption>
</figure>
<p>Automated model building in machine learning involves using input data for pattern identification relevant to the learning task. Shallow machine learning relies on predefined features such as pixel values in images or word frequencies in text. For example, in image classification, shallow learning might rely on handcrafted features like color histograms or edge detectors. In contrast, deep learning can operate directly on high-dimensional raw input data, such as the raw pixel values of an image or the sequence of words in text. It automatically learns features at multiple levels of abstraction, allowing it to capture patterns in the data without the need for manual feature engineering. For instance, in image classification with deep learning, the model learns to detect edges, shapes, and textures from raw pixel data, resulting in improved accuracy <span class="citation" data-cites="13wKjJvoL">[<a href="#ref-13wKjJvoL" role="doc-biblioref">70</a>]</span>.</p>
<p>Deep learning architectures often combine both aspects into end-to-end systems or extract features for use in other learning subsystems. Various deep learning architectures have emerged, including convolutional neural networks (CNNs) <span class="citation" data-cites="7jG4ORkP">[<a href="#ref-7jG4ORkP" role="doc-biblioref">71</a>]</span>, recurrent neural networks (RNNs) <span class="citation" data-cites="To07TIhw">[<a href="#ref-To07TIhw" role="doc-biblioref">72</a>]</span>, distributed representations <span class="citation" data-cites="yxIOlxEU">[<a href="#ref-yxIOlxEU" role="doc-biblioref">73</a>]</span>, autoencoders <span class="citation" data-cites="LoYLzWSf">[<a href="#ref-LoYLzWSf" role="doc-biblioref">74</a>]</span>, generative adversarial neural networks (GANs) <span class="citation" data-cites="J5FKNc97">[<a href="#ref-J5FKNc97" role="doc-biblioref">75</a>]</span>, among others. CNNs excel in computer vision and speech recognition tasks, learning hierarchical features essential for image recognition. RNNs specialize in sequential data structures like time-series data and natural language processing (NLP), addressing the challenges of vanishing gradients through advanced mechanisms like long short-term memory (LSTM) networks <span class="citation" data-cites="7ikCKv12">[<a href="#ref-7ikCKv12" role="doc-biblioref">76</a>]</span>. Distributed representations, such as word embeddings, play a crucial role in NLP tasks by projecting language entities into numerical representations, preserving semantic relationships between words. Autoencoders provide dense feature representations and are applied for unsupervised feature learning, dimensionality reduction, and anomaly detection. GANs, belonging to generative models, learn probability distributions over training data to generate new data samples, using a generator-discriminator framework in a non-cooperative game setting.</p>
<h2 id="generative-ai-and-transformers">Generative AI and Transformers</h2>
<p>Generative AI (GenAI) analyzes vast amounts of data, looking for patterns and relationships, then uses these insights to create fresh, new content that mimics the original data <span class="citation" data-cites="1J5MgQt2">[<a href="#ref-1J5MgQt2" role="doc-biblioref">77</a>]</span>. It does this by leveraging machine learning models, especially unsupervised and semi-supervised algorithms. There are three popular techniques for implementing Generative AI: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Transformers.</p>
<p>Variational Autoencoders (VAEs) <span class="citation" data-cites="12eS31RFj">[<a href="#ref-12eS31RFj" role="doc-biblioref">78</a>]</span> first introduced by Diederik P. Kingma et al. in 2013 are generative models in unsupervised machine learning that generate new data similar to the input data. They consist of an encoder that compresses the input data into a lower-dimensional latent space by producing parameters for a probability distribution (mean and variance). The decoder reconstructs the data from this latent representation. The loss function, which combines reconstruction loss and regularization loss (KL Divergence), ensures the output data is both accurate and diverse. VAEs are used in applications like image generation, data imputation, anomaly detection, offering a flexible framework for generating and understanding data despite some challenges in balancing the loss components and achieving high-quality outputs <span class="citation" data-cites="z4zN2ueG OvkIs2vK Miw01DVK">[<a href="#ref-z4zN2ueG" role="doc-biblioref">79</a>,<a href="#ref-OvkIs2vK" role="doc-biblioref">80</a>,<a href="#ref-Miw01DVK" role="doc-biblioref">81</a>]</span>.</p>
<p>In 2014, GANs <span class="citation" data-cites="J5FKNc97">[<a href="#ref-J5FKNc97" role="doc-biblioref">75</a>]</span> were proposed by researchers at the University of Montreal. GANs use two models that work in tandem: One learns to generate a target output (like an image) and the other learns to discriminate true data from the generator’s output. The generator tries to fool the discriminator, and in the process learns to make more realistic outputs. The image generator StyleGAN <span class="citation" data-cites="1EHGNXskO">[<a href="#ref-1EHGNXskO" role="doc-biblioref">82</a>]</span> is based on these types of models.</p>
<p>Diffusion models <span class="citation" data-cites="tzRTBD2w">[<a href="#ref-tzRTBD2w" role="doc-biblioref">83</a>]</span> were introduced a year later by researchers at Stanford University and the University of California at Berkeley. By iteratively refining their output, these models learn to generate new data samples that resemble samples in a training dataset and have been used to create realistic-looking images. A diffusion model is at the heart of the text-to-image generation system Stable Diffusion <span class="citation" data-cites="emwEKYnI">[<a href="#ref-emwEKYnI" role="doc-biblioref">84</a>]</span>.</p>
<p>Recurrent neural networks (RNNs) and their variants like long short-term memory (LSTM) networks are commonly used for sequential data processing tasks. However, these models suffer from limitations such as vanishing gradients and inefficiency in parallelization. Transformers revolutionized the field with the ability to capture long-range dependencies in sequential data efficiently and was first reported in the seminal 2017 paper, “Attention is All You Need” <span class="citation" data-cites="gdGFbXoj">[<a href="#ref-gdGFbXoj" role="doc-biblioref">85</a>]</span>. The introduction of transformers, with their superior performance and scalability, initiated a departure from RNNs. Transformers were used to train the large language models (LLMs) that power ChatGPT <span class="citation" data-cites="UgpiYzZc">[<a href="#ref-UgpiYzZc" role="doc-biblioref">86</a>]</span>.</p>
<p>The transformer architecture consists of an encoder and a decoder, each with multiple layers of self-attention and feedforward neural networks. The self-attention mechanism enables the model to assess the significance of a piece of data, such as a word in a sentence, based on that word’s relations with other words in the sentence. To preserve the ordering of the words and the meaning of the sentence, the transformer incorporates positional bias to maintain the relative positions of words within a sentence.</p>
<p>The transformer encoder-decoder architecture performs well at tasks like language translation. In a language translation task, the model transforms a sentence by encoding inputs from one language and then decoding outputs in another. The encoder processes the input sentence and creates a fixed-size vector representation, which the decoder then uses to generate the output sentence. The encoder-decoder employs both self-attention and cross-attention mechanisms, where self-attention is applied to the decoder’s inputs, and cross-attention focuses on the encoder’s output.</p>
<p>A prominent example of the transformer encoder-decoder architecture is Google’s T5 (Text-to-Text Transfer Transformer) <span class="citation" data-cites="1DdwoI1Kr">[<a href="#ref-1DdwoI1Kr" role="doc-biblioref">87</a>]</span>, introduced in 2019. T5 can be fine-tuned for various NLP tasks, including language translation, question answering, and summarization.Real-world applications of the transformer encoder-decoder architecture include Google Translate, which utilizes the T5 model for translating text between languages, and Facebook’s M2M-10080, a multilingual machine translation model capable of translating among 100 different languages.</p>
<figure>
<img src="https://github.com/samadon1/generative-biology/assets/56901167/0b812bd8-b2a8-48b6-9768-e2f447114bf6" alt="Figure 3: The encoder-decoder structure of the Transformer architecture. Adapted from “Attention Is All You Need” Encoder-only models: Ideal for tasks requiring a deep understanding of the input, such as sentence classification and named entity recognition. Decoder-only models: Suited for generative tasks like text generation. Encoder-decoder models (or sequence-to-sequence models): Best for generative tasks that depend on an input, such as translation or summarization." />
<figcaption aria-hidden="true"><strong>Figure 3</strong>: The encoder-decoder structure of the Transformer architecture. Adapted from “Attention Is All You Need” <strong>Encoder-only models</strong>: Ideal for tasks requiring a deep understanding of the input, such as sentence classification and named entity recognition. <strong>Decoder-only models</strong>: Suited for generative tasks like text generation. <strong>Encoder-decoder models (or sequence-to-sequence models)</strong>: Best for generative tasks that depend on an input, such as translation or summarization.</figcaption>
</figure>
<h3 id="transformer-encoder">Transformer Encoder</h3>
<p>The transformer encoder architecture is used for tasks such as text classification, where the goal is to categorize a piece of text into predefined categories. Text classification tasks include determining the sentiment of a piece of text, determining the topic and detecting if the text is spam. The encoder processes a sequence of tokens and produces a fixed-size vector representation of the entire sequence, which is then used for classification. The most notable transformer encoder model is BERT (Bidirectional Encoder Representations from Transformers) <span class="citation" data-cites="cnHJ2ZPM">[<a href="#ref-cnHJ2ZPM" role="doc-biblioref">88</a>]</span>, introduced by Google in 2018. BERT is pre-trained on large text datasets and can be fine-tuned for a wide range of NLP tasks.</p>
<p>Unlike the encoder-decoder architecture, the transformer encoder focuses solely on the input sequence without generating an output sequence and instead the output is a classification task. It uses the self-attention mechanism to identify the most relevant parts of the input for the given task. Real-world applications of the transformer encoder architecture include sentiment analysis, where models classify reviews as positive or negative, and email spam detection, where models classify emails as spam or not.</p>
<h3 id="transformer-decoder">Transformer Decoder</h3>
<p>The transformer decoder architecture is tailored for tasks like language generation, where the model creates a sequence of words based on an input prompt or context. The decoder takes a fixed-size vector representation of the context and generates a sequence of words one at a time, with each word depending on the previously generated words. A well-known transformer decoder model is GPT-3 (Generative Pre-trained Transformer 3) <span class="citation" data-cites="ihwUK6Sk">[<a href="#ref-ihwUK6Sk" role="doc-biblioref">89</a>]</span>, introduced by OpenAI in 2020. GPT-3 is a large language model capable of generating human-like text across various styles and genres. ChatGPT, which is based on the GPT-3 model, was officially launched by OpenAI in November 2020. It was a significant milestone in the development of large language models (LLMs), characterized by its ability to generate human-like text across various styles and genres. Real-world applications of the transformer decoder architecture include text generation, where models generate stories or articles based on a given prompt, and chatbots, where models create natural and engaging responses to user inputs.</p>
<h3 id="large-language-models-llms">Large Language Models (LLMs)</h3>
<p>Large language models are machine learning models that can comprehend and generate human language text. In the life sciences, LLMs such as GPT (Generative Pre-trained Transformer) and BERT, have revolutionized natural language processing, enabling researchers to extract insights from vast repositories of biomedical literature, accelerate drug discovery, and personalize patient care <span class="citation" data-cites="eRP3HctH">[<a href="#ref-eRP3HctH" role="doc-biblioref">90</a>]</span>.</p>
<p>Large language models use transformer models and are trained using massive datasets — hence, large. This enables them to recognize, translate, predict, or generate text or other content. They are composed of multiple neural network layers – recurrent layers, feedforward layers, embedding layers, and attention layers work in tandem to process the input text and generate output content.</p>
<p>There are three main kinds of large language models:</p>
<ul>
<li><strong>Generic or raw language models</strong> predict the next word based on the language in the training data. These language models perform information retrieval tasks.</li>
<li><strong>Instruction-tuned language models</strong> are trained to predict responses to the instructions given in the input. This allows them to perform sentiment analysis, or to generate text or code.</li>
<li><strong>Dialog-tuned language models</strong> are trained to have a dialog by predicting the next response. Think of chatbots or conversational AI.</li>
</ul>
<p>Before functioning, LLMs undergo two crucial processes: training and fine-tuning. They are pre-trained on massive textual datasets from sources like Wikipedia and GitHub, comprising trillions of words to form a foundation model or a pre-trained model. This unsupervised learning stage allows the model to understand word meanings, relationships, and contextual distinctions, such as discerning whether “right” means “correct” or the opposite of “left.”. To perform specific tasks, pretrained models undergo fine-tuning, which tailors them to particular activities like translation. This process optimizes task-specific performance. A related method, prompt-tuning, trains the model using few-shot or zero-shot prompting. Few-shot prompting provides examples to teach the model how to respond, while zero-shot prompting directly instructs the model on the task without examples.</p>
<p>LLMs serve various purposes:</p>
<ul>
<li><strong>Information retrieval</strong>: Used by search engines like Google and Bing to produce and communicate answers conversationally.</li>
<li><strong>Sentiment analysis</strong>: Used to evaluate the sentiment of textual data.</li>
<li><strong>Text generation</strong>: Powers generative AI, such as ChatGPT, to create text based on prompts.</li>
<li><strong>Code generation</strong>: Similar to text generation, LLMs can generate code by recognizing patterns.</li>
<li><strong>Chatbots and conversational AI</strong>: Facilitate customer service interactions by interpreting and responding to customer queries.</li>
</ul>
<h1 id="ai-in-the-life-sciences">AI in the Life Sciences</h1>
<p>The intersection of AI and the life sciences (AIxBio) has given rise to new capabilities where advanced computational techniques are applied to understand the complexities of biological systems and engineer novel solutions to pressing challenges in medicine and biotechnology <span class="citation" data-cites="1BrqDsUlc">[<a href="#ref-1BrqDsUlc" role="doc-biblioref">91</a>]</span>. The two primary modern AI categories used in the life sciences are large language models (LLMs) and bio-AI tools.</p>
<p>LLM-based chatbots like ChatGPT are designed to process human language inputs and generate output in human-like fashion. In the life sciences, ChatGPT can assist researchers by drafting and editing scientific manuscripts, generating hypotheses, summarizing datasets, and retrieving information from the scientific literature. LLM-based chatbots can also streamline literature reviews and facilitate the comprehension of complex biological concepts.</p>
<p>As a general-purpose LLM, ChatGPT and its equivalents are trained on a broad range of text from the internet. This results in models that function across topics and contexts. However, the generalist nature comes at the cost of precision and depth required for highly specialized tasks. For example, LLM-based chatbots can provide outputs with information with unfounded details, aiming to fill knowledge gaps. This behavior is known as “Confabulation”, and it can limit the utility of the tool. Furthermore, ethical concerns related to biased outputs are often attributed to biases within the training data.</p>
<p>Additionally, training and using general-purpose language models can be computationally expensive, time-consuming, and resource and energy intensive. Given the cost of training general purpose LLMs and their limitations, evaluations are essential for understanding their performance. Evaluations help developers identify strengths and weaknesses of the model, and often measure generalizability of models to real-world applications. This process can also identify biased or misleading model outputs. Typically, models undergo evaluation on standardized benchmarks such as GLUE (General Language Understanding Evaluation) <span class="citation" data-cites="OrFhTkp9">[<a href="#ref-OrFhTkp9" role="doc-biblioref">92</a>]</span>, SuperGLUE <span class="citation" data-cites="IyaaHAPt">[<a href="#ref-IyaaHAPt" role="doc-biblioref">93</a>]</span>, HellaSwag <span class="citation" data-cites="eMFlKM40">[<a href="#ref-eMFlKM40" role="doc-biblioref">94</a>]</span>, TruthfulQA <span class="citation" data-cites="z4KRV9wF">[<a href="#ref-z4KRV9wF" role="doc-biblioref">95</a>]</span>, and MMLU (Massive Multitask Language Understanding) <span class="citation" data-cites="1DdXyopJh">[<a href="#ref-1DdXyopJh" role="doc-biblioref">96</a>]</span> using established metrics, as shown in Table 2.</p>
<h3 id="table-2.-common-benchmarks-for-llms">Table 2. Common Benchmarks for LLMs</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Description</th>
<th>Format of Task</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MMLU</td>
<td>MMLU (Massive Multitask Language Understanding) evaluates how well the LLM can multitask</td>
<td>Multiple-choice</td>
</tr>
<tr class="even">
<td>TruthfulQA</td>
<td>Measures truthfulness of model responses</td>
<td>Generation, Multiple-choice</td>
</tr>
<tr class="odd">
<td>HellaSwag</td>
<td>Evaluates how well an LLM can complete a sentence</td>
<td>Sentence completion</td>
</tr>
<tr class="even">
<td>SuperGLUE Benchmark</td>
<td>Compares more challenging and diverse tasks with GLUE, with comprehensive human baselines</td>
<td>Sentence- and sentence-pair classification (main task), coreference resolution and question answering</td>
</tr>
<tr class="odd">
<td>GLUE Benchmark</td>
<td>GLUE (General Language Understanding Evaluation) benchmark provides a standardized set of diverse NLP tasks to evaluate the effectiveness of different language models</td>
<td>Classification and prediction</td>
</tr>
</tbody>
</table>
<p>The behavior of LLMs can be modified through model alignment, domain-specific pre-training, and supervised fine-tuning. These methods can be used to address limitations of generic LLMs, tailor behavior to meet specific requirements, and infuse general knowledge into the LLMs. Domain-specific language models, trained or fine-tuned on specific datasets relevant to particular domains, offer more contextually accurate responses for specific domains. Evaluating domain-specific or fine-tuned models typically involves comparing their performance against a ground truth dataset if available. This process is crucial because it ensures that the model performs as expected and generates the desired outputs.</p>
<p>In the life sciences, specialized models can interpret complex biological data, provide detailed insights, thereby enhancing both the accuracy and reliability of the information provided. These models are known as scientific large language models (Sci-LLMs) <span class="citation" data-cites="xoXi6JBv">[<a href="#ref-xoXi6JBv" role="doc-biblioref">97</a>]</span>.</p>
<figure>
<img src="https://github.com/samadon1/generative-biology/assets/56901167/3e2e3d63-981d-409a-89e3-0c979aa9debc" alt="Figure 4: AI-enabled tools used in the biological sciences; Large Language Models (LLMs) and Bio-AI." />
<figcaption aria-hidden="true"><strong>Figure 4</strong>: AI-enabled tools used in the biological sciences; Large Language Models (LLMs) and Bio-AI.</figcaption>
</figure>
<h2 id="scientific-large-language-models-sci-llms">Scientific Large Language Models (Sci-LLMs)</h2>
<p>LLMs in the life sciences have been trained on natural language, molecular, protein, and genomic sequence data. These LLMs are collectively known as Scientific Large Language Models (Sci-LLMs). Sci-LLMs are specialized models designed to process and understand various types of scientific data. They extend the capabilities of general LLMs to handle domain-specific tasks in biology, chemistry, and other scientific fields. Sci-LLMs in the biological field include Textual Scientific Large Language Models (Text-Sci-LLMs), Protein Large Language Models (Pro-LLMs), and Genomic Large Language Models (Gene-LLMs) <span class="citation" data-cites="xoXi6JBv">[<a href="#ref-xoXi6JBv" role="doc-biblioref">97</a>]</span>.</p>
<h2 id="textual-scientific-large-language-models-text-sci-llms">Textual Scientific Large Language Models (Text-Sci-LLMs)</h2>
<p>Text-Sci-LLMs are trained on vast amounts of scientific textual data, such as scientific publications. Text-Sci-LLMs excel at understanding, generating, and interacting with written human language from scientific domains. LLMs trained on vast, diverse datasets, such as BERT <span class="citation" data-cites="cnHJ2ZPM">[<a href="#ref-cnHJ2ZPM" role="doc-biblioref">88</a>]</span> and its variations which have been fine-tuned specifically on biological corpora with the encoder-only architecture, have demonstrated significant potential in natural language processing (NLP) tasks within biology. Models initially trained on broad corpora such as Wikipedia and textbooks and then fine-tuned on specific biological NLP tasks, show substantial improvements in various downstream tasks including biological terminology understanding, named entity recognition, text similarity, and relation extraction <span class="citation" data-cites="11KKjDIVI WWUD4rgh 1E2ZqNklN xBZDDNz8 7oKEC5H3">[<a href="#ref-11KKjDIVI" role="doc-biblioref">98</a>,<a href="#ref-WWUD4rgh" role="doc-biblioref">99</a>,<a href="#ref-1E2ZqNklN" role="doc-biblioref">100</a>,<a href="#ref-xBZDDNz8" role="doc-biblioref">101</a>,<a href="#ref-7oKEC5H3" role="doc-biblioref">102</a>]</span>.</p>
<p>GPT and its variants <span class="citation" data-cites="ihwUK6Sk 4RZT0xlw BUVSzC66">[<a href="#ref-ihwUK6Sk" role="doc-biblioref">89</a>,<a href="#ref-4RZT0xlw" role="doc-biblioref">103</a>,<a href="#ref-BUVSzC66" role="doc-biblioref">104</a>]</span>, with decoder-only architectures, have become dominant in the field of biological NLP because they can generate textual information as an output. BioGPT <span class="citation" data-cites="17n2DOU6H">[<a href="#ref-17n2DOU6H" role="doc-biblioref">105</a>]</span>, an extension of GPT-2 <span class="citation" data-cites="BUVSzC66">[<a href="#ref-BUVSzC66" role="doc-biblioref">104</a>]</span>, has been extensively fine-tuned on biomedical literature, showcasing remarkable performance in biomedical relation extraction and question answering. It also generates coherent and fluent descriptions within the biomedical context. BioMedGPT-LM <span class="citation" data-cites="J8Pydzc8">[<a href="#ref-J8Pydzc8" role="doc-biblioref">106</a>]</span>, incrementally pre-trained on LLaMA2 <span class="citation" data-cites="ZUDBmsmY">[<a href="#ref-ZUDBmsmY" role="doc-biblioref">107</a>]</span>, enables a comprehensive understanding of various biological modalities and aligns them with natural language. BioGPT and BioMedGPT-LM are both specialized language models designed for biomedical applications; however, BioGPT focuses on generating and understanding biomedical literature, while BioMedGPT-LM integrates a broader range of tasks including text generation, question answering, and classification within the biomedical domain.</p>
<h3 id="capabilities-evaluation">Capabilities Evaluation</h3>
<p>The evaluation of LLMs often uses Bloom’s taxonomy <span class="citation" data-cites="YKj6eC6u 9POuL4HH">[<a href="#ref-YKj6eC6u" role="doc-biblioref">108</a>,<a href="#ref-9POuL4HH" role="doc-biblioref">109</a>]</span>, which includes six cognitive levels:</p>
<h3 id="table-3.-blooms-taxonomy">Table 3. Bloom’s Taxonomy</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Cognitive Level</th>
<th>Description</th>
<th>Examples of Activities/Tasks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Remember</td>
<td>Recall facts and basic concepts</td>
<td>List, define, identify, memorize, repeat, state</td>
</tr>
<tr class="even">
<td>Understand</td>
<td>Explain ideas or concepts</td>
<td>Describe, explain, interpret, summarize, paraphrase, discuss</td>
</tr>
<tr class="odd">
<td>Apply</td>
<td>Use information or existing knowledge in new contexts</td>
<td>Use, demonstrate, solve, implement, execute, carry out</td>
</tr>
<tr class="even">
<td>Analyze</td>
<td>Explore connections, causes, and relationships among ideas</td>
<td>Differentiate, organize, relate, compare, contrast, examine</td>
</tr>
<tr class="odd">
<td>Evaluate</td>
<td>Justify a decision or course of action based on sound analysis</td>
<td>Judge, critique, recommend, justify, assess, appraise</td>
</tr>
<tr class="even">
<td>Create</td>
<td>Produce new or original work using existing information</td>
<td>Design, assemble, construct, develop, formulate, author</td>
</tr>
</tbody>
</table>
<p>SciEval <span class="citation" data-cites="AU2DrFMM">[<a href="#ref-AU2DrFMM" role="doc-biblioref">110</a>]</span> has recently introduced a framework for evaluating scientific LLMs across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability. These dimensions are based on the cognitive domains in Bloom’s taxonomy. KnowEval <span class="citation" data-cites="xoXi6JBv">[<a href="#ref-xoXi6JBv" role="doc-biblioref">97</a>]</span> assesses the depth of knowledge LLMs can grasp, aiming for human-level comprehension. KnowEval categorizes Text-Sci-LLMs into Pre-college, College, and Post-college levels based on the complexity of scientific knowledge.</p>
<h3 id="table-4.-categories-for-knoweval">Table 4. Categories for KnowEval</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Pre-college Level</td>
<td>This level covers fundamental concepts and principles, aligning with the Remember and Understand stages of Bloom’s taxonomy and the basic knowledge dimension of SciEval. Evaluations focus on basic knowledge comprehension, using benchmarks like MMLU <span class="citation" data-cites="1DdXyopJh">[<a href="#ref-1DdXyopJh" role="doc-biblioref">96</a>]</span> and C-Eval <span class="citation" data-cites="11eq54zNQ">[<a href="#ref-11eq54zNQ" role="doc-biblioref">111</a>]</span></td>
</tr>
<tr class="even">
<td>College Level</td>
<td>At this level, knowledge becomes more specialized and abstract, requiring logical reasoning and proof. It corresponds to the Apply and Analyze stages of Bloom’s taxonomy and the knowledge application and scientific calculation dimensions of SciEval. Evaluations like PubMedQA <span class="citation" data-cites="C6CjQZug">[<a href="#ref-C6CjQZug" role="doc-biblioref">112</a>]</span> and SciQ <span class="citation" data-cites="1AbLepXdc">[<a href="#ref-1AbLepXdc" role="doc-biblioref">113</a>]</span> focus on this advanced understanding.</td>
</tr>
<tr class="odd">
<td>Post-college Level</td>
<td>This level involves mastering current knowledge and generating innovative ideas, aligning with the Evaluate and Create stages of Bloom’s taxonomy and the research ability dimension of SciEval. It requires capabilities beyond standard question-answering, including summarizing advancements and designing novel experiments. Few benchmarks, such as a subset in the SciEval dataset <span class="citation" data-cites="AU2DrFMM">[<a href="#ref-AU2DrFMM" role="doc-biblioref">110</a>]</span>, assess these high-level capabilities.</td>
</tr>
</tbody>
</table>
<h3 id="benchmarks-for-text-sci-llms">Benchmarks for Text-Sci-LLMs</h3>
<h3 id="table-5.-summary-of-benchmarks-for-text-sci-llms">Table 5. Summary of Benchmarks for Text-Sci-LLMs</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Description</th>
<th>Type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MMLU</td>
<td>Offers a detailed and challenging benchmark that tests the comprehension and problem-solving capabilities of LLMs across a wide spectrum of tasks and subjects.</td>
<td>Multiple choice</td>
</tr>
<tr class="even">
<td>C-Eval</td>
<td>Consists of 13,948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.</td>
<td>Multiple choice</td>
</tr>
<tr class="odd">
<td>AGIEval</td>
<td>Evaluates the general abilities of foundation models in tasks pertinent to human cognition and problem-solving.</td>
<td>Multiple choice</td>
</tr>
<tr class="even">
<td>ScienceQA</td>
<td>A dataset designed for question answering in the scientific domain, covering various scientific topics and requiring reasoning over structured and unstructured information.</td>
<td>Multiple choice / Question answering (QA)</td>
</tr>
<tr class="odd">
<td>SciEval</td>
<td>A benchmark dataset for evaluating language models in the scientific domain, covering a range of tasks related to scientific text understanding and generation.</td>
<td>Multiple choice / Question answering (QA)</td>
</tr>
<tr class="even">
<td>Bioinfo-Bench-QA</td>
<td>A benchmark dataset focused on question answering in the field of bioinformatics, covering topics related to biological information processing and analysis.</td>
<td>Multiple choice</td>
</tr>
<tr class="odd">
<td>SciQ</td>
<td>A dataset designed for evaluating language models in scientific question answering tasks, covering various scientific disciplines and requiring both factual and reasoning-based answers.</td>
<td>Multiple choice</td>
</tr>
<tr class="even">
<td>ARC</td>
<td>A dataset that challenges models with questions that require a mix of comprehension and reasoning skills across a wide range of topics, including science.</td>
<td>Multiple choice</td>
</tr>
<tr class="odd">
<td>BLURB</td>
<td>A comprehensive set of datasets and tasks designed to evaluate the performance of natural language processing (NLP) models specifically in the biomedical domain.</td>
<td>Multiple NLP tasks</td>
</tr>
<tr class="even">
<td>PubMedQA</td>
<td>A dataset designed for question answering based on biomedical literature available on PubMed, aiming to evaluate models’ ability to comprehend and extract information from scientific articles.</td>
<td>True or False</td>
</tr>
</tbody>
</table>
<h2 id="protein-large-language-models-prot-llms.">Protein Large Language Models (Prot-LLMs).</h2>
<p>Protein Large Language Models (Prot-LLMs) are trained on protein-related sequence data, including amino acid sequences, protein folding patterns, and other biological information. As a result, they can accurately predict protein structures, functions, and interactions. Prot-LLMs can be categorized into three main types based on their architectures: encoder-only, decoder-only, and encoder-decoder models, each suited for various protein research applications. For instance, encoder-only models are primarily used for predicting protein functions or properties, while decoder-only models are mainly employed for protein generation tasks.</p>
<p><strong>Encoder-only models</strong>: Encoder-only models are a specialized form of the transformer architecture, dedicated solely to understanding and encoding input sequences.The essence of an encoder-only model revolves around extracting significant context from input sequences. These models encode protein sequences into fixed-length vectors for tasks like pattern recognition and prediction. Techniques like the Pairwise Masked Language Model (PMLM) <span class="citation" data-cites="ws5eobRv">[<a href="#ref-ws5eobRv" role="doc-biblioref">114</a>]</span> and mixed-chunk attention aim to capture co-evolutionary information and reduce complexity. Non-parametric models like ProteinNPT <span class="citation" data-cites="RQ40o1d0">[<a href="#ref-RQ40o1d0" role="doc-biblioref">115</a>]</span> handle sparse labels and multitask learning. Some models, like ESM-GearNet <span class="citation" data-cites="3AjKbAlp">[<a href="#ref-3AjKbAlp" role="doc-biblioref">116</a>]</span> and LM-GVP <span class="citation" data-cites="18PNFVXMG">[<a href="#ref-18PNFVXMG" role="doc-biblioref">117</a>]</span>, integrate 3D structure information for better performance.</p>
<p><strong>Decoder-only models</strong>: Utilizing the GPT <span class="citation" data-cites="ihwUK6Sk">[<a href="#ref-ihwUK6Sk" role="doc-biblioref">89</a>]</span> architecture, these models, such as ProGen <span class="citation" data-cites="1EwWbaeg2">[<a href="#ref-1EwWbaeg2" role="doc-biblioref">118</a>]</span> and ProGen2 <span class="citation" data-cites="NNPUXrCp">[<a href="#ref-NNPUXrCp" role="doc-biblioref">119</a>]</span>, are essential for controllable protein generation. They explore unseen regions of the protein space while designing proteins with nature-like properties. Similar capabilities are exemplified by models like RITA <span class="citation" data-cites="o5h39u4V">[<a href="#ref-o5h39u4V" role="doc-biblioref">120</a>]</span>, PoET <span class="citation" data-cites="10tIBLTT8">[<a href="#ref-10tIBLTT8" role="doc-biblioref">121</a>]</span>, and LM-Design <span class="citation" data-cites="R8VlFsXM">[<a href="#ref-R8VlFsXM" role="doc-biblioref">122</a>]</span>.</p>
<p><strong>Encoder-decoder models</strong>: Used for sequence-to-sequence tasks, these models, including ProstT5 <span class="citation" data-cites="L387tcBg">[<a href="#ref-L387tcBg" role="doc-biblioref">123</a>]</span> and pAbT5 <span class="citation" data-cites="1HFrE3NU1">[<a href="#ref-1HFrE3NU1" role="doc-biblioref">124</a>]</span> are adept at tasks where an input sequence is transformed into an output sequence. A common example of a sequence-to-sequence task is machine translation, where a model translates a sentence from one language to another. In the context of Prot-LLMs, sequence-to-sequence tasks could involve tasks such as translating between protein sequences and structures. They can incorporate Multiple Sequence Alignment (MSA) modules to improve sequence generation and utilize reinforcement learning for structure-based design, as seen in Fold2Seq <span class="citation" data-cites="5ndwLMAS">[<a href="#ref-5ndwLMAS" role="doc-biblioref">125</a>]</span>.</p>
<h3 id="capabilities-evaluation-1">Capabilities Evaluation</h3>
<p>Prot-LLMs are evaluated in three key areas: protein structure prediction, protein function prediction, and protein sequence generation.</p>
<p><strong>Protein Structure Prediction</strong>: Prot-LLMs can predict the 3D structure of proteins from their sequences, which aids in understanding protein function, drug design, and biomedical research. Based on the 3D structure of known proteins, prot-LLMs can predict the three-dimensional structure of proteins based on an input sequence, which includes determining the atomic coordinates and the spatial relationships between atoms. Encoder-based Prot-LLMs are used to extract sequence information from the training data and predict tertiary and quaternary structures.</p>
<p><strong>Protein Function Prediction</strong>: Prot-LLMs can predict the biological function of proteins and their interactions with other biomolecules. These tasks can be grouped into several categories. Firstly, protein classification involves categorizing proteins based on their structure, function, or sequence similarity. Prediction of protein-protein interactions focuses on identifying and forecasting interactions crucial for various biological processes. Localization and homology detection tasks include predicting a protein’s subcellular location and identifying distant relationships between protein sequences. Spectral characteristics and stability prediction involve forecasting fluorescence properties and stability under specific conditions, respectively. Furthermore, specific tasks such as 𝛽-Lactamase activity prediction, solubility prediction, and mutation effect prediction focus on understanding specific protein functions, compound solubility, and the effects of genetic mutations on protein function, respectively. These tasks collectively contribute to explaining the complex functions and behaviors of proteins in biological systems. Biological systems are inherently complex and multifaceted, often requiring the simultaneous optimization of multiple properties. Unlike single-objective optimization, which focuses on one specific goal, multi-objective optimization allows researchers to consider and balance several objectives at once. This is particularly important in protein function prediction, where factors such as stability, activity, solubility, and interaction with other molecules need to be optimized concurrently. By providing a more comprehensive optimization framework and utilizing techniques such as Pareto optimization, researchers can identify solutions that offer the best trade-offs among different objectives, rather than a single optimal solution for one objective. multi-objective optimization can enhance the practical applicability of Prot-LLMs, leading to more effective and efficient solutions in understanding and manipulating protein functions.</p>
<p><strong>Protein Sequence Generation</strong>: Prot-LLMs can propose amino acid sequences not found in nature and with a predicted function, useful in drug design and enzyme engineering. It includes:</p>
<ul>
<li>De novo protein design: Proposing protein sequences with a desired property that are not based on existing proteins with some or all of the desired property. Autoregressive generative models, such as the ProGen series, are commonly utilized for tasks involving the generation of protein sequences.</li>
<li>Protein sequence optimization: proposing modification to an existing protein sequence to alter (i.e., optimize) its function or characteristic in an intended manner.</li>
</ul>
<h3 id="benchmarks-for-prot-llms">Benchmarks for Prot-LLMs</h3>
<h3 id="table-6.-summary-of-benchmarks-for-prot-llms">Table 6. Summary of Benchmarks for Prot-LLMs</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CASP</td>
<td>CASP (Critical Assessment of Structure Prediction) evaluates different methods and algorithms for protein structure prediction, providing a standard assessment for progress in the field.</td>
</tr>
<tr class="even">
<td>EC</td>
<td>EC (Enzyme Commission) dataset is used to classify enzymes based on the chemical reactions they catalyze. This system is used to evaluate the functional prediction of proteins, specifically enzymes.</td>
</tr>
<tr class="odd">
<td>GO</td>
<td>GO (Gene Ontology) provides a framework for the representation of gene and gene product attributes across species. GO terms are used to annotate proteins with their associated biological processes, cellular components, and molecular functions.</td>
</tr>
<tr class="even">
<td>CATH</td>
<td>CATH (Class, Architecture, Topology, Homologous superfamily) is a protein structure classification database that organizes protein domains into a hierarchical structure based on their folding patterns. It is used to classify protein domains into these categories: Class, Architecture, Topology, Homologous superfamily.</td>
</tr>
<tr class="odd">
<td>SCOP</td>
<td>SCOP (Structural Classification of Proteins) classifies proteins based on their structural and evolutionary relationships. SCOP benchmarks evaluate the ability of computational methods to classify protein structures into appropriate categories: Class, Fold, Superfamily, and Family.</td>
</tr>
<tr class="even">
<td>ProteinGym</td>
<td>ProteinGym is a benchmark suite designed for evaluating the generalization capabilities of machine learning models in protein sequence prediction tasks. It includes various datasets and metrics to assess the performance of models in predicting protein sequences and related properties under different conditions.</td>
</tr>
<tr class="odd">
<td>TAPE</td>
<td>TAPE (Task Assessing Protein Embeddings) is a benchmark suite designed to evaluate the performance of protein sequence embeddings learned by machine learning models. It includes a variety of tasks, such as secondary structure prediction, contact prediction, and remote homology detection, to assess how well these embeddings capture the underlying biological properties of proteins.</td>
</tr>
</tbody>
</table>
<h2 id="genomic-large-language-models-gene-llms">Genomic Large Language Models (Gene-LLMs)</h2>
<p>Gene-LLMs, specialized in genomic data, are trained to comprehend and predict genetic and genomic aspects of biology. They analyze DNA sequences, interpret genetic variations, and aid in genetic research, like identifying disease-related genetic markers or exploring evolutionary biology. Built on the Transformer architecture, genomic LLMs effectively model nucleic acid sequence data, capturing long-range dependencies for prediction and generation tasks. Through self-supervised learning on genomic sequences, Gene-LLMs gradually grasp genome understanding. Once fine-tuned or contextually learned, they prove valuable for downstream tasks, enhancing accuracy and reducing manual intervention.</p>
<p><strong>Encoder-only models</strong>: With an encoder-only architecture for genomics, numerous significant models utilize the Transformer encoder to process gene sequences and extract meaningful patterns. Models like SpliceBERT, DNABERT, DNABERT-2, iEnhancer-BERT <span class="citation" data-cites="FLyjjIcB 1Fjz4nRHW q1jtH9xA 7LBeQ0qG">[<a href="#ref-FLyjjIcB" role="doc-biblioref">126</a>,<a href="#ref-1Fjz4nRHW" role="doc-biblioref">127</a>,<a href="#ref-q1jtH9xA" role="doc-biblioref">128</a>,<a href="#ref-7LBeQ0qG" role="doc-biblioref">129</a>]</span>, and others employ mask training mechanisms to predict and complete masked gene sequences, achieving improved performance in tasks such as promoter prediction and transcription factor binding site prediction.</p>
<p>For instance, MoDNA <span class="citation" data-cites="boBvfMUu">[<a href="#ref-boBvfMUu" role="doc-biblioref">130</a>]</span> adopts a BERT-like encoder with a unique stacked Generator-Discriminator training paradigm, facilitating motif-oriented learning. GENA-LM <span class="citation" data-cites="xvk2JF2w">[<a href="#ref-xvk2JF2w" role="doc-biblioref">131</a>]</span> introduces encoder-based foundational DNA language models capable of handling sequences up to 36,000 base pairs. The Nucleotide-Transformer model <span class="citation" data-cites="34PAkn0s">[<a href="#ref-34PAkn0s" role="doc-biblioref">132</a>]</span>, pre-trained on diverse human and species genomes, enhances the prediction of molecular phenotypes from DNA sequences. EpiGePT <span class="citation" data-cites="zszw9syp">[<a href="#ref-zszw9syp" role="doc-biblioref">133</a>]</span> predicts genome-wide epigenomics signals, offering insights into gene regulation. Uni-RNA <span class="citation" data-cites="LZbja2NS">[<a href="#ref-LZbja2NS" role="doc-biblioref">134</a>]</span> predicts RNA structures and functions, useful in RNA research and drug development. Models like Enformer <span class="citation" data-cites="16YP71KXq">[<a href="#ref-16YP71KXq" role="doc-biblioref">135</a>]</span> and LOGO <span class="citation" data-cites="VQVJDKnn">[<a href="#ref-VQVJDKnn" role="doc-biblioref">136</a>]</span> address the quadratic time complexity of attention mechanisms in handling long sequences, while BioSeq-BLM <span class="citation" data-cites="oDgLqpUO">[<a href="#ref-oDgLqpUO" role="doc-biblioref">137</a>]</span> integrates traditional analysis methods with language models, marking advancements in pre-training and fine-tuning.</p>
<p><strong>Decoder-only models</strong>: Decoder-only models, like GenSLMs <span class="citation" data-cites="6VSmIaGf">[<a href="#ref-6VSmIaGf" role="doc-biblioref">138</a>]</span> and DNAGPT <span class="citation" data-cites="OSbr1Vwp">[<a href="#ref-OSbr1Vwp" role="doc-biblioref">139</a>]</span>, demonstrate generative capabilities, capturing the evolutionary dynamics of viruses and enabling species identification and regulatory factor prediction. HyenaDNA <span class="citation" data-cites="171NrBohw">[<a href="#ref-171NrBohw" role="doc-biblioref">140</a>]</span> stands out for its exceptional ability to efficiently handle ultra-long DNA sequences while preserving single-nucleotide resolution. This unique combination of features enables researchers to analyze and manipulate genetic data at an unprecedented level of detail. Its capability to handle long sequences while maintaining single-nucleotide resolution greatly enhances its utility in various genomic applications, representing a significant advancement in computational genomics.</p>
<p><strong>Encoder-decoder models</strong>: Encoder-decoder models in genomics, such as ENBED <span class="citation" data-cites="1HaTNUuyf">[<a href="#ref-1HaTNUuyf" role="doc-biblioref">141</a>]</span>, combine the strengths of both components to compress and encode genomic data into meaningful representations. These representations are then used by the decoder to generate sequences or make predictions, enhancing bioinformatics research capabilities.</p>
<h3 id="capabilities-evaluation-2">Capabilities Evaluation</h3>
<p>Gene-LLMs undergo evaluation across four key domains: function prediction, structure prediction, sequence generation, and sequence variation and evolution analysis.</p>
<p><strong>Protein Function Prediction</strong>: Traditionally, gene function prediction relied on models trained on specific sequences. With the advent of LLMs, pre-training on extensive genomic data followed by task-specific fine-tuning has enhanced accuracy and contextual understanding. Key subtasks include promoter prediction, enhancer prediction, and binding site prediction, tackled by models like DNABERT <span class="citation" data-cites="1Fjz4nRHW">[<a href="#ref-1Fjz4nRHW" role="doc-biblioref">127</a>]</span> and EpiGePT <span class="citation" data-cites="zszw9syp">[<a href="#ref-zszw9syp" role="doc-biblioref">133</a>]</span></p>
<p><strong>Structure Prediction</strong>: Leverages computational tools to identify and model biologically significant nucleic acid structures, aiding in the design of novel molecular architectures for nanotechnology and synthetic biology. Recent advancements include predicting RNA three-dimensional structures directly from sequences and designing sequences for predefined DNA and RNA nanostructures, demonstrating that nucleic acid structure can be both predictable and controllable. Subtasks include chromatin profile prediction and DNA/RNA-protein interaction prediction, addressed by models like HyenaDNA <span class="citation" data-cites="171NrBohw">[<a href="#ref-171NrBohw" role="doc-biblioref">140</a>]</span> and TFBert <span class="citation" data-cites="GcVrid39">[<a href="#ref-GcVrid39" role="doc-biblioref">142</a>]</span>.</p>
<p><strong>Sequence Generation</strong>: Proposing artificial sequences resembling real biological ones is crucial for bioinformatics, particularly for creating artificial human genomes serving as tools to safeguard genetic privacy and reduce costs linked with genetic sample collection <span class="citation" data-cites="XD0xBDxb 19IrNgQEN">[<a href="#ref-XD0xBDxb" role="doc-biblioref">143</a>,<a href="#ref-19IrNgQEN" role="doc-biblioref">144</a>]</span>. The generated data strives to retain the utility of the source data by replicating most of its characteristics. Consequently, they could serve as viable alternatives for many genomic databases that are either not publicly available or face accessibility barriers. DNAGPT <span class="citation" data-cites="OSbr1Vwp">[<a href="#ref-OSbr1Vwp" role="doc-biblioref">139</a>]</span> excels in this task, generating artificial genomes covering regions of single nucleotide polymorphisms (SNPs).</p>
<p><strong>Sequence Variation and Evolution Analysis</strong>: Understanding biological sequence variation and evolution is vital for uncovering the genetic basis of traits, disease, and evolutionary patterns. Models like GenSLMs <span class="citation" data-cites="6VSmIaGf">[<a href="#ref-6VSmIaGf" role="doc-biblioref">138</a>]</span> and GPN-MSA <span class="citation" data-cites="AXwYjitz">[<a href="#ref-AXwYjitz" role="doc-biblioref">145</a>]</span> analyze the evolutionary landscape of genomes, focusing on species-specific and whole-genome sequence alignments.</p>
<h3 id="benchmarks-for-gene-llms">Benchmarks for Gene-LLMs</h3>
<h3 id="table-7.-summary-of-benchmarks-for-gene-llms">Table 7. Summary of Benchmarks for Gene-LLMs</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CAGI5 Challenge Benchmark</td>
<td>The Critical Assessment of Genome Interpretation (CAGI) is a benchmark designed to rigorously assess computational methods in predicting a wide array of genetic and genomic outcomes.</td>
</tr>
<tr class="even">
<td>Protein-RNA Interaction Prediction Benchmark (Protein-RNA)</td>
<td>A set of 37 machine learning (primarily deep learning) methods for in vivo RNA-binding proteins RBP–RNA interaction prediction. This benchmark systematically evaluates a subset of 11 representative methods across hundreds of CLIP-seq datasets and RBPs.</td>
</tr>
<tr class="odd">
<td>Nucleotide Transformer Benchmark (NT-Bench)</td>
<td>A comprehensive evaluation framework designed to assess the performance of genomics foundational models. This benchmark pits the Nucleotide Transformer models against other prominent genomics models, such as DNABERT, HyenaDNA (with both 1kb and 32kb context lengths), and Enformer.</td>
</tr>
</tbody>
</table>
<h2 id="multimodal-scientific-large-language-models-mm-sci-llms">Multimodal Scientific Large Language Models (MM-Sci-LLMs)</h2>
<p>Multimodal scientific large language models (MM-Sci-LLMs) possess the ability to process and combine various types of scientific data, including text, molecules, and proteins, making them indispensable for interdisciplinary research requiring insights from multiple domains. An emerging research area, MM-Sci-LLMs utilize LLMs as their core to handle diverse data types effectively. These models exhibit remarkable adaptability in incorporating text, images, audio, and other forms of information, enabling comprehensive problem-solving across scientific domains, particularly in biological sciences encompassing protein, molecular, and genomic studies.</p>
<p>Categorized into four distinct groups based on the specific modality they focus on, MM-Sci-LLMs demonstrate specialized capabilities.</p>
<h3 id="table-8.-summary-of-mm-sci-llms">Table 8. Summary of MM-Sci-LLMs</h3>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Description</th>
<th>Encoder-only models</th>
<th>Encoder-Decoder models</th>
<th>Decoder-only models</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Molecule-to-text</td>
<td>Leverage various techniques like multimodal embedding and cross-modal learning to associate chemical structures with textual descriptions, enhancing tasks such as cross-modal retrieval and molecular property prediction.</td>
<td>Text2Mol, KV-PLM, MoMu</td>
<td>DrugChat, MolReGPT, Text+Chem, ChatMol, GIT-Mol</td>
<td>MoIET5, MolFM, GPT-MoI</td>
</tr>
<tr class="even">
<td>Protein-to-text models</td>
<td>Utilize textual data for protein function prediction and multimodal representation learning, enriching protein annotation and design by integrating natural language descriptions with protein data.</td>
<td>ProTranslator, ProtST-ProtBert</td>
<td>InstructionProtein</td>
<td>ProteinDT, Prot2Text,  ProtST-ESM-1B, ProtST-ESM-2</td>
</tr>
<tr class="odd">
<td>Protein-to-molecule models </td>
<td>Focus on linking protein sequences with molecular information, improving drug discovery through techniques like adversarial networks and contrastive learning.</td>
<td>DrugCLIP</td>
<td>DrugGPT</td>
<td>ChemBERTaLM, DeepTarget</td>
</tr>
<tr class="even">
<td>Comprehensive models </td>
<td>Integrate multiple scientific modalities to excel in diverse tasks like biological data analysis, and material prediction, leveraging advanced multimodal learning techniques to support fundamental science research.</td>
<td>BioTranslator</td>
<td>Galactica, ChatDrug,  DARWIN-MDP, BioMedGPT-10B, Mol-Instructions   </td>
<td>BioT5</td>
</tr>
</tbody>
</table>
<h3 id="capabilities-evaluation-3">Capabilities Evaluation</h3>
<p>MM-Sci-LLMs undergo evaluation focusing on three pivotal areas: cross-modal prediction, retrieval, and generation.</p>
<p><strong>Cross-Modal Prediction</strong>: This involves using multimodal models to predict the functionality of biological entities like molecules, proteins, and genomes based on textual instructions. Models like MoleculeSTM <span class="citation" data-cites="vPDj8bGo">[<a href="#ref-vPDj8bGo" role="doc-biblioref">146</a>]</span> and Mol-Instructions <span class="citation" data-cites="1sH9m3JL">[<a href="#ref-1sH9m3JL" role="doc-biblioref">147</a>]</span> integrate molecular structures and text data for function prediction, which is crucial for bioinformatics and drug discovery.</p>
<p><strong>Cross-Modal Retrieval</strong>: Involves retrieving information from one modality based on a query from another modality. Key models like KV-PLM <span class="citation" data-cites="8pwrkuN4">[<a href="#ref-8pwrkuN4" role="doc-biblioref">148</a>]</span> and ProtST-ESM-1b <span class="citation" data-cites="m1jSSz9o">[<a href="#ref-m1jSSz9o" role="doc-biblioref">149</a>]</span> enable retrieving molecules, proteins, or genes based on textual descriptions, aiding drug discovery and biological mechanism understanding.</p>
<p><strong>Cross-Modal Generation</strong>: Aims to create data in one modality based on data from another. Models like Text2Mol <span class="citation" data-cites="2aufANiR">[<a href="#ref-2aufANiR" role="doc-biblioref">150</a>]</span> and ProteinDT <span class="citation" data-cites="tP9JVlEF">[<a href="#ref-tP9JVlEF" role="doc-biblioref">151</a>]</span> generate molecular information from text descriptions, while models like Prot2Text <span class="citation" data-cites="1A950Jvos">[<a href="#ref-1A950Jvos" role="doc-biblioref">152</a>]</span> and ChemBERTaLM <span class="citation" data-cites="iU5gKzdn">[<a href="#ref-iU5gKzdn" role="doc-biblioref">153</a>]</span> convert protein sequences into detailed text descriptions. This capability facilitates cohesive multi-modal data creation, bridging the gap between different modalities in scientific research.</p>
<h3 id="benchmarks-for-mm-sci-llms">Benchmarks for MM-Sci-LLMs</h3>
<h3 id="table-9.-summary-of-benchmarks-for-mm-sci-llms">Table 9. Summary of Benchmarks for MM-Sci-LLMs</h3>
<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th>Benchmark</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>MoleculeNet</td>
<td>MoleculeNet is a large-scale benchmark for molecular machine learning. It curates multiple public datasets, establishes metrics for evaluation, and offers high-quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms.</td>
</tr>
<tr class="even">
<td>MARCEL</td>
<td>MARCEL (MoleculAR Conformer Ensemble Learning) provides a comprehensive platform for evaluating learning from molecular conformer ensembles. It focuses on diverse molecular conformer structures, marking a significant shift in molecular representation learning.</td>
</tr>
<tr class="odd">
<td>GuacaMol</td>
<td>GuacaMol is an evaluation framework designed for de novo molecular design. It aims to generate molecules with specific property profiles through virtual design-make-test cycles.</td>
</tr>
</tbody>
</table>
<p>Our technical exploration is primarily confined to Transformer-based languages, excluding alternative neural architectures like graph neural networks and diffusion models, despite their widespread applications in protein folding. However, the concepts discussed in biological languages can be extended to other scientific languages, such as molecular and mathematical languages.</p>
<p>Molecular large language models (Mol-LLMs) are specialized LLMs trained on molecular data, enabling them to understand and predict the chemical properties and behaviors of molecules. This specialized knowledge makes them invaluable tools in drug discovery, materials science, and the study of complex chemical interactions.</p>
<p>Encoder-only Mol-LLMs, like SMILES-BERT <span class="citation" data-cites="17px28BEq">[<a href="#ref-17px28BEq" role="doc-biblioref">154</a>]</span>, focus on understanding and interpreting input molecules, making them ideal for tasks requiring a deep comprehension of molecular structures and properties. SMILES-BERT, for instance, leverages the BERT architecture to interpret SMILES representations of molecules.</p>
<p>Decoder-only Mol-LLMs, such as MolGPT <span class="citation" data-cites="1865CJ7gK">[<a href="#ref-1865CJ7gK" role="doc-biblioref">155</a>]</span> and SMILESGPT <span class="citation" data-cites="1C5bcQVLo">[<a href="#ref-1C5bcQVLo" role="doc-biblioref">156</a>]</span>, use SMILES strings as input to navigate the vast chemical space. These models are crucial in drug discovery and materials science, enabling the synthesis of molecules with specific properties. MolGPT, which utilizes GPT for molecular generation with conditional training for property optimization, excels in molecular modeling and drug discovery by demonstrating strong control over multiple properties for accurate generation.</p>
<p>In encoder-decoder Mol-LLMs, encoders convert raw molecules into latent vectors, which decoders then reconstruct into functional chemical structures. Most Transformer-based encoder-decoder models use SMILES or SELFIES as inputs for the encoder, with outputs varying by task. For example, in chemical reaction prediction, the decoder generates the anticipated outcomes for reactants. The Molecular Transformer <span class="citation" data-cites="Ho9yh6th">[<a href="#ref-Ho9yh6th" role="doc-biblioref">157</a>]</span>, a Transformer-based model for reaction prediction, effectively handles complex, long-range sequence interactions.</p>
<p>Biological data with graph structures can be modeled in two primary ways: molecular structure-based modeling and biological network-based modeling. In molecular structure-based modeling, atoms or valid chemical substructures are used as nodes, and bonds serve as edges to construct the molecular graph. Molecular graphs are extensively used for predicting molecular properties and designing new molecules.</p>
<p>In biological network-based modeling, nodes represent various entities such as genes, diseases, or RNAs, with edges indicating known associations between pairs of entities, such as miRNA–disease interactions. This creates a relational network. Graph Neural Networks (GNNs) excel at extracting information from graph structures, making them suitable for processing omics data in fields such as genomics, proteomics, RNomics, and radiomics. By applying GNNs to these omics data using the aforementioned modeling methods, a variety of tasks can be performed, including molecular property prediction, de novo molecular design, link prediction, and node classification in biological networks.</p>
<h2 id="bio-ai-tools-bdts">Bio-AI Tools (BDTs)</h2>
<p>Bio-AI tools, commonly referred to as biological design tools (BDTs) are computational tools that help design proteins, viral vectors, or other biological agents. Traditional methods molecular biology like site-directed mutagenesis (SDM) involve the deliberate alteration of specific nucleotide sequences in DNA to create desired changes in the resulting protein. This process typically requires designing and synthesizing specific DNA primers, followed by PCR amplification and cloning steps to introduce the mutated DNA into a host organism. While SDM allows for precise modifications at predetermined sites, it can be time-consuming and labor-intensive, especially when multiple iterations are required to achieve the desired outcome. Additionally, the success rate of SDM experiments can vary depending on factors such as the efficiency of DNA synthesis and the stability of the resulting mutant proteins.</p>
<p>Random mutagenesis, another traditional method, involves introducing random mutations throughout the genome of an organism using techniques such as chemical mutagenesis or UV irradiation. This approach generates a pool of mutants with diverse genetic variations, which are then screened to identify individuals with desired phenotypic traits. While random mutagenesis can uncover novel genetic variants and phenotypes, it lacks the precision and control offered by targeted mutagenesis techniques like SDM. A related concept that enhances the utility of random mutagenesis is directed evolution. Directed evolution is an iterative process where organisms undergo random mutations, are tested against a screening process, and the best performers are selected for subsequent rounds of mutation. This cycle of mutating, screening, and selecting can be analogized to the training process of deep learning models. In deep learning, a model makes predictions based on input data, receives feedback on the accuracy of these predictions, and then adjusts its parameters through a process known as backpropagation.</p>
<p>In directed evolution, the organism’s genetic material is repeatedly altered and tested, much like a model’s parameters are iteratively refined to improve performance. Each cycle of directed evolution involves creating genetic diversity through random mutations, screening the resultant mutants for desirable traits, and then selecting the top performers for the next round of mutations. This method has been instrumental in fields such as enzyme engineering, where it has led to the development of proteins with enhanced or novel functions. However, It is resource-intensive, requiring significant time and high-throughput screening capabilities.</p>
<p>In contrast, BDTs can accelerate experimentation by suggesting optimized properties of biological agents upfront, thereby potentially reducing the number of tests required to achieve desired outcomes. While the speed of individual experiments may not change, the efficiency of the overall experimentation process is enhanced, as researchers may need to conduct fewer experiments to reach the same or improved results <span class="citation" data-cites="rTpWDKdy">[<a href="#ref-rTpWDKdy" role="doc-biblioref">158</a>]</span>. Examples of BDTs include RFDiffusion <span class="citation" data-cites="iwz5DRLF">[<a href="#ref-iwz5DRLF" role="doc-biblioref">159</a>]</span>, Protein MPNN <span class="citation" data-cites="Ar0akTY8">[<a href="#ref-Ar0akTY8" role="doc-biblioref">160</a>]</span>, and protein language models like ProGen2 <span class="citation" data-cites="NNPUXrCp">[<a href="#ref-NNPUXrCp" role="doc-biblioref">119</a>]</span> and Ankh <span class="citation" data-cites="zATp0wNO">[<a href="#ref-zATp0wNO" role="doc-biblioref">161</a>]</span>. These models can be considered both Prot-LLMs and specific instances within the broader category of BDTs due to their training and output characteristics.</p>
<p>A crucial difference between LLMs and BDTs is both the training data — as LLMs are trained on natural language while BDTs are trained on biological data — and the output — LLMs typically produce outputs in natural language while BDTs produce outputs in the form of biological sequences, structures, and predictions. Although BDTs currently focus on creating sequences by optimizing for a single function, they may eventually evolve to design complex proteins and enzymes with multiple functions and properties. BDTs may eventually develop the capability to engineer whole organisms optimized for various functions and characteristics, addressing a comprehensive range of biological properties <span class="citation" data-cites="hXLTkVKX">[<a href="#ref-hXLTkVKX" role="doc-biblioref">162</a>]</span>.</p>
<p>Of all the categories of AI-enabled BDTs, protein structural prediction tools have the highest relative maturity. Protein structure prediction tools, commonly referred to as ‘folding tools,’ contribute to the field by predicting a protein’s 3D structure, including its secondary, tertiary and quaternary structures from its amino acid sequences. This prediction aids in understanding protein function and interactions. Determining the precise structure of proteins, vital for their functions, has historically posed significant challenges in experimental biology <span class="citation" data-cites="JkRFkNmt">[<a href="#ref-JkRFkNmt" role="doc-biblioref">163</a>]</span>, often requiring years of dedicated effort. However, the landscape has shifted with the advent of AI, tailored to predict protein structures directly from their amino acid sequences.</p>
<p>Notably, pioneering AI systems like AlphaFold <span class="citation" data-cites="qdpFhm5d">[<a href="#ref-qdpFhm5d" role="doc-biblioref">164</a>]</span> and RoseTTAFold <span class="citation" data-cites="aTuvh2K">[<a href="#ref-aTuvh2K" role="doc-biblioref">165</a>]</span> have emerged, revolutionizing the field by drastically reducing structure determination times from months to mere hours. While AlphaFold provides measured structures based on experimental data and computational predictions, RoseTTAFold predicts structures solely through computational methods, sometimes eliminating the need for experimental measurements. AlphaFold 2, released in 2021, marked a significant breakthrough for deep learning in biology by unveiling a vast array of previously unknown protein structures. It quickly became a valuable tool for researchers working to understand everything from cellular structures <span class="citation" data-cites="JVovM7kx">[<a href="#ref-JVovM7kx" role="doc-biblioref">166</a>]</span> to tuberculosis <span class="citation" data-cites="5C9SlgjM">[<a href="#ref-5C9SlgjM" role="doc-biblioref">167</a>]</span>. It also inspired the development of other biological deep learning tools. Most notably, the biochemist David Baker and his team at the University of Washington developed a competing algorithm in 2021 called RoseTTAFold, which, like AlphaFold2, predicts protein structures from sequence data. Both systems have since been enhanced with new features. RoseTTAFold Diffusion is designed to create new proteins that do not exist in nature, while AlphaFold Multimer focuses on the interaction of multiple proteins. These advancements have propelled the development of numerous complementary tools that contextualize biochemical data, screen for protein interactions, and aid in experimental structure elucidation. Furthermore, the predictions from these tools have been integrated into publicly accessible databases, fostering widespread access and collaboration.</p>
<p>Proteins, intricate molecular machines honed by evolution, are built from a repertoire of 20 canonical amino acids, intricately arranged to yield diverse structures crucial for biological functions. Understanding a protein’s 3D structure is paramount, as it dictates its functional properties; for instance, an enzyme’s precise folding enables effective catalysis. Thus, deciphering protein structures not only determines their biological roles but also sheds light on disease-related mutations and their impacts. A longstanding aspiration in structural biology has been the computational prediction of protein structures, circumventing the laborious and expensive experimental methods. Milestones such as the Critical Assessment of Structure Prediction (CASP) <span class="citation" data-cites="t5RQ2moY">[<a href="#ref-t5RQ2moY" role="doc-biblioref">168</a>]</span> have gauged progress in this domain. AlphaFold’s breakthrough at the 13th CASP competition, and subsequent advancements like AlphaFold2 and RoseTTAFold at 14th CASP competition, harnessed the pattern recognition prowess of machine-learning algorithms, trained on vast structural data repositories like the Protein Data Bank (PDB) <span class="citation" data-cites="dCXEowTE">[<a href="#ref-dCXEowTE" role="doc-biblioref">169</a>]</span>. These algorithms, unencumbered by prior exposure to certain proteins, demonstrated remarkable accuracy in structure prediction.</p>
<p>Following the 14th CASP competition, a proliferation of AI-enabled structure predictors has emerged. These predictors employ diverse strategies but share a common goal of understanding spatial proximity among amino acids by tracing evolutionary relationships. Multiple sequence alignment structure predictors (MSA-SPs), exemplified by AlphaFold 2 and RoseTTAFold, analyze co-evolutionary signals gleaned from input sequences to predict structures. In contrast, protein language model structure predictors (pLM-SPs), exemplified by ESMFold <span class="citation" data-cites="tqicyXuM">[<a href="#ref-tqicyXuM" role="doc-biblioref">170</a>]</span> and OmegaFold <span class="citation" data-cites="mG5bRSDU">[<a href="#ref-mG5bRSDU" role="doc-biblioref">171</a>]</span>, embed evolutionary insights directly into their algorithms, eliminating the need for explicit MSA generation.</p>
<p>AlphaFold 3 <span class="citation" data-cites="1Dw3zwV6h">[<a href="#ref-1Dw3zwV6h" role="doc-biblioref">172</a>]</span>, a successor to previous AlphaFold models, was released in 2024 by Google DeepMind. This new version extends its capabilities by predicting the structures of nearly all biological molecules and modeling their interactions. While researchers have previously developed specialized computational methods for modeling interactions between specific types of biological molecules, AlphaFold 3 is the first system capable of predicting interactions between almost all molecular types with state-of-the-art performance. The properties and functions of molecules in biological systems typically depend on their interactions with other molecules. Experimental methods to understand these interactions can take years and be prohibitively expensive. However, if these interactions can be accurately estimated computationally, biological research can be significantly accelerated. For instance, researchers looking for a promising drug candidate that binds a specific protein site can use computational systems like AlphaFold 3 to test potential drug molecules efficiently.</p>
<p>Other subcategories of BDTs include:</p>
<h3 id="table-10.-other-subcategories-of-bdts">Table 10. Other subcategories of BDTs</h3>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Category</th>
<th>Description</th>
<th>Examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Protein sequence design tools</td>
<td>Also known as ‘inverse folding tools,’ predict the sequence of a protein with a user-specified structure and/or functional property, such as binding to a target. These tools play a crucial role in designing proteins tailored to specific requirements.</td>
<td>Rosetta, RoseTTAFold, RF Diffusion</td>
</tr>
<tr class="even">
<td>Small molecule design tools</td>
<td>Designed to predict molecular structures with specific profiles, such as generating drugs that provoke desired biological responses while maintaining acceptable pharmacokinetic properties. These tools are essential in drug discovery and development processes.</td>
<td>REINVENT 4, Chemistry42</td>
</tr>
<tr class="odd">
<td>Vaccine design tools</td>
<td>Pivotal in predicting protective antigens or vaccine subunits from the protein or proteome of target pathogens. By identifying vaccine candidates, these tools contribute significantly to the development of effective vaccines against infectious diseases.</td>
<td>LinearDesign, VSeq-Toolkit</td>
</tr>
<tr class="even">
<td>Viral vector design tools</td>
<td>Focus on predicting the amino acid sequences of virus capsids with the aim of optimizing them as delivery vectors. These vectors are crucial in gene therapy and vaccine development, enabling the efficient delivery of therapeutic genes or vaccine antigens into target cells.</td>
<td>VSeq-Toolkit</td>
</tr>
<tr class="odd">
<td>Genetic modification tools</td>
<td>Analyze genetic sequences to identify sequence features or optimize them for specific purposes. These tools aid in genetic engineering applications by facilitating the modification of DNA sequences to achieve desired outcomes.</td>
<td>OpenCRISPR-1, ZFDesign</td>
</tr>
<tr class="even">
<td>Genome assembly tools</td>
<td>Play a vital role in assembling genomes from multiple short reads generated by DNA sequencing technologies. These tools contribute to genome sequencing projects by reconstructing complete genome sequences from fragmented data.</td>
<td>DeepConsensus</td>
</tr>
<tr class="odd">
<td>Toxicity prediction/detection tools</td>
<td>Designed to predict or detect the molecular toxicity of given molecules or metabolites. These tools are valuable in drug safety assessment and environmental toxicology, aiding in the identification of potentially harmful substances.</td>
<td>TOXSCAPE, GENESCAPE</td>
</tr>
<tr class="even">
<td>Pathogen property prediction tools</td>
<td>Predict or detect features of pathogens, such as propensity for zoonotic spillover or virulence. These tools are crucial in infectious disease surveillance and control, providing insights into the behavior and potential risks associated with pathogens.</td>
<td>MP4</td>
</tr>
<tr class="odd">
<td>Host-pathogen interaction prediction tools</td>
<td>Focus on predicting protein-protein interactions between hosts and pathogenic agents. By elucidating the mechanisms of host-pathogen interactions, these tools contribute to understanding disease pathogenesis and identifying potential therapeutic targets.</td>
<td>HPIPred, deepHPI</td>
</tr>
<tr class="even">
<td>Immunological system modeling tools</td>
<td>Replicate components of the human immune system to predict immune responses, such as T-cell receptor epitope recognition. These tools aid in vaccine design and immunotherapy development by simulating immune responses to pathogens or therapeutic agents.</td>
<td>SIMMUNE</td>
</tr>
<tr class="odd">
<td>Experimental design/planning tools</td>
<td>Generate designs for experiments based on predefined objectives, optimizing experimental variables and methods to achieve desired outcomes. These tools streamline the experimental process, improving efficiency and data quality.</td>
<td>The Experimental Design Assistant (EDA)</td>
</tr>
<tr class="even">
<td>Experimental simulation tools</td>
<td>Simulate and predict experimental outcomes in silico, aiding in the design and interpretation of experiments. By providing insights into potential experimental outcomes, these tools inform experimental planning and hypothesis testing.</td>
<td>PhET, BioSimulators</td>
</tr>
<tr class="odd">
<td>Autonomous experimental platforms</td>
<td>Conduct experiments without human intervention, utilizing laboratory automation equipment to perform physical tests, modeling, or data mining. These platforms enhance experimental throughput and reproducibility, accelerating scientific research and discovery.</td>
<td>BO algorithm with expected Improvement based (EI-based) policy</td>
</tr>
</tbody>
</table>
<h1 id="risks-limitations-and-future-directions">Risks, Limitations and Future Directions</h1>
<p>While recent advancements in AI have enabled rapid progress in the life sciences, it also has several limitations and presents potential risks.</p>
<h2 id="risks-and-limitations">Risks and Limitations</h2>
<h3 id="inaccurate-outputs-from-ai-models">Inaccurate outputs from AI models</h3>
<p>The effectiveness of AI tools relies heavily on the quality of their algorithms and the data they are trained on. When these algorithms contain errors or the datasets are biased or incomplete, the AI models can produce inaccurate outputs. If the models and logic underlying an AI algorithm are incorrect, the AI’s predictions or recommendations will also be incorrect. This can occur due to coding errors, incorrect assumptions in the model design, or inadequate tuning of the model parameters. AI models learn from the data they are trained on. If the training data is biased (e.g., over-represents certain conditions or populations) or incomplete (e.g., missing critical variables or having insufficient diversity), the model’s outputs will reflect these shortcomings. This means the AI could give incorrect advice or predictions, which in biological experiments can lead to wasted time and resources as researchers follow flawed directions. The inaccuracies can misguide researchers, causing them to conduct experiments based on false premises. This not only wastes valuable resources like time, money, and materials but can also delay scientific progress.</p>
<h3 id="development-of-harmful-biological-agents">Development of harmful biological agents</h3>
<p>AI models have the potential to assist in the creation and distribution of harmful biological agents. They could, for example, enable an actor to design a biological agent with favorable properties <span class="citation" data-cites="SnkCHfpX">[<a href="#ref-SnkCHfpX" role="doc-biblioref">173</a>]</span> and modify the agent’s delivery mechanism in a manner that optimizes infectious doses and ensures environmental survival <span class="citation" data-cites="Cicp6dTw">[<a href="#ref-Cicp6dTw" role="doc-biblioref">174</a>]</span>.This possibility raises significant biosecurity concerns. Amateur users are unlikely to utilize BDTs, but experts with malicious intent could leverage their scientific training and specific AI models to design new pathogens, develop synthetic DNA strands that evade screening measures, or enhance the efficiency of bioweapon production <span class="citation" data-cites="e2upB1EQ">[<a href="#ref-e2upB1EQ" role="doc-biblioref">175</a>]</span>. As with any AI system, BDTs depend on the quality of their training data, which can sometimes be limited by incompleteness or unintentional biases. While BDTs have been used to digitally generate potentially risky genetic sequences, research has yet to show if the synthesized sequences could be used to create harmful biological agents. Establishing empirical baselines metrics is essential for conducting risk assessments and tracking changes in risk over time. In AI applications within the life sciences, these metrics and baselines are not yet defined. To assess this risk, we need to systematically evaluate current AI systems’ abilities to generate new sequences versus enhancing existing ones.</p>
<h3 id="ethics-in-ai-for-life-sciences">Ethics in AI for Life Sciences</h3>
<p>The integration of artificial intelligence (AI) in life sciences presents significant ethical challenges that must be addressed to ensure responsible and beneficial use. Key ethical considerations include data privacy, informed consent, and the potential for bias in AI algorithms. Ensuring data privacy is paramount, as AI systems often require access to vast amounts of sensitive biological and medical data. This necessitates robust data protection measures and compliance with legal standards to prevent misuse and unauthorized access <span class="citation" data-cites="12hNAUYKX">[<a href="#ref-12hNAUYKX" role="doc-biblioref">176</a>]</span>. Informed consent is another critical issue, as individuals must be fully aware of how their data will be used and the potential implications of AI-driven analyses <span class="citation" data-cites="17K5Ty7Fj">[<a href="#ref-17K5Ty7Fj" role="doc-biblioref">177</a>]</span>. Additionally, AI algorithms can inadvertently perpetuate or exacerbate existing biases if the training data is not representative of diverse populations, leading to inequitable outcomes in healthcare and research <span class="citation" data-cites="12hNAUYKX">[<a href="#ref-12hNAUYKX" role="doc-biblioref">176</a>]</span>. Addressing these ethical concerns requires a multi-faceted approach, including rigorous testing of AI systems, transparency in AI operations, and the establishment of ethical guidelines and governance frameworks to guide the development and deployment of AI in life sciences <span class="citation" data-cites="ye7UmZPY">[<a href="#ref-ye7UmZPY" role="doc-biblioref">178</a>]</span>. By prioritizing these ethical considerations, we can harness the transformative potential of AI while safeguarding human rights and promoting equitable access to its benefits.</p>
<h2 id="future-directions">Future Directions</h2>
<h3 id="introduction-of-new-benchmarks">Introduction of new benchmarks</h3>
<p>Recent studies have highlighted the shortcomings of existing benchmarks in evaluating LLMs for clinical applications <span class="citation" data-cites="FzEFtq9e nIxULWpe">[<a href="#ref-FzEFtq9e" role="doc-biblioref">179</a>,<a href="#ref-nIxULWpe" role="doc-biblioref">180</a>]</span>. Traditional benchmarks, which focus mainly on accuracy in medical question-answering, fail to capture the full range of clinical skills necessary for LLMs <span class="citation" data-cites="1DrOhLdUs">[<a href="#ref-1DrOhLdUs" role="doc-biblioref">181</a>]</span>. Critics argue that using human-centric standardized medical exams to evaluate LLMs is insufficient, as passing these tests does not reflect the nuanced expertise required in real-world clinical settings <span class="citation" data-cites="1DrOhLdUs">[<a href="#ref-1DrOhLdUs" role="doc-biblioref">181</a>]</span>.</p>
<p>There is a growing consensus on the need for more comprehensive benchmarks. These new benchmarks should assess capabilities such as sourcing information from authoritative medical references, adapting to the evolving medical knowledge landscape, and clearly communicating uncertainties <span class="citation" data-cites="1DrOhLdUs TKv5YqOB">[<a href="#ref-1DrOhLdUs" role="doc-biblioref">181</a>,<a href="#ref-TKv5YqOB" role="doc-biblioref">182</a>]</span>. To further enhance their relevance, benchmarks should include scenarios that test an LLM’s performance in real-world applications and its ability to adapt to feedback from clinicians while maintaining robustness. Given the sensitive nature of healthcare, these benchmarks should evaluate factors like fairness, ethics, and equity, which are crucial yet challenging to quantify <span class="citation" data-cites="1DrOhLdUs">[<a href="#ref-1DrOhLdUs" role="doc-biblioref">181</a>]</span>. By expanding benchmarks to encompass scientific domains, especially the biological domain, we can ensure that LLMs are rigorously evaluated across a broad spectrum of applications, thereby promoting their responsible and effective use in advancing scientific and medical knowledge.</p>
<h3 id="red-blue-and-violet-teaming">Red, blue and violet teaming</h3>
<p>Due to increasing concerns about the safety, security, and trustworthiness of Generative AI models, both practitioners and regulators emphasize the importance of AI red-teaming <span class="citation" data-cites="Q1QW0BcS">[<a href="#ref-Q1QW0BcS" role="doc-biblioref">183</a>]</span>. Originally from cybersecurity, red-teaming involves adopting an adversary’s perspective to find vulnerabilities. In AI, this means simulating attacks on AI applications to identify weaknesses and develop preventive measures <span class="citation" data-cites="3WhKhYXF">[<a href="#ref-3WhKhYXF" role="doc-biblioref">184</a>]</span>. For example, red teams can simulate backdoor attacks or data poisoning to test the AI model’s defenses. Prompt injection, a common attack on generative AI models like LLMs, tricks the model into producing harmful content. Red teams can also prompt AI systems to extract sensitive information from training data.</p>
<p>Blue teaming, which focuses on defending against these simulated attacks, and purple teaming, which combines both red and blue teams for a comprehensive security assessment <span class="citation" data-cites="TKrwMi6z">[<a href="#ref-TKrwMi6z" role="doc-biblioref">185</a>]</span>. However, as AI systems continuously evolve, these strategies might be insufficient, especially in critical sectors like the life sciences <span class="citation" data-cites="9ODUtCAc">[<a href="#ref-9ODUtCAc" role="doc-biblioref">186</a>]</span>.</p>
<p>Violet teaming goes further by pairing red and blue teams to build resilient systems that intend to simultaneously minimize harm and maximize benefit using the very technology that poses potential security risks <span class="citation" data-cites="hhou3YWO">[<a href="#ref-hhou3YWO" role="doc-biblioref">187</a>]</span>. In the life sciences, this might involve using AI models to screen for harmful sequences generated by the models themselves, preventing them from being produced and shared with the end user.</p>
<p>Additionally, Machine Learning Security Operations (MLSecOps) could play a crucial role in ensuring the safety of AI models in the life sciences by employing machine learning (ML) techniques to protect against cyber threats and secure AI/ML models <span class="citation" data-cites="ntSixaJH">[<a href="#ref-ntSixaJH" role="doc-biblioref">188</a>]</span>. MLSecOps focuses on encrypting sensitive genome data, detecting ransomware and Trojan attacks, and ensuring the integrity of ML algorithms used in critical applications. It also addresses vulnerabilities in software and IoT devices within biotechnology labs, enhances supply chain security, and mitigates biases in healthcare ML systems.</p>
<h1 id="conclusion">Conclusion</h1>
<p>The integration of computing technologies into the life sciences has profoundly transformed the field, enabling unprecedented advancements in biological research and applications. From the early days of population genetics calculations in the 1950s to the sophisticated AI-driven models of today, the evolution of computational tools has paralleled and propelled the growth of life sciences.</p>
<h2 id="historical-milestones-and-technological-advancements">Historical Milestones and Technological Advancements</h2>
<p>The journey began with the use of primitive computers for biological modeling, such as Alan Turing’s work on morphogenesis and the MANIAC computer’s genetic code measurements. The 1960s and 1970s saw the rise of computational biology, driven by protein crystallography and the development of bioinformatics software like COMPROTEIN. The advent of dynamic programming algorithms for sequence alignment and the shift from protein to DNA analysis marked significant milestones.</p>
<h2 id="the-genomic-era-and-beyond">The Genomic Era and Beyond</h2>
<p>The 1980s and 1990s were pivotal, with the development of gene targeting techniques, the polymerase chain reaction (PCR), and the emergence of bioinformatics software suites. The completion of the Haemophilus influenzae genome and the human genome project ushered in the genomic era, leading to the creation of specialized software for whole-genome sequencing.</p>
<h2 id="artificial-intelligence-and-machine-learning">Artificial Intelligence and Machine Learning</h2>
<p>The recent decades have witnessed the integration of artificial intelligence (AI) and machine learning (ML) into life sciences, revolutionizing data analysis, drug discovery, and personalized medicine. AI models, from expert systems like DENDRAL and MYCIN to modern deep learning architectures, have enhanced our ability to predict protein structures, analyze genomic data, and design novel biological entities.</p>
<h2 id="emerging-technologies-and-future-directions">Emerging Technologies and Future Directions</h2>
<p>Emerging technologies such as cloud computing, big data analytics, and the Internet of Things (IoT) are further enhancing the capabilities of life sciences research. Cloud-based high-performance computing enables complex data analysis and reduces research cycles, while IoT facilitates real-time data collection and patient monitoring.</p>
<h2 id="challenges-and-ethical-considerations">Challenges and Ethical Considerations</h2>
<p>Despite these advancements, challenges remain. The accuracy of AI models depends on the quality of training data, and there are significant ethical concerns regarding data privacy and the potential misuse of AI in creating harmful biological agents. Addressing these challenges requires robust ethical frameworks, continuous monitoring, and the development of explainable AI systems.</p>
<p>Altogether, the integration of computing in the life sciences has not only accelerated research but also opened new frontiers in understanding and manipulating biological systems. As we move forward, the synergy between computational technologies and life sciences will continue to drive innovation, offering new solutions to complex biological problems and improving human health. The future promises even greater advancements as we harness the power of AI, cloud computing, and other emerging technologies to explore the intricacies of life at unprecedented scales.</p>
<h1 id="glossary">Glossary</h1>
<ul>
<li><p><strong>Algorithm:</strong> A step-by-step procedure or formula for solving a problem, often used in computer programming and computational biology.</p></li>
<li><p><strong>AlphaFold:</strong> An AI program developed by DeepMind that predicts protein structures with high accuracy.</p></li>
<li><p><strong>Autoencoders:</strong> A type of artificial neural network used to learn efficient codings of unlabeled data.</p></li>
<li><p><strong>Bioinformatics:</strong> The application of computer technology to the management and analysis of biological data.</p></li>
<li><p><strong>Computational Biology:</strong> A field that uses mathematical models, algorithms, and computational techniques to understand and analyze biological systems.</p></li>
<li><p><strong>CRISPR:</strong> A technology used for editing genomes, allowing researchers to alter DNA sequences and modify gene function.</p></li>
<li><p><strong>Deep Learning:</strong> A subset of machine learning involving neural networks with many layers.</p></li>
<li><p><strong>DNA Sequencing:</strong> The process of determining the nucleic acid sequence – the order of nucleotides in DNA.</p></li>
<li><p><strong>Fourier Analysis:</strong> A mathematical technique used to transform signals between time (or spatial) domain and frequency domain, applied in protein crystallography to determine structures.</p></li>
<li><p><strong>Fixed-size vector representation:</strong> A numerical representation of a fixed length that encapsulates the information or features extracted from a variable-length input. In machine learning and natural language processing (NLP), fixed-size vector representations are commonly used to represent textual or sequential data.</p></li>
<li><p><strong>Genome:</strong> The complete set of genes or genetic material present in a cell or organism.</p></li>
<li><p><strong>Genomics:</strong> The study of genomes, the complete set of DNA within an organism, including its structure, function, evolution, and mapping.</p></li>
<li><p><strong>Generative Adversarial Networks (GANs):</strong> A class of machine learning systems where two neural networks contest with each other in a game.</p></li>
<li><p><strong>Machine Learning (ML):</strong> A subset of artificial intelligence (AI) that involves the development of algorithms that allow computers to learn from and make predictions based on data.</p></li>
<li><p><strong>Maximum Likelihood Methods:</strong> Statistical methods for estimating the parameters of a model, used in phylogenetic inference to determine the most likely tree structure.</p></li>
<li><p><strong>Metagenomics:</strong> The study of genetic material recovered directly from environmental samples.</p></li>
<li><p><strong>Multiple Sequence Alignment (MSA):</strong> A method used to align three or more biological sequences to identify regions of similarity that may indicate functional, structural, or evolutionary relationships.</p></li>
<li><p><strong>Natural Language Processing (NLP):</strong> The ability of a computer program to understand human language as it is spoken.</p></li>
<li><p><strong>Needleman-Wunsch Algorithm:</strong> An algorithm used for pairwise sequence alignment that employs dynamic programming to find the optimal alignment between two sequences.</p></li>
<li><p><strong>Neural Networks:</strong> A series of algorithms that mimic the operations of a human brain to recognize relationships between vast amounts of data.</p></li>
<li><p><strong>Next-Generation Sequencing (NGS):</strong> High-throughput sequencing technologies that allow for rapid sequencing of DNA or RNA samples.</p></li>
<li><p><strong>PCR (Polymerase Chain Reaction):</strong> A method widely used in molecular biology to make several copies of a specific DNA segment.</p></li>
<li><p><strong>Phylogenetics:</strong> The study of the evolutionary history and relationships among individuals or groups of organisms.</p></li>
<li><p><strong>Proteomics:</strong> The large-scale study of proteins, particularly their structures and functions.</p></li>
<li><p><strong>Reinforcement Learning:</strong> A type of machine learning where an agent learns to behave in an environment by performing actions and seeing the results.</p></li>
<li><p><strong>Systems Biology:</strong> An approach in biomedical research to understanding the larger picture by putting its pieces together (holism instead of reductionism).</p></li>
<li><p><strong>Transcriptomics:</strong> The study of the complete set of RNA transcripts produced by the genome.</p></li>
<li><p><strong>Unsupervised Learning:</strong> A type of machine learning that looks for previously undetected patterns in a data set with no pre-existing labels.</p></li>
</ul>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-gldEpQ9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>Computers in the study of evolution</strong> <div class="csl-block">JL Crosby</div> <em>Science Progress</em> (1967) <a href="https://pubmed.ncbi.nlm.nih.gov/4859964">https://pubmed.ncbi.nlm.nih.gov/4859964</a></div>
</div>
<div id="ref-UK6rEYVY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>The chemical basis of morphogenesis</strong> <div class="csl-block">Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</div> (1952-08-14) <a href="https://royalsocietypublishing.org/doi/10.1098/rstb.1952.0012">https://royalsocietypublishing.org/doi/10.1098/rstb.1952.0012</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1098/rstb.1952.0012">10.1098/rstb.1952.0012</a></div></div>
</div>
<div id="ref-RqzQpRL0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>Scientific uses of the MANIAC</strong> <div class="csl-block">HL Anderson</div> <em>Journal of Statistical Physics</em> (1986-06-01) <a href="https://doi.org/10.1007/BF02628301">https://doi.org/10.1007/BF02628301</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/BF02628301">10.1007/bf02628301</a></div></div>
</div>
<div id="ref-AtEWfwCG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>A three-dimensional model of the myoglobin molecule obtained by x-ray analysis</strong> <div class="csl-block">JC Kendrew, G Bodo, HM Dintzis, RG Parrish, H Wyckoff, DC Phillips</div> <em>Nature</em> (1958-03-08) <a href="https://pubmed.ncbi.nlm.nih.gov/13517261">https://pubmed.ncbi.nlm.nih.gov/13517261</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/181662a0">10.1038/181662a0</a></div></div>
</div>
<div id="ref-WmbDqOWv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Structure of myoglobin: A three-dimensional Fourier synthesis at 2 A. resolution</strong> <div class="csl-block">JC Kendrew, RE Dickerson, BE Strandberg, RG Hart, DR Davies, DC Phillips, VC Shore</div> <em>Nature</em> (1960-02-13) <a href="https://pubmed.ncbi.nlm.nih.gov/18990802">https://pubmed.ncbi.nlm.nih.gov/18990802</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/185422a0">10.1038/185422a0</a></div></div>
</div>
<div id="ref-3t4YHcSd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>The amino-acid sequence in the glycyl chain of insulin. 1. The identification of lower peptides from partial hydrolysates</strong> <div class="csl-block">F Sanger, EOP Thompson</div> <em>Biochemical Journal</em> (1953-02) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198157/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198157/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1042/bj0530353">10.1042/bj0530353</a></div></div>
</div>
<div id="ref-Htmw8erk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>The amino-acid sequence in the glycyl chain of insulin. 2. The investigation of peptides from enzymic hydrolysates</strong> <div class="csl-block">F Sanger, EOP Thompson</div> <em>Biochemical Journal</em> (1953-02) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198158/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198158/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1042/bj0530366">10.1042/bj0530366</a></div></div>
</div>
<div id="ref-Dq9jMQ6v" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>A method for the determination of amino acid sequence in peptides</strong> <div class="csl-block">P Edman</div> <em>Archives of Biochemistry</em> (1949-07) <a href="https://pubmed.ncbi.nlm.nih.gov/18134557">https://pubmed.ncbi.nlm.nih.gov/18134557</a></div>
</div>
<div id="ref-prCR1hpf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>The origins of bioinformatics</strong> <div class="csl-block">JB Hagen</div> <em>Nature Reviews. Genetics</em> (2000-12) <a href="https://pubmed.ncbi.nlm.nih.gov/11252753">https://pubmed.ncbi.nlm.nih.gov/11252753</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/35042090">10.1038/35042090</a></div></div>
</div>
<div id="ref-1Bxyvu5tQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>Comprotein: a computer program to aid primary protein structure determination</strong> <div class="csl-block">Margaret Oakley Dayhoff, Robert S Ledley</div> <em>Proceedings of the December 4-6, 1962, fall joint computer conference on - AFIPS '62 (Fall)</em> (1962) <a href="http://portal.acm.org/citation.cfm?doid=1461518.1461546">http://portal.acm.org/citation.cfm?doid=1461518.1461546</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/1461518.1461546">10.1145/1461518.1461546</a></div></div>
</div>
<div id="ref-ibvKtnrl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><a href="https://febs.onlinelibrary.wiley.com/toc/14321033/5/2">https://febs.onlinelibrary.wiley.com/toc/14321033/5/2</a></div>
</div>
<div id="ref-YYcdJkZI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Collecting, comparing, and computing sequences: the making of Margaret O. Dayhoff's Atlas of Protein Sequence and Structure, 1954-1965</strong> <div class="csl-block">Bruno J Strasser</div> <em>Journal of the History of Biology</em> (2010) <a href="https://pubmed.ncbi.nlm.nih.gov/20665074">https://pubmed.ncbi.nlm.nih.gov/20665074</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s10739-009-9221-0">10.1007/s10739-009-9221-0</a></div></div>
</div>
<div id="ref-C1oThCRN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>A general method applicable to the search for similarities in the amino acid sequence of two proteins</strong> <div class="csl-block">SB Needleman, CD Wunsch</div> <em>Journal of Molecular Biology</em> (1970-03) <a href="https://pubmed.ncbi.nlm.nih.gov/5420325">https://pubmed.ncbi.nlm.nih.gov/5420325</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/0022-2836(70)90057-4">10.1016/0022-2836(70)90057-4</a></div></div>
</div>
<div id="ref-9IbNBDhy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>Progressive sequence alignment as a prerequisite to correct phylogenetic trees</strong> <div class="csl-block">DF Feng, RF Doolittle</div> <em>Journal of Molecular Evolution</em> (1987) <a href="https://pubmed.ncbi.nlm.nih.gov/3118049">https://pubmed.ncbi.nlm.nih.gov/3118049</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/BF02603120">10.1007/bf02603120</a></div></div>
</div>
<div id="ref-r9luxm5X" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>CLUSTAL: a package for performing multiple sequence alignment on a microcomputer</strong> <div class="csl-block">DG Higgins, PM Sharp</div> <em>Gene</em> (1988-12-15) <a href="https://pubmed.ncbi.nlm.nih.gov/3243435">https://pubmed.ncbi.nlm.nih.gov/3243435</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/0378-1119(88)90330-7">10.1016/0378-1119(88)90330-7</a></div></div>
</div>
<div id="ref-10odPMDxs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Clustal Omega, accurate alignment of very large numbers of sequences</strong> <div class="csl-block">Fabian Sievers, Desmond G Higgins</div> <em>Methods in Molecular Biology (Clifton, N.J.)</em> (2014) <a href="https://pubmed.ncbi.nlm.nih.gov/24170397">https://pubmed.ncbi.nlm.nih.gov/24170397</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-1-62703-646-7_6">10.1007/978-1-62703-646-7_6</a></div></div>
</div>
<div id="ref-NQbXc701" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>A new method for sequencing DNA</strong> <div class="csl-block">AM Maxam, W Gilbert</div> <em>Proceedings of the National Academy of Sciences of the United States of America</em> (1977-02) <a href="https://pubmed.ncbi.nlm.nih.gov/265521">https://pubmed.ncbi.nlm.nih.gov/265521</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1073/pnas.74.2.560">10.1073/pnas.74.2.560</a></div></div>
</div>
<div id="ref-10pibmvLC" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>Summary statement of the Asilomar conference on recombinant DNA molecules.</strong> <div class="csl-block">P Berg, D Baltimore, S Brenner, RO Roblin, MF Singer</div> <em>Proceedings of the National Academy of Sciences of the United States of America</em> (1975-06) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432675/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432675/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1073/pnas.72.6.1981">10.1073/pnas.72.6.1981</a></div></div>
</div>
<div id="ref-M8aKGlSe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline"><strong>A strategy of DNA sequencing employing computer programs.</strong> <div class="csl-block">R Staden</div> <em>Nucleic Acids Research</em> (1979-06-06) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC327874/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC327874/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/6.7.2601">10.1093/nar/6.7.2601</a></div></div>
</div>
<div id="ref-nzs4mjEF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline"><strong>Evolutionary trees from DNA sequences: a maximum likelihood approach</strong> <div class="csl-block">J Felsenstein</div> <em>Journal of Molecular Evolution</em> (1981) <a href="https://pubmed.ncbi.nlm.nih.gov/7288891">https://pubmed.ncbi.nlm.nih.gov/7288891</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/BF01734359">10.1007/bf01734359</a></div></div>
</div>
<div id="ref-1CARqSM3b" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline"><strong>Probability distribution of molecular evolutionary trees: a new method of phylogenetic inference</strong> <div class="csl-block">B Rannala, Z Yang</div> <em>Journal of Molecular Evolution</em> (1996-09) <a href="https://pubmed.ncbi.nlm.nih.gov/8703097">https://pubmed.ncbi.nlm.nih.gov/8703097</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/BF02338839">10.1007/bf02338839</a></div></div>
</div>
<div id="ref-17nhyKsKm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline"><strong>A biologist's guide to Bayesian phylogenetic analysis</strong> <div class="csl-block">Fabrícia F Nascimento, Mario Dos Reis, Ziheng Yang</div> <em>Nature Ecology &amp; Evolution</em> (2017-10) <a href="https://pubmed.ncbi.nlm.nih.gov/28983516">https://pubmed.ncbi.nlm.nih.gov/28983516</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41559-017-0280-x">10.1038/s41559-017-0280-x</a></div></div>
</div>
<div id="ref-CvVpaf4a" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline"><strong>The Nobel Prize in Chemistry 1993</strong> <div class="csl-block">NobelPrize.org</div> <a href="https://www.nobelprize.org/prizes/chemistry/1993/mullis/lecture/">https://www.nobelprize.org/prizes/chemistry/1993/mullis/lecture/</a></div>
</div>
<div id="ref-12FKql4zv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline"><strong>A comprehensive set of sequence analysis programs for the VAX</strong> <div class="csl-block">J Devereux, P Haeberli, O Smithies</div> <em>Nucleic Acids Research</em> (1984-01-11) <a href="https://pubmed.ncbi.nlm.nih.gov/6546423">https://pubmed.ncbi.nlm.nih.gov/6546423</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/12.1part1.387">10.1093/nar/12.1part1.387</a></div></div>
</div>
<div id="ref-17EunIaqv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline"><strong>DNASTAR’s Lasergene Sequence Analysis Software</strong> <div class="csl-block">Timothy G Burland</div> <em>Bioinformatics Methods and Protocols</em> (1999) <a href="https://doi.org/10.1385/1-59259-192-2:71">https://doi.org/10.1385/1-59259-192-2:71</a> <div class="csl-block">ISBN: 9781592591923</div></div>
</div>
<div id="ref-dSNqAXK0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline"><strong>Apple II PASCAL programs for molecular biologists</strong> <div class="csl-block">B Malthiery, B Bellon, D Giorgi, B Jacq</div> <em>Nucleic Acids Research</em> (1984-01-11) <a href="https://pubmed.ncbi.nlm.nih.gov/6320099">https://pubmed.ncbi.nlm.nih.gov/6320099</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/12.1part2.569">10.1093/nar/12.1part2.569</a></div></div>
</div>
<div id="ref-17gPkdn86" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline"><strong>The GNU Manifesto - GNU Project - Free Software Foundation</strong> <a href="https://www.gnu.org/gnu/manifesto.en.html">https://www.gnu.org/gnu/manifesto.en.html</a></div>
</div>
<div id="ref-WD5AIwcy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/221307757_The_Free_Software_Movement_and_the_GNULinux_Operating_System">https://www.researchgate.net/publication/221307757_The_Free_Software_Movement_and_the_GNULinux_Operating_System</a></div>
</div>
<div id="ref-11GoDnHp3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline"><strong>Whole-genome random sequencing and assembly of Haemophilus influenzae Rd</strong> <div class="csl-block">RD Fleischmann, MD Adams, O White, RA Clayton, EF Kirkness, AR Kerlavage, CJ Bult, JF Tomb, BA Dougherty, JM Merrick</div> <em>Science (New York, N.Y.)</em> (1995-07-28) <a href="https://pubmed.ncbi.nlm.nih.gov/7542800">https://pubmed.ncbi.nlm.nih.gov/7542800</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.7542800">10.1126/science.7542800</a></div></div>
</div>
<div id="ref-7GkBaAWF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline"><strong>The sequence of the human genome</strong> <div class="csl-block">JC Venter, MD Adams, EW Myers, PW Li, RJ Mural, GG Sutton, HO Smith, M Yandell, CA Evans, RA Holt, … X Zhu</div> <em>Science (New York, N.Y.)</em> (2001-02-16) <a href="https://pubmed.ncbi.nlm.nih.gov/11181995">https://pubmed.ncbi.nlm.nih.gov/11181995</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.1058040">10.1126/science.1058040</a></div></div>
</div>
<div id="ref-Ero6Muzo" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline"><strong>Consed: a graphical tool for sequence finishing</strong> <div class="csl-block">D Gordon, C Abajian, P Green</div> <em>Genome Research</em> (1998-03) <a href="https://pubmed.ncbi.nlm.nih.gov/9521923">https://pubmed.ncbi.nlm.nih.gov/9521923</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/gr.8.3.195">10.1101/gr.8.3.195</a></div></div>
</div>
<div id="ref-18guJvxxN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline"><strong>A whole-genome assembly of Drosophila</strong> <div class="csl-block">EW Myers, GG Sutton, AL Delcher, IM Dew, DP Fasulo, MJ Flanigan, SA Kravitz, CM Mobarry, KH Reinert, KA Remington, … JC Venter</div> <em>Science (New York, N.Y.)</em> (2000-03-24) <a href="https://pubmed.ncbi.nlm.nih.gov/10731133">https://pubmed.ncbi.nlm.nih.gov/10731133</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.287.5461.2196">10.1126/science.287.5461.2196</a></div></div>
</div>
<div id="ref-i2bVokif" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline"><strong>The EMBL data library</strong> <div class="csl-block">CM Rice, R Fuchs, DG Higgins, PJ Stoehr, GN Cameron</div> <em>Nucleic Acids Research</em> (1993-07-01) <a href="https://pubmed.ncbi.nlm.nih.gov/8332519">https://pubmed.ncbi.nlm.nih.gov/8332519</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/21.13.2967">10.1093/nar/21.13.2967</a></div></div>
</div>
<div id="ref-Tx5CQPm8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline"><strong>GenBank</strong> <div class="csl-block">Dennis A Benson, Mark Cavanaugh, Karen Clark, Ilene Karsch-Mizrachi, David J Lipman, James Ostell, Eric W Sayers</div> <em>Nucleic Acids Research</em> (2013-01) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531190/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531190/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/gks1195">10.1093/nar/gks1195</a></div></div>
</div>
<div id="ref-qPG24gP6" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline"><strong>BLAST: at the core of a powerful and diverse set of sequence analysis tools</strong> <div class="csl-block">Scott McGinnis, Thomas L Madden</div> <em>Nucleic Acids Research</em> (2004-07-07) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC441573/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC441573/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/gkh435">10.1093/nar/gkh435</a></div></div>
</div>
<div id="ref-14cnpLsxs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline"><strong>Genomes OnLine Database (GOLD): a monitor of genome projects world-wide</strong> <div class="csl-block">Axel Bernal, Uy Ear, Nikos Kyrpides</div> <em>Nucleic Acids Research</em> (2001-01-01) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC29859/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC29859/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/29.1.126">10.1093/nar/29.1.126</a></div></div>
</div>
<div id="ref-1DNwvIpHH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline"><strong>PubMed</strong> <div class="csl-block">PubMed</div> <a href="https://pubmed.ncbi.nlm.nih.gov/">https://pubmed.ncbi.nlm.nih.gov/</a></div>
</div>
<div id="ref-yOdF9F1J" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline"><a href="https://academic.oup.com/nar/article/26/1/94/2379498">https://academic.oup.com/nar/article/26/1/94/2379498</a></div>
</div>
<div id="ref-XVMV1nZ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline"><strong>Accurate structure prediction of biomolecular interactions with AlphaFold 3</strong> <div class="csl-block">Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, … John M Jumper</div> <em>Nature</em> (2024-06) <a href="https://pubmed.ncbi.nlm.nih.gov/38718835">https://pubmed.ncbi.nlm.nih.gov/38718835</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-024-07487-w">10.1038/s41586-024-07487-w</a></div></div>
</div>
<div id="ref-UglDtG3f" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline"><strong>The BLEND system Programme for the study of some ‘electronic journals’∗</strong> <div class="csl-block">B Shackel</div> <em>Ergonomics</em> (1982-04) <a href="http://www.tandfonline.com/doi/abs/10.1080/00140138208924954">http://www.tandfonline.com/doi/abs/10.1080/00140138208924954</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1080/00140138208924954">10.1080/00140138208924954</a></div></div>
</div>
<div id="ref-jMSKwR4o" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline"><strong>arXiv.org e-Print archive</strong> <a href="https://arxiv.org/">https://arxiv.org/</a></div>
</div>
<div id="ref-iWYRgRGv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline"><strong>bioRxiv.org - the preprint server for Biology</strong> <a href="https://www.biorxiv.org/">https://www.biorxiv.org/</a></div>
</div>
<div id="ref-4gIVWSCR" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline"><strong>Configurations of Polypeptide Chains With Favored Orientations Around Single Bonds: Two New Pleated Sheets</strong> <div class="csl-block">L Pauling, RB Corey</div> <em>Proceedings of the National Academy of Sciences of the United States of America</em> (1951-11) <a href="https://pubmed.ncbi.nlm.nih.gov/16578412">https://pubmed.ncbi.nlm.nih.gov/16578412</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1073/pnas.37.11.729">10.1073/pnas.37.11.729</a></div></div>
</div>
<div id="ref-cc2nygki" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline"><strong>Sixty-five years of the long march in protein secondary structure prediction: the final stretch?</strong> <div class="csl-block">Yuedong Yang, Jianzhao Gao, Jihua Wang, Rhys Heffernan, Jack Hanson, Kuldip Paliwal, Yaoqi Zhou</div> <em>Briefings in Bioinformatics</em> (2018-05-01) <a href="https://pubmed.ncbi.nlm.nih.gov/28040746">https://pubmed.ncbi.nlm.nih.gov/28040746</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bib/bbw129">10.1093/bib/bbw129</a></div></div>
</div>
<div id="ref-eM1kmLmc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline"><strong>Computational methods for protein structure prediction and modeling</strong> <div class="csl-block">New York, N.Y. : Springer</div> (2007) <a href="http://archive.org/details/computationalmet0000unse_u4q5">http://archive.org/details/computationalmet0000unse_u4q5</a> <div class="csl-block">ISBN: 9780387333212</div></div>
</div>
<div id="ref-Z7e2T8go" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline"><strong>Molecular dynamics simulations: advances and applications</strong> <div class="csl-block">Adam Hospital, Josep Ramon Goñi, Modesto Orozco, Josep L Gelpí</div> <em>Advances and Applications in Bioinformatics and Chemistry : AABC</em> (2015) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4655909/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4655909/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2147/AABC.S70333">10.2147/aabc.s70333</a></div></div>
</div>
<div id="ref-sT4CEJLS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline"><strong>To Milliseconds and Beyond: Challenges in the Simulation of Protein Folding</strong> <div class="csl-block">Thomas J Lane, Diwakar Shukla, Kyle A Beauchamp, Vijay S Pande</div> <em>Current opinion in structural biology</em> (2013-02) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673555/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673555/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.sbi.2012.11.002">10.1016/j.sbi.2012.11.002</a></div></div>
</div>
<div id="ref-16JwdMPBH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline"><strong>Next-Generation Sequencing and Its Application: Empowering in Public Health Beyond Reality</strong> <div class="csl-block">Nidhi Gupta, Vijay K Verma</div> <em>Microbial Technology for the Welfare of Society</em> (2019) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7122948/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7122948/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-981-13-8844-6_15">10.1007/978-981-13-8844-6_15</a></div></div>
</div>
<div id="ref-1BWjfloX3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline"><strong>BOINC: A Platform for Volunteer Computing</strong> <div class="csl-block">David P Anderson</div> <em>Journal of Grid Computing</em> (2020-03-01) <a href="https://doi.org/10.1007/s10723-019-09497-9">https://doi.org/10.1007/s10723-019-09497-9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s10723-019-09497-9">10.1007/s10723-019-09497-9</a></div></div>
</div>
<div id="ref-1DngqA2eE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline"><strong>Structural proteomics by NMR spectroscopy</strong> <div class="csl-block">Joon Shin, Woonghee Lee, Weontae Lee</div> <em>Expert Review of Proteomics</em> (2008-08) <a href="https://pubmed.ncbi.nlm.nih.gov/18761469">https://pubmed.ncbi.nlm.nih.gov/18761469</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1586/14789450.5.4.589">10.1586/14789450.5.4.589</a></div></div>
</div>
<div id="ref-FWkpcKEh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline"><strong>A whole-cell computational model predicts phenotype from genotype</strong> <div class="csl-block">Jonathan R Karr, Jayodita C Sanghvi, Derek N Macklin, Miriam V Gutschow, Jared M Jacobs, Benjamin Bolival, Nacyra Assad-Garcia, John I Glass, Markus W Covert</div> <em>Cell</em> (2012-07-20) <a href="https://pubmed.ncbi.nlm.nih.gov/22817898">https://pubmed.ncbi.nlm.nih.gov/22817898</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cell.2012.05.044">10.1016/j.cell.2012.05.044</a></div></div>
</div>
<div id="ref-MNG16ntb" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline"><strong>Using deep learning to model the hierarchical structure and function of a cell</strong> <div class="csl-block">Jianzhu Ma, Michael Ku Yu, Samson Fong, Keiichiro Ono, Eric Sage, Barry Demchak, Roded Sharan, Trey Ideker</div> <em>Nature Methods</em> (2018-04) <a href="https://www.nature.com/articles/nmeth.4627">https://www.nature.com/articles/nmeth.4627</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nmeth.4627">10.1038/nmeth.4627</a></div></div>
</div>
<div id="ref-ZSOb6iY1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline"><strong>Why Build Whole-Cell Models?</strong> <div class="csl-block">Javier Carrera, Markus W Covert</div> <em>Trends in Cell Biology</em> (2015-12) <a href="https://pubmed.ncbi.nlm.nih.gov/26471224">https://pubmed.ncbi.nlm.nih.gov/26471224</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.tcb.2015.09.004">10.1016/j.tcb.2015.09.004</a></div></div>
</div>
<div id="ref-EOFcwWBm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline"><strong>The future of whole-cell modeling</strong> <div class="csl-block">Derek N Macklin, Nicholas A Ruggero, Markus W Covert</div> <em>Current Opinion in Biotechnology</em> (2014-08) <a href="https://pubmed.ncbi.nlm.nih.gov/24556244">https://pubmed.ncbi.nlm.nih.gov/24556244</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.copbio.2014.01.012">10.1016/j.copbio.2014.01.012</a></div></div>
</div>
<div id="ref-OHHpDn0z" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline"><strong>Computer-Aided Whole-Cell Design: Taking a Holistic Approach by Integrating Synthetic With Systems Biology</strong> <div class="csl-block">Lucia Marucci, Matteo Barberis, Jonathan Karr, Oliver Ray, Paul R Race, Miguel de Souza Andrade, Claire Grierson, Stefan Andreas Hoffmann, Sophie Landon, Elibio Rech, … Christopher Woods</div> <em>Frontiers in Bioengineering and Biotechnology</em> (2020) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426639/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426639/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3389/fbioe.2020.00942">10.3389/fbioe.2020.00942</a></div></div>
</div>
<div id="ref-16yino3ku" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline"><strong>Accelerated discovery via a whole-cell model</strong> <div class="csl-block">Jayodita C Sanghvi, Sergi Regot, Silvia Carrasco, Jonathan R Karr, Miriam V Gutschow, Jr Benjamin Bolival, Markus W Covert</div> <em>Nature methods</em> (2013-12) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3856890/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3856890/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nmeth.2724">10.1038/nmeth.2724</a></div></div>
</div>
<div id="ref-MgRWKCpM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline"><strong>A blueprint for human whole-cell modeling</strong> <div class="csl-block">Balázs Szigeti, Yosef D Roth, John AP Sekar, Arthur P Goldberg, Saahith C Pochiraju, Jonathan R Karr</div> <em>Current opinion in systems biology</em> (2018-02) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966287/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966287/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.coisb.2017.10.005">10.1016/j.coisb.2017.10.005</a></div></div>
</div>
<div id="ref-AviPiwRn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/30874496_Artificial_Intelligence_A_Modern_Approach">https://www.researchgate.net/publication/30874496_Artificial_Intelligence_A_Modern_Approach</a></div>
</div>
<div id="ref-Gz18T5lX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/223020409_Chen_CM_Intelligent_Web-based_Learning_System_with_Personalized_Learning_Path_Guidance_Computers_Education_512_787-814">https://www.researchgate.net/publication/223020409_Chen_CM_Intelligent_Web-based_Learning_System_with_Personalized_Learning_Path_Guidance_Computers_Education_512_787-814</a></div>
</div>
<div id="ref-dae8L6vr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline"><strong>Expert systems: An overview | IEEE Journals &amp; Magazine | IEEE Xplore</strong> <a href="https://ieeexplore.ieee.org/document/1145205">https://ieeexplore.ieee.org/document/1145205</a></div>
</div>
<div id="ref-bUBwlqml" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline"><strong>On Interface Requirements for Expert Systems</strong> <div class="csl-block">R Wexelblat</div> <em>The AI Magazine</em> (1989) <a href="https://www.semanticscholar.org/paper/On-Interface-Requirements-for-Expert-Systems-Wexelblat/291bffa7fec4fafff62462d015dd86c466273d4c">https://www.semanticscholar.org/paper/On-Interface-Requirements-for-Expert-Systems-Wexelblat/291bffa7fec4fafff62462d015dd86c466273d4c</a></div>
</div>
<div id="ref-TjcFIKAL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline"><a href="https://stacks.stanford.edu/file/druid:pj337tr4694/pj337tr4694.pdf">https://stacks.stanford.edu/file/druid:pj337tr4694/pj337tr4694.pdf</a></div>
</div>
<div id="ref-DOfI6ZOm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline"><strong>MYCIN: a knowledge-based consultation program for infectious disease diagnosis</strong> <div class="csl-block">William van Melle</div> <em>International Journal of Man-Machine Studies</em> (1978-05-01) <a href="https://www.sciencedirect.com/science/article/pii/S0020737378800492">https://www.sciencedirect.com/science/article/pii/S0020737378800492</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/S0020-7373(78)80049-2">10.1016/s0020-7373(78)80049-2</a></div></div>
</div>
<div id="ref-om2dJrZ8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/235028224_The_Applicability_and_Limitations_of_Expert_System_Shells">https://www.researchgate.net/publication/235028224_The_Applicability_and_Limitations_of_Expert_System_Shells</a></div>
</div>
<div id="ref-Q6HnLWXu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline"><strong>Artificial intelligence, machine learning and deep learning: Potential resources for the infection clinician</strong> <div class="csl-block">Anastasia A Theodosiou, Robert C Read</div> <em>The Journal of Infection</em> (2023-10) <a href="https://pubmed.ncbi.nlm.nih.gov/37468046">https://pubmed.ncbi.nlm.nih.gov/37468046</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jinf.2023.07.006">10.1016/j.jinf.2023.07.006</a></div></div>
</div>
<div id="ref-117qAPYe7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">66. </div><div class="csl-right-inline"><strong>A survey on semi-supervised learning</strong> <div class="csl-block">Jesper E van Engelen, Holger H Hoos</div> <em>Machine Learning</em> (2020-02-01) <a href="https://doi.org/10.1007/s10994-019-05855-6">https://doi.org/10.1007/s10994-019-05855-6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s10994-019-05855-6">10.1007/s10994-019-05855-6</a></div></div>
</div>
<div id="ref-DWZxgqvi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">67. </div><div class="csl-right-inline"><strong>Unsupervised Learning and Pattern Recognition of Biological Data Structures with Density Functional Theory and Machine Learning</strong> <div class="csl-block">Chien-Chang Chen, Hung-Hui Juan, Meng-Yuan Tsai, Henry Horng-Shing Lu</div> <em>Scientific Reports</em> (2018) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5765025/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5765025/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41598-017-18931-5">10.1038/s41598-017-18931-5</a></div></div>
</div>
<div id="ref-bLNWjb9u" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">68. </div><div class="csl-right-inline"><strong>Unsupervised K-Means Clustering Algorithm | IEEE Journals &amp; Magazine | IEEE Xplore</strong> <a href="https://ieeexplore.ieee.org/document/9072123">https://ieeexplore.ieee.org/document/9072123</a></div>
</div>
<div id="ref-IWQdHNWs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">69. </div><div class="csl-right-inline"><strong>Efficient Training Management for Mobile Crowd-Machine Learning: A Deep Reinforcement Learning Approach | IEEE Journals &amp; Magazine | IEEE Xplore</strong> <a href="https://ieeexplore.ieee.org/document/8716527">https://ieeexplore.ieee.org/document/8716527</a></div>
</div>
<div id="ref-13wKjJvoL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">70. </div><div class="csl-right-inline"><strong>Machine learning in construction: From shallow to deep learning</strong> <div class="csl-block">Yayin Xu, Ying Zhou, Przemyslaw Sekula, Lieyun Ding</div> <em>Developments in the Built Environment</em> (2021-05-01) <a href="https://www.sciencedirect.com/science/article/pii/S2666165921000041">https://www.sciencedirect.com/science/article/pii/S2666165921000041</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.dibe.2021.100045">10.1016/j.dibe.2021.100045</a></div></div>
</div>
<div id="ref-7jG4ORkP" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">71. </div><div class="csl-right-inline"><strong>An Introduction to Convolutional Neural Networks</strong> <div class="csl-block">Keiron O'Shea, Ryan Nash</div> <em>arXiv</em> (2015-12-02) <a href="http://arxiv.org/abs/1511.08458">http://arxiv.org/abs/1511.08458</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1511.08458">10.48550/arxiv.1511.08458</a></div></div>
</div>
<div id="ref-To07TIhw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">72. </div><div class="csl-right-inline"><strong>Recurrent Neural Networks (RNNs): A gentle Introduction and Overview</strong> <div class="csl-block">Robin M Schmidt</div> <em>arXiv</em> (2019-11-23) <a href="http://arxiv.org/abs/1912.05911">http://arxiv.org/abs/1912.05911</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1912.05911">10.48550/arxiv.1912.05911</a></div></div>
</div>
<div id="ref-yxIOlxEU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">73. </div><div class="csl-right-inline"><strong>Distributed representations, simple recurrent networks, and grammatical structure</strong> <div class="csl-block">Jeffrey L Elman</div> <em>Machine Learning</em> (1991-09-01) <a href="https://doi.org/10.1007/BF00114844">https://doi.org/10.1007/BF00114844</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/BF00114844">10.1007/bf00114844</a></div></div>
</div>
<div id="ref-LoYLzWSf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">74. </div><div class="csl-right-inline"><strong>Autoencoders</strong> <div class="csl-block">Dor Bank, Noam Koenigstein, Raja Giryes</div> <em>arXiv</em> (2021-04-03) <a href="http://arxiv.org/abs/2003.05991">http://arxiv.org/abs/2003.05991</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2003.05991">10.48550/arxiv.2003.05991</a></div></div>
</div>
<div id="ref-J5FKNc97" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">75. </div><div class="csl-right-inline"><strong>Generative Adversarial Networks</strong> <div class="csl-block">Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio</div> <em>arXiv</em> (2014-06-10) <a href="http://arxiv.org/abs/1406.2661">http://arxiv.org/abs/1406.2661</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1406.2661">10.48550/arxiv.1406.2661</a></div></div>
</div>
<div id="ref-7ikCKv12" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">76. </div><div class="csl-right-inline"><strong>Natural language processing: state of the art, current trends and challenges</strong> <div class="csl-block">Diksha Khurana, Aditya Koli, Kiran Khatter, Sukhdev Singh</div> <em>Multimedia Tools and Applications</em> (2023) <a href="https://pubmed.ncbi.nlm.nih.gov/35855771">https://pubmed.ncbi.nlm.nih.gov/35855771</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s11042-022-13428-4">10.1007/s11042-022-13428-4</a></div></div>
</div>
<div id="ref-1J5MgQt2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">77. </div><div class="csl-right-inline"><strong>Generative AI: A systematic review using topic modelling techniques</strong> <div class="csl-block">Priyanka Gupta, Bosheng Ding, Chong Guan, Ding Ding</div> <em>Data and Information Management</em> (2024-06-01) <a href="https://www.sciencedirect.com/science/article/pii/S2543925124000020">https://www.sciencedirect.com/science/article/pii/S2543925124000020</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.dim.2024.100066">10.1016/j.dim.2024.100066</a></div></div>
</div>
<div id="ref-12eS31RFj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">78. </div><div class="csl-right-inline"><strong>Auto-Encoding Variational Bayes</strong> <div class="csl-block">Diederik P Kingma, Max Welling</div> <em>arXiv</em> (2022-12-10) <a href="http://arxiv.org/abs/1312.6114">http://arxiv.org/abs/1312.6114</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1312.6114">10.48550/arxiv.1312.6114</a></div></div>
</div>
<div id="ref-z4zN2ueG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">79. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/377955158_Autoencoders_and_their_applications_in_machine_learning_a_survey">https://www.researchgate.net/publication/377955158_Autoencoders_and_their_applications_in_machine_learning_a_survey</a></div>
</div>
<div id="ref-OvkIs2vK" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">80. </div><div class="csl-right-inline"><strong>Unsupervised deep learning with variational autoencoders applied to breast tumor genome-wide DNA methylation data with biologic feature extraction</strong> <div class="csl-block">Alexander J Titus, Owen M Wilkins, Carly A Bobak, Brock C Christensen</div> <em>bioRxiv</em> (2018-11-07) <a href="https://www.biorxiv.org/content/10.1101/433763v5">https://www.biorxiv.org/content/10.1101/433763v5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/433763">10.1101/433763</a></div></div>
</div>
<div id="ref-Miw01DVK" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">81. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/322870935_A_New_Dimension_of_Breast_Cancer_Epigenetics_-_Applications_of_Variational_Autoencoders_with_DNA_Methylation">https://www.researchgate.net/publication/322870935_A_New_Dimension_of_Breast_Cancer_Epigenetics_-_Applications_of_Variational_Autoencoders_with_DNA_Methylation</a></div>
</div>
<div id="ref-1EHGNXskO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">82. </div><div class="csl-right-inline"><strong>A Style-Based Generator Architecture for Generative Adversarial Networks</strong> <div class="csl-block">Tero Karras, Samuli Laine, Timo Aila</div> <em>arXiv</em> (2019-03-29) <a href="http://arxiv.org/abs/1812.04948">http://arxiv.org/abs/1812.04948</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1812.04948">10.48550/arxiv.1812.04948</a></div></div>
</div>
<div id="ref-tzRTBD2w" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">83. </div><div class="csl-right-inline"><strong>Denoising Diffusion Probabilistic Models</strong> <div class="csl-block">Jonathan Ho, Ajay Jain, Pieter Abbeel</div> <em>arXiv</em> (2020-12-16) <a href="http://arxiv.org/abs/2006.11239">http://arxiv.org/abs/2006.11239</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2006.11239">10.48550/arxiv.2006.11239</a></div></div>
</div>
<div id="ref-emwEKYnI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">84. </div><div class="csl-right-inline"><strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong> <div class="csl-block">Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer</div> <em>arXiv</em> (2022-04-13) <a href="http://arxiv.org/abs/2112.10752">http://arxiv.org/abs/2112.10752</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2112.10752">10.48550/arxiv.2112.10752</a></div></div>
</div>
<div id="ref-gdGFbXoj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">85. </div><div class="csl-right-inline"><strong>Attention Is All You Need</strong> <div class="csl-block">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin</div> <em>arXiv</em> (2023-08-01) <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1706.03762">10.48550/arxiv.1706.03762</a></div></div>
</div>
<div id="ref-UgpiYzZc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">86. </div><div class="csl-right-inline"><a href="https://openai.com/index/chatgpt">https://openai.com/index/chatgpt</a></div>
</div>
<div id="ref-1DdwoI1Kr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">87. </div><div class="csl-right-inline"><strong>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</strong> <div class="csl-block">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu</div> <em>arXiv</em> (2023-09-19) <a href="http://arxiv.org/abs/1910.10683">http://arxiv.org/abs/1910.10683</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1910.10683">10.48550/arxiv.1910.10683</a></div></div>
</div>
<div id="ref-cnHJ2ZPM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">88. </div><div class="csl-right-inline"><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</strong> <div class="csl-block">Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</div> <em>arXiv</em> (2019-05-24) <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1810.04805">10.48550/arxiv.1810.04805</a></div></div>
</div>
<div id="ref-ihwUK6Sk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">89. </div><div class="csl-right-inline"><strong>Language Models are Few-Shot Learners</strong> <div class="csl-block">Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, … Dario Amodei</div> <em>arXiv</em> (2020-07-22) <a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2005.14165">10.48550/arxiv.2005.14165</a></div></div>
</div>
<div id="ref-eRP3HctH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">90. </div><div class="csl-right-inline"><strong>A Survey of Large Language Models in Medicine: Progress, Application, and Challenge</strong> <div class="csl-block">Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S Chen, Peilin Zhou, Junling Liu, … David A Clifton</div> <em>arXiv</em> (2024-05-15) <a href="http://arxiv.org/abs/2311.05112">http://arxiv.org/abs/2311.05112</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2311.05112">10.48550/arxiv.2311.05112</a></div></div>
</div>
<div id="ref-1BrqDsUlc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">91. </div><div class="csl-right-inline"><strong>Artificial intelligence in medicine: current trends and future possibilities</strong> <div class="csl-block">Varun H Buch, Irfan Ahmed, Mahiben Maruthappu</div> <em>The British Journal of General Practice</em> (2018-03) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5819974/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5819974/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3399/bjgp18X695213">10.3399/bjgp18x695213</a></div></div>
</div>
<div id="ref-OrFhTkp9" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">92. </div><div class="csl-right-inline"><strong>GLUE Benchmark</strong> <a href="https://gluebenchmark.com/">https://gluebenchmark.com/</a></div>
</div>
<div id="ref-IyaaHAPt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">93. </div><div class="csl-right-inline"><strong>SuperGLUE Benchmark</strong> <div class="csl-block">SuperGLUE Benchmark</div> <a href="https://super.gluebenchmark.com/">https://super.gluebenchmark.com/</a></div>
</div>
<div id="ref-eMFlKM40" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">94. </div><div class="csl-right-inline"><strong>HellaSwag: Can a Machine Really Finish Your Sentence?</strong> <div class="csl-block">Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi</div> <em>arXiv</em> (2019-05-19) <a href="http://arxiv.org/abs/1905.07830">http://arxiv.org/abs/1905.07830</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1905.07830">10.48550/arxiv.1905.07830</a></div></div>
</div>
<div id="ref-z4KRV9wF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">95. </div><div class="csl-right-inline"><strong>TruthfulQA: Measuring How Models Mimic Human Falsehoods</strong> <div class="csl-block">Stephanie Lin, Jacob Hilton, Owain Evans</div> <em>arXiv</em> (2022-05-07) <a href="http://arxiv.org/abs/2109.07958">http://arxiv.org/abs/2109.07958</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2109.07958">10.48550/arxiv.2109.07958</a></div></div>
</div>
<div id="ref-1DdXyopJh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">96. </div><div class="csl-right-inline"><strong>Measuring Massive Multitask Language Understanding</strong> <div class="csl-block">Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt</div> <em>arXiv</em> (2021-01-12) <a href="http://arxiv.org/abs/2009.03300">http://arxiv.org/abs/2009.03300</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2009.03300">10.48550/arxiv.2009.03300</a></div></div>
</div>
<div id="ref-xoXi6JBv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">97. </div><div class="csl-right-inline"><strong>Scientific Large Language Models: A Survey on Biological &amp; Chemical Domains</strong> <div class="csl-block">Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, … Huajun Chen</div> <em>arXiv</em> (2024-01-26) <a href="http://arxiv.org/abs/2401.14656">http://arxiv.org/abs/2401.14656</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2401.14656">10.48550/arxiv.2401.14656</a></div></div>
</div>
<div id="ref-11KKjDIVI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">98. </div><div class="csl-right-inline"><strong>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</strong> <div class="csl-block">Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang</div> <em>Bioinformatics</em> (2020-02-15) <a href="http://arxiv.org/abs/1901.08746">http://arxiv.org/abs/1901.08746</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btz682">10.1093/bioinformatics/btz682</a></div></div>
</div>
<div id="ref-WWUD4rgh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">99. </div><div class="csl-right-inline"><strong>BioMegatron: Larger Biomedical Domain Language Model</strong> <div class="csl-block">Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani</div> <em>arXiv</em> (2020-10-13) <a href="http://arxiv.org/abs/2010.06060">http://arxiv.org/abs/2010.06060</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2010.06060">10.48550/arxiv.2010.06060</a></div></div>
</div>
<div id="ref-1E2ZqNklN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">100. </div><div class="csl-right-inline"><strong>Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing</strong> <div class="csl-block">Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, Hoifung Poon</div> <em>ACM Transactions on Computing for Healthcare</em> (2022-01-31) <a href="http://arxiv.org/abs/2007.15779">http://arxiv.org/abs/2007.15779</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3458754">10.1145/3458754</a></div></div>
</div>
<div id="ref-xBZDDNz8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">101. </div><div class="csl-right-inline"><strong>BioM-Transformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA</strong> <div class="csl-block">Sultan Alrowili, Vijay Shanker</div> <em>Proceedings of the 20th Workshop on Biomedical Language Processing</em> (2021-06) <a href="https://aclanthology.org/2021.bionlp-1.24">https://aclanthology.org/2021.bionlp-1.24</a> <div class="csl-block">DOI: <a href="https://doi.org/10.18653/v1/2021.bionlp-1.24">10.18653/v1/2021.bionlp-1.24</a></div></div>
</div>
<div id="ref-7oKEC5H3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">102. </div><div class="csl-right-inline"><strong>LinkBERT: Pretraining Language Models with Document Links</strong> <div class="csl-block">Michihiro Yasunaga, Jure Leskovec, Percy Liang</div> <em>arXiv</em> (2022-03-29) <a href="http://arxiv.org/abs/2203.15827">http://arxiv.org/abs/2203.15827</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2203.15827">10.48550/arxiv.2203.15827</a></div></div>
</div>
<div id="ref-4RZT0xlw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">103. </div><div class="csl-right-inline"><strong>Improving Language Understanding by Generative Pre-Training</strong> <div class="csl-block">Alec Radford, Karthik Narasimhan</div> (2018) <a href="https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035">https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035</a></div>
</div>
<div id="ref-BUVSzC66" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">104. </div><div class="csl-right-inline"><strong>Language Models are Unsupervised Multitask Learners</strong> <div class="csl-block">Alec Radford, Jeff Wu, R Child, D Luan, Dario Amodei, I Sutskever</div> (2019) <a href="https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe">https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe</a></div>
</div>
<div id="ref-17n2DOU6H" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">105. </div><div class="csl-right-inline"><strong>BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining</strong> <div class="csl-block">Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu</div> <em>Briefings in Bioinformatics</em> (2022-11-19) <a href="http://arxiv.org/abs/2210.10341">http://arxiv.org/abs/2210.10341</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bib/bbac409">10.1093/bib/bbac409</a></div></div>
</div>
<div id="ref-J8Pydzc8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">106. </div><div class="csl-right-inline"><strong>BioMedGPT: Open Multimodal Generative Pre-trained Transformer for BioMedicine</strong> <div class="csl-block">Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, Zaiqing Nie</div> <em>arXiv</em> (2023-08-21) <a href="http://arxiv.org/abs/2308.09442">http://arxiv.org/abs/2308.09442</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2308.09442">10.48550/arxiv.2308.09442</a></div></div>
</div>
<div id="ref-ZUDBmsmY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">107. </div><div class="csl-right-inline"><strong>LLaMA: Open and Efficient Foundation Language Models</strong> <div class="csl-block">Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, … Guillaume Lample</div> <em>arXiv</em> (2023-02-27) <a href="http://arxiv.org/abs/2302.13971">http://arxiv.org/abs/2302.13971</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2302.13971">10.48550/arxiv.2302.13971</a></div></div>
</div>
<div id="ref-YKj6eC6u" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">108. </div><div class="csl-right-inline"><strong>Article Citations - References - Scientific Research Publishing</strong> <a href="https://www.scirp.org/reference/referencespapers?referenceid">https://www.scirp.org/reference/referencespapers?referenceid</a></div>
</div>
<div id="ref-9POuL4HH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">109. </div><div class="csl-right-inline"><strong>A Revision of Bloom's Taxonomy: An Overview</strong> <div class="csl-block">David R Krathwohl</div> <em>Theory Into Practice</em> (2002-11-01) <a href="https://www.tandfonline.com/doi/full/10.1207/s15430421tip4104_2">https://www.tandfonline.com/doi/full/10.1207/s15430421tip4104_2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1207/s15430421tip4104_2">10.1207/s15430421tip4104_2</a></div></div>
</div>
<div id="ref-AU2DrFMM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">110. </div><div class="csl-right-inline"><strong>SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research</strong> <div class="csl-block">Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu</div> <em>arXiv</em> (2023-08-24) <a href="http://arxiv.org/abs/2308.13149">http://arxiv.org/abs/2308.13149</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2308.13149">10.48550/arxiv.2308.13149</a></div></div>
</div>
<div id="ref-11eq54zNQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">111. </div><div class="csl-right-inline"><strong>C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models</strong> <div class="csl-block">Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, … Junxian He</div> <em>arXiv</em> (2023-11-06) <a href="http://arxiv.org/abs/2305.08322">http://arxiv.org/abs/2305.08322</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2305.08322">10.48550/arxiv.2305.08322</a></div></div>
</div>
<div id="ref-C6CjQZug" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">112. </div><div class="csl-right-inline"><strong>PubMedQA: A Dataset for Biomedical Research Question Answering</strong> <div class="csl-block">Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, Xinghua Lu</div> <em>arXiv</em> (2019-09-13) <a href="http://arxiv.org/abs/1909.06146">http://arxiv.org/abs/1909.06146</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1909.06146">10.48550/arxiv.1909.06146</a></div></div>
</div>
<div id="ref-1AbLepXdc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">113. </div><div class="csl-right-inline"><strong>Crowdsourcing Multiple Choice Science Questions</strong> <div class="csl-block">Johannes Welbl, Nelson F Liu, Matt Gardner</div> <em>arXiv</em> (2017-07-19) <a href="http://arxiv.org/abs/1707.06209">http://arxiv.org/abs/1707.06209</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1707.06209">10.48550/arxiv.1707.06209</a></div></div>
</div>
<div id="ref-ws5eobRv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">114. </div><div class="csl-right-inline"><strong>Pre-training Co-evolutionary Protein Representation via A Pairwise Masked Language Model</strong> <div class="csl-block">Liang He, Shizhuo Zhang, Lijun Wu, Huanhuan Xia, Fusong Ju, He Zhang, Siyuan Liu, Yingce Xia, Jianwei Zhu, Pan Deng, … Tie-Yan Liu</div> <em>arXiv</em> (2021-10-29) <a href="http://arxiv.org/abs/2110.15527">http://arxiv.org/abs/2110.15527</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2110.15527">10.48550/arxiv.2110.15527</a></div></div>
</div>
<div id="ref-RQ40o1d0" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">115. </div><div class="csl-right-inline"><strong>ProteinNPT: Improving Protein Property Prediction and Design with Non-Parametric Transformers</strong> <div class="csl-block">Pascal Notin, Ruben Weitzman, Debora S Marks, Yarin Gal</div> <em>bioRxiv</em> (2023-12-07) <a href="https://www.biorxiv.org/content/10.1101/2023.12.06.570473v1">https://www.biorxiv.org/content/10.1101/2023.12.06.570473v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.12.06.570473">10.1101/2023.12.06.570473</a></div></div>
</div>
<div id="ref-3AjKbAlp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">116. </div><div class="csl-right-inline"><a href="https://openreview.net/forum?id">https://openreview.net/forum?id</a></div>
</div>
<div id="ref-18PNFVXMG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">117. </div><div class="csl-right-inline"><strong>LM-GVP: an extensible sequence and structure informed deep learning framework for protein property prediction</strong> <div class="csl-block">Zichen Wang, Steven A Combs, Ryan Brand, Miguel Romero Calvo, Panpan Xu, George Price, Nataliya Golovach, Emmanuel O Salawu, Colby J Wise, Sri Priya Ponnapalli, Peter M Clark</div> <em>Scientific Reports</em> (2022-04-27) <a href="https://pubmed.ncbi.nlm.nih.gov/35477726">https://pubmed.ncbi.nlm.nih.gov/35477726</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41598-022-10775-y">10.1038/s41598-022-10775-y</a></div></div>
</div>
<div id="ref-1EwWbaeg2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">118. </div><div class="csl-right-inline"><strong>ProGen: Language Modeling for Protein Generation</strong> <div class="csl-block">Ali Madani, Bryan McCann, Nikhil Naik, Nitish Shirish Keskar, Namrata Anand, Raphael R Eguchi, Po-Ssu Huang, Richard Socher</div> <em>arXiv</em> (2020-03-07) <a href="http://arxiv.org/abs/2004.03497">http://arxiv.org/abs/2004.03497</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2004.03497">10.48550/arxiv.2004.03497</a></div></div>
</div>
<div id="ref-NNPUXrCp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">119. </div><div class="csl-right-inline"><strong>ProGen2: Exploring the Boundaries of Protein Language Models</strong> <div class="csl-block">Erik Nijkamp, Jeffrey Ruffolo, Eli N Weinstein, Nikhil Naik, Ali Madani</div> <em>arXiv</em> (2022-06-27) <a href="http://arxiv.org/abs/2206.13517">http://arxiv.org/abs/2206.13517</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2206.13517">10.48550/arxiv.2206.13517</a></div></div>
</div>
<div id="ref-o5h39u4V" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">120. </div><div class="csl-right-inline"><strong>RITA: a Study on Scaling Up Generative Protein Sequence Models</strong> <div class="csl-block">Daniel Hesslow, Niccoló Zanichelli, Pascal Notin, Iacopo Poli, Debora Marks</div> <em>arXiv</em> (2022-07-14) <a href="http://arxiv.org/abs/2205.05789">http://arxiv.org/abs/2205.05789</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2205.05789">10.48550/arxiv.2205.05789</a></div></div>
</div>
<div id="ref-10tIBLTT8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">121. </div><div class="csl-right-inline"><strong>PoET: A generative model of protein families as sequences-of-sequences</strong> <div class="csl-block">Timothy F Truong Jr, Tristan Bepler</div> <em>arXiv</em> (2023-11-01) <a href="http://arxiv.org/abs/2306.06156">http://arxiv.org/abs/2306.06156</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2306.06156">10.48550/arxiv.2306.06156</a></div></div>
</div>
<div id="ref-R8VlFsXM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">122. </div><div class="csl-right-inline"><strong>Structure-informed Language Models Are Protein Designers</strong> <div class="csl-block">Zaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou, Fei YE, Quanquan Gu</div> <em>arXiv</em> (2023-02-09) <a href="http://arxiv.org/abs/2302.01649">http://arxiv.org/abs/2302.01649</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2302.01649">10.48550/arxiv.2302.01649</a></div></div>
</div>
<div id="ref-L387tcBg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">123. </div><div class="csl-right-inline"><strong>Bilingual Language Model for Protein Sequence and Structure</strong> <div class="csl-block">Michael Heinzinger, Konstantin Weissenow, Joaquin Gomez Sanchez, Adrian Henkel, Milot Mirdita, Martin Steinegger, Burkhard Rost</div> <em>bioRxiv</em> (2024-03-24) <a href="https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2">https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.07.23.550085">10.1101/2023.07.23.550085</a></div></div>
</div>
<div id="ref-1HFrE3NU1" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">124. </div><div class="csl-right-inline"><strong>Generative Antibody Design for Complementary Chain Pairing Sequences through Encoder-Decoder Language Model</strong> <div class="csl-block">Simon KS Chu, Kathy Y Wei</div> <em>arXiv</em> (2023-11-20) <a href="http://arxiv.org/abs/2301.02748">http://arxiv.org/abs/2301.02748</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2301.02748">10.48550/arxiv.2301.02748</a></div></div>
</div>
<div id="ref-5ndwLMAS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">125. </div><div class="csl-right-inline"><strong>Fold2Seq: A Joint Sequence(1D)-Fold(3D) Embedding-based Generative Model for Protein Design</strong> <div class="csl-block">Yue Cao, Payel Das, Vijil Chenthamarakshan, Pin-Yu Chen, Igor Melnyk, Yang Shen</div> <em>arXiv</em> (2021-06-24) <a href="http://arxiv.org/abs/2106.13058">http://arxiv.org/abs/2106.13058</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2106.13058">10.48550/arxiv.2106.13058</a></div></div>
</div>
<div id="ref-FLyjjIcB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">126. </div><div class="csl-right-inline"><strong>Predicting Retrosynthetic Reaction using Self-Corrected Transformer Neural Networks</strong> <div class="csl-block">Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, Yuedong Yang</div> <em>arXiv</em> (2019-07-02) <a href="http://arxiv.org/abs/1907.01356">http://arxiv.org/abs/1907.01356</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.1907.01356">10.48550/arxiv.1907.01356</a></div></div>
</div>
<div id="ref-1Fjz4nRHW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">127. </div><div class="csl-right-inline"><strong>DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome</strong> <div class="csl-block">Yanrong Ji, Zhihan Zhou, Han Liu, Ramana V Davuluri</div> <em>Bioinformatics (Oxford, England)</em> (2021-08-09) <a href="https://pubmed.ncbi.nlm.nih.gov/33538820">https://pubmed.ncbi.nlm.nih.gov/33538820</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btab083">10.1093/bioinformatics/btab083</a></div></div>
</div>
<div id="ref-q1jtH9xA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">128. </div><div class="csl-right-inline"><strong>DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome</strong> <div class="csl-block">Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri, Han Liu</div> <em>arXiv</em> (2024-03-18) <a href="http://arxiv.org/abs/2306.15006">http://arxiv.org/abs/2306.15006</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2306.15006">10.48550/arxiv.2306.15006</a></div></div>
</div>
<div id="ref-7LBeQ0qG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">129. </div><div class="csl-right-inline"><strong>iEnhancer-BERT: A Novel Transfer Learning Architecture Based on DNA-Language Model for Identifying Enhancers and Their Strength</strong> <div class="csl-block">springerprofessional.de</div> <a href="https://www.springerprofessional.de/en/ienhancer-bert-a-novel-transfer-learning-architecture-based-on-d/23365796">https://www.springerprofessional.de/en/ienhancer-bert-a-novel-transfer-learning-architecture-based-on-d/23365796</a></div>
</div>
<div id="ref-boBvfMUu" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">130. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/362540943_MoDNA_motif-oriented_pre-training_for_DNA_language_model">https://www.researchgate.net/publication/362540943_MoDNA_motif-oriented_pre-training_for_DNA_language_model</a></div>
</div>
<div id="ref-xvk2JF2w" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">131. </div><div class="csl-right-inline"><strong>GENA-LM: A Family of Open-Source Foundational Models for Long DNA Sequences</strong> <div class="csl-block">Veniamin Fishman, Yuri Kuratov, Maxim Petrov, Aleksei Shmelev, Denis Shepelin, Nikolay Chekanov, Olga Kardymon, Mikhail Burtsev</div> <em>bioRxiv</em> (2023-06-13) <a href="https://www.biorxiv.org/content/10.1101/2023.06.12.544594v1">https://www.biorxiv.org/content/10.1101/2023.06.12.544594v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.06.12.544594">10.1101/2023.06.12.544594</a></div></div>
</div>
<div id="ref-34PAkn0s" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">132. </div><div class="csl-right-inline"><strong>The Nucleotide Transformer: Building and Evaluating Robust Foundation Models for Human Genomics</strong> <div class="csl-block">Hugo Dalla-Torre, Liam Gonzalez, Javier Mendoza Revilla, Nicolas Lopez Carranza, Adam Henryk Grzywaczewski, Francesco Oteri, Christian Dallago, Evan Trop, Hassan Sirelkhatim, Guillaume Richard, … Thomas Pierrot</div> <em>bioRxiv</em> (2023-01-15) <a href="https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1">https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.01.11.523679">10.1101/2023.01.11.523679</a></div></div>
</div>
<div id="ref-zszw9syp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">133. </div><div class="csl-right-inline"><strong>EpiGePT: a Pretrained Transformer model for epigenomics</strong> <div class="csl-block">Zijing Gao, Qiao Liu, Wanwen Zeng, Rui Jiang, Wing Hung Wong</div> <em>bioRxiv</em> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370089/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370089/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.07.15.549134">10.1101/2023.07.15.549134</a></div></div>
</div>
<div id="ref-LZbja2NS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">134. </div><div class="csl-right-inline"><strong>Uni-Rna: Universal Pre-Trained Models Revolutionize Rna Research</strong> <div class="csl-block">Xi Wang, Ruichu Gu, Zhiyuan Chen, Yongge Li, Xiaohong Ji, Guolin Ke, Han Wen</div> <em>bioRxiv</em> (2023-07-12) <a href="https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1">https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.07.11.548588">10.1101/2023.07.11.548588</a></div></div>
</div>
<div id="ref-16YP71KXq" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">135. </div><div class="csl-right-inline"><strong>Effective gene expression prediction from sequence by integrating long-range interactions</strong> <div class="csl-block">Žiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R Kelley</div> <em>Nature Methods</em> (2021-10) <a href="https://pubmed.ncbi.nlm.nih.gov/34608324">https://pubmed.ncbi.nlm.nih.gov/34608324</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-021-01252-x">10.1038/s41592-021-01252-x</a></div></div>
</div>
<div id="ref-VQVJDKnn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">136. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/354105080_LOGO_a_contextualized_pre-trained_language_model_of_human_genome_flexibly_adapts_to_various_downstream_tasks_by_fine-tuning">https://www.researchgate.net/publication/354105080_LOGO_a_contextualized_pre-trained_language_model_of_human_genome_flexibly_adapts_to_various_downstream_tasks_by_fine-tuning</a></div>
</div>
<div id="ref-oDgLqpUO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">137. </div><div class="csl-right-inline"><strong>BioSeq-BLM: a platform for analyzing DNA, RNA and protein sequences based on biological language models</strong> <div class="csl-block">Hong-Liang Li, Yi-He Pang, Bin Liu</div> <em>Nucleic Acids Research</em> (2021-12-16) <a href="https://pubmed.ncbi.nlm.nih.gov/34581805">https://pubmed.ncbi.nlm.nih.gov/34581805</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/gkab829">10.1093/nar/gkab829</a></div></div>
</div>
<div id="ref-6VSmIaGf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">138. </div><div class="csl-right-inline"><strong>GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics</strong> <div class="csl-block">Maxim Zvyagin, Alexander Brace, Kyle Hippe, Yuntian Deng, Bin Zhang, Cindy Orozco Bohorquez, Austin Clyde, Bharat Kale, Danilo Perez-Rivera, Heng Ma, … Arvind Ramanathan</div> <em>bioRxiv</em> (2022-10-11) <a href="https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1">https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2022.10.10.511571">10.1101/2022.10.10.511571</a></div></div>
</div>
<div id="ref-OSbr1Vwp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">139. </div><div class="csl-right-inline"><strong>DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks</strong> <div class="csl-block">Daoan Zhang, Weitong Zhang, Bing He, Jianguo Zhang, Chenchen Qin, Jianhua Yao</div> <em>bioRxiv</em> (2023-07-12) <a href="https://www.biorxiv.org/content/10.1101/2023.07.11.548628v1">https://www.biorxiv.org/content/10.1101/2023.07.11.548628v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.07.11.548628">10.1101/2023.07.11.548628</a></div></div>
</div>
<div id="ref-171NrBohw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">140. </div><div class="csl-right-inline"><strong>HyenaDNA: Long-Range Genomic Sequence Modeling at Single Nucleotide Resolution</strong> <div class="csl-block">Eric Nguyen, Michael Poli, Marjan Faizi, Armin Thomas, Callum Birch-Sykes, Michael Wornow, Aman Patel, Clayton Rabideau, Stefano Massaroli, Yoshua Bengio, … Chris Ré</div> <em>arXiv</em> (2023-11-14) <a href="http://arxiv.org/abs/2306.15794">http://arxiv.org/abs/2306.15794</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2306.15794">10.48550/arxiv.2306.15794</a></div></div>
</div>
<div id="ref-1HaTNUuyf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">141. </div><div class="csl-right-inline"><strong>Understanding the Natural Language of DNA using Encoder-Decoder Foundation Models with Byte-level Precision</strong> <div class="csl-block">Aditya Malusare, Harish Kothandaraman, Dipesh Tamboli, Nadia A Lanman, Vaneet Aggarwal</div> <em>arXiv</em> (2024-02-13) <a href="http://arxiv.org/abs/2311.02333">http://arxiv.org/abs/2311.02333</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2311.02333">10.48550/arxiv.2311.02333</a></div></div>
</div>
<div id="ref-GcVrid39" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">142. </div><div class="csl-right-inline"><strong>Improving language model of human genome for DNA-protein binding prediction based on task-specific pre-training</strong> <div class="csl-block">Hanyu Luo, Wenyu Shan, Cheng Chen, Pingjian Ding, Lingyun Luo</div> <em>Interdisciplinary Sciences, Computational Life Sciences</em> (2023-03) <a href="https://pubmed.ncbi.nlm.nih.gov/36136096">https://pubmed.ncbi.nlm.nih.gov/36136096</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s12539-022-00537-9">10.1007/s12539-022-00537-9</a></div></div>
</div>
<div id="ref-XD0xBDxb" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">143. </div><div class="csl-right-inline"><strong>AnomiGAN: Generative Adversarial Networks for Anonymizing Private Medical Data</strong> <div class="csl-block">Ho Bae, Dahuin Jung, Hyun-Soo Choi, Sungroh Yoon</div> <em>Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing</em> (2020) <a href="https://pubmed.ncbi.nlm.nih.gov/31797628">https://pubmed.ncbi.nlm.nih.gov/31797628</a></div>
</div>
<div id="ref-19IrNgQEN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">144. </div><div class="csl-right-inline"><strong>Creating artificial human genomes using generative neural networks</strong> <div class="csl-block">Burak Yelmen, Aurélien Decelle, Linda Ongaro, Davide Marnetto, Corentin Tallec, Francesco Montinaro, Cyril Furtlehner, Luca Pagani, Flora Jay</div> <em>PLoS genetics</em> (2021-02) <a href="https://pubmed.ncbi.nlm.nih.gov/33539374">https://pubmed.ncbi.nlm.nih.gov/33539374</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pgen.1009303">10.1371/journal.pgen.1009303</a></div></div>
</div>
<div id="ref-AXwYjitz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">145. </div><div class="csl-right-inline"><strong>GPN-MSA: an alignment-based DNA language model for genome-wide variant effect prediction</strong> <div class="csl-block">Gonzalo Benegas, Carlos Albors, Alan J Aw, Chengzhong Ye, Yun S Song</div> <em>bioRxiv</em> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10592768/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10592768/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2023.10.10.561776">10.1101/2023.10.10.561776</a></div></div>
</div>
<div id="ref-vPDj8bGo" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">146. </div><div class="csl-right-inline"><strong>Multi-modal Molecule Structure-text Model for Text-based Retrieval and Editing</strong> <div class="csl-block">Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao, Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar</div> <em>arXiv</em> (2024-01-29) <a href="http://arxiv.org/abs/2212.10789">http://arxiv.org/abs/2212.10789</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2212.10789">10.48550/arxiv.2212.10789</a></div></div>
</div>
<div id="ref-1sH9m3JL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">147. </div><div class="csl-right-inline"><strong>Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models</strong> <div class="csl-block">Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen</div> <em>arXiv</em> (2024-03-04) <a href="http://arxiv.org/abs/2306.08018">http://arxiv.org/abs/2306.08018</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2306.08018">10.48550/arxiv.2306.08018</a></div></div>
</div>
<div id="ref-8pwrkuN4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">148. </div><div class="csl-right-inline"><strong>A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals</strong> <div class="csl-block">Zheni Zeng, Yuan Yao, Zhiyuan Liu, Maosong Sun</div> <em>Nature Communications</em> (2022-02-14) <a href="https://pubmed.ncbi.nlm.nih.gov/35165275">https://pubmed.ncbi.nlm.nih.gov/35165275</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41467-022-28494-3">10.1038/s41467-022-28494-3</a></div></div>
</div>
<div id="ref-m1jSSz9o" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">149. </div><div class="csl-right-inline"><strong>ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts</strong> <div class="csl-block">Minghao Xu, Xinyu Yuan, Santiago Miret, Jian Tang</div> <em>arXiv</em> (2023-07-04) <a href="http://arxiv.org/abs/2301.12040">http://arxiv.org/abs/2301.12040</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2301.12040">10.48550/arxiv.2301.12040</a></div></div>
</div>
<div id="ref-2aufANiR" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">150. </div><div class="csl-right-inline"><strong>Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries</strong> <div class="csl-block">Carl Edwards, ChengXiang Zhai, Heng Ji</div> <em>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em> (2021-11) <a href="https://aclanthology.org/2021.emnlp-main.47">https://aclanthology.org/2021.emnlp-main.47</a> <div class="csl-block">DOI: <a href="https://doi.org/10.18653/v1/2021.emnlp-main.47">10.18653/v1/2021.emnlp-main.47</a></div></div>
</div>
<div id="ref-tP9JVlEF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">151. </div><div class="csl-right-inline"><strong>A Text-guided Protein Design Framework</strong> <div class="csl-block">Shengchao Liu, Yanjing Li, Zhuoxinran Li, Anthony Gitter, Yutao Zhu, Jiarui Lu, Zhao Xu, Weili Nie, Arvind Ramanathan, Chaowei Xiao, … Anima Anandkumar</div> <em>arXiv</em> (2023-12-03) <a href="http://arxiv.org/abs/2302.04611">http://arxiv.org/abs/2302.04611</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2302.04611">10.48550/arxiv.2302.04611</a></div></div>
</div>
<div id="ref-1A950Jvos" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">152. </div><div class="csl-right-inline"><strong>Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers</strong> <div class="csl-block">Hadi Abdine, Michail Chatzianastasis, Costas Bouyioukos, Michalis Vazirgiannis</div> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em> (2024-03-24) <a href="http://arxiv.org/abs/2307.14367">http://arxiv.org/abs/2307.14367</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1609/aaai.v38i10.28948">10.1609/aaai.v38i10.28948</a></div></div>
</div>
<div id="ref-iU5gKzdn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">153. </div><div class="csl-right-inline"><strong>Exploiting pretrained biochemical language models for targeted drug design</strong> <div class="csl-block">Gökçe Uludoğan, Elif Ozkirimli, Kutlu O Ulgen, Nilgün Karalı, Arzucan Özgür</div> <em>Bioinformatics (Oxford, England)</em> (2022-09-16) <a href="https://pubmed.ncbi.nlm.nih.gov/36124801">https://pubmed.ncbi.nlm.nih.gov/36124801</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btac482">10.1093/bioinformatics/btac482</a></div></div>
</div>
<div id="ref-17px28BEq" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">154. </div><div class="csl-right-inline"><strong>SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction</strong> <div class="csl-block">Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, Junzhou Huang</div> <em>Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</em> (2019-09-04) <a href="https://dl.acm.org/doi/10.1145/3307339.3342186">https://dl.acm.org/doi/10.1145/3307339.3342186</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3307339.3342186">10.1145/3307339.3342186</a></div></div>
</div>
<div id="ref-1865CJ7gK" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">155. </div><div class="csl-right-inline"><strong>MolGPT: Molecular Generation Using a Transformer-Decoder Model</strong> <div class="csl-block">Viraj Bagal, Rishal Aggarwal, PK Vinod, UDeva Priyakumar</div> <em>Journal of Chemical Information and Modeling</em> (2022-05-09) <a href="https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600">https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1021/acs.jcim.1c00600">10.1021/acs.jcim.1c00600</a></div></div>
</div>
<div id="ref-1C5bcQVLo" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">156. </div><div class="csl-right-inline"><strong>Generative Pre-Training from Molecules</strong> <div class="csl-block">Sanjar Adilov</div> <em>ChemRxiv</em> (2021-09-16) <a href="https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e">https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e</a> <div class="csl-block">DOI: <a href="https://doi.org/10.26434/chemrxiv-2021-5fwjd">10.26434/chemrxiv-2021-5fwjd</a></div></div>
</div>
<div id="ref-Ho9yh6th" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">157. </div><div class="csl-right-inline"><strong>Molecular Transformer: A Model for Uncertainty-Calibrated Chemical Reaction Prediction</strong> <div class="csl-block">Philippe Schwaller, Teodoro Laino, Théophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, Alpha A Lee</div> <em>ACS Central Science</em> (2019-09-25) <a href="https://pubs.acs.org/doi/10.1021/acscentsci.9b00576">https://pubs.acs.org/doi/10.1021/acscentsci.9b00576</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1021/acscentsci.9b00576">10.1021/acscentsci.9b00576</a></div></div>
</div>
<div id="ref-rTpWDKdy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">158. </div><div class="csl-right-inline"><strong>AI-based screening method could boost speed of new drug discovery</strong> <div class="csl-block">ScienceDaily</div> <a href="https://www.sciencedaily.com/releases/2022/09/220923090832.htm">https://www.sciencedaily.com/releases/2022/09/220923090832.htm</a></div>
</div>
<div id="ref-iwz5DRLF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">159. </div><div class="csl-right-inline"><strong>Broadly applicable and accurate protein design by integrating structure prediction networks and diffusion generative models</strong> <div class="csl-block">Joseph L Watson, David Juergens, Nathaniel R Bennett, Brian L Trippe, Jason Yim, Helen E Eisenach, Woody Ahern, Andrew J Borst, Robert J Ragotte, Lukas F Milles, … David Baker</div> <em>bioRxiv</em> (2022-12-10) <a href="https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1">https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2022.12.09.519842">10.1101/2022.12.09.519842</a></div></div>
</div>
<div id="ref-Ar0akTY8" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">160. </div><div class="csl-right-inline"><strong>Robust deep learning-based protein sequence design using ProteinMPNN</strong> <div class="csl-block">J Dauparas, I Anishchenko, N Bennett, H Bai, RJ Ragotte, LF Milles, BIM Wicky, A Courbet, RJ de Haas, N Bethel, … D Baker</div> <em>Science (New York, N.Y.)</em> (2022-10-07) <a href="https://pubmed.ncbi.nlm.nih.gov/36108050">https://pubmed.ncbi.nlm.nih.gov/36108050</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.add2187">10.1126/science.add2187</a></div></div>
</div>
<div id="ref-zATp0wNO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">161. </div><div class="csl-right-inline"><strong>Ankh: Optimized Protein Language Model Unlocks General-Purpose Modelling</strong> <div class="csl-block">Ahmed Elnaggar, Hazem Essam, Wafaa Salah-Eldin, Walid Moustafa, Mohamed Elkerdawy, Charlotte Rochereau, Burkhard Rost</div> <em>arXiv</em> (2023-01-16) <a href="http://arxiv.org/abs/2301.06568">http://arxiv.org/abs/2301.06568</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2301.06568">10.48550/arxiv.2301.06568</a></div></div>
</div>
<div id="ref-hXLTkVKX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">162. </div><div class="csl-right-inline"><strong>DNAI - The Artificial Intelligence / Artificial Life convergence</strong> <div class="csl-block">Jim Thomas</div> <a href="https://www.scanthehorizon.org/p/dnai-the-artificial-intelligence">https://www.scanthehorizon.org/p/dnai-the-artificial-intelligence</a></div>
</div>
<div id="ref-JkRFkNmt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">163. </div><div class="csl-right-inline"><strong>Challenges in protein folding simulations: Timescale, representation, and analysis</strong> <div class="csl-block">Peter L Freddolino, Christopher B Harrison, Yanxin Liu, Klaus Schulten</div> <em>Nature physics</em> (2010-10-10) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3032381/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3032381/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nphys1713">10.1038/nphys1713</a></div></div>
</div>
<div id="ref-qdpFhm5d" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">164. </div><div class="csl-right-inline"><strong>Highly accurate protein structure prediction with AlphaFold</strong> <div class="csl-block">John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, … Demis Hassabis</div> <em>Nature</em> (2021-08) <a href="https://www.nature.com/articles/s41586-021-03819-2">https://www.nature.com/articles/s41586-021-03819-2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-021-03819-2">10.1038/s41586-021-03819-2</a></div></div>
</div>
<div id="ref-aTuvh2K" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">165. </div><div class="csl-right-inline"><strong>Accurate prediction of protein structures and interactions using a three-track neural network</strong> <div class="csl-block">Minkyung Baek, Frank DiMaio, Ivan Anishchenko, Justas Dauparas, Sergey Ovchinnikov, Gyu Rie Lee, Jue Wang, Qian Cong, Lisa N Kinch, RDustin Schaeffer, … David Baker</div> <em>Science (New York, N.Y.)</em> (2021-08-20) <a href="https://pubmed.ncbi.nlm.nih.gov/34282049">https://pubmed.ncbi.nlm.nih.gov/34282049</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.abj8754">10.1126/science.abj8754</a></div></div>
</div>
<div id="ref-JVovM7kx" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">166. </div><div class="csl-right-inline"><strong>Building the nuclear pore complex</strong> <div class="csl-block">Di Jiang</div> <em>Science</em> (2022-06-10) <a href="https://www.science.org/doi/10.1126/science.add2210">https://www.science.org/doi/10.1126/science.add2210</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.add2210">10.1126/science.add2210</a></div></div>
</div>
<div id="ref-5C9SlgjM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">167. </div><div class="csl-right-inline"><strong>Structure of an endogenous mycobacterial MCE lipid transporter</strong> <div class="csl-block">James Chen, Alice Fruhauf, Catherine Fan, Jackeline Ponce, Beatrix Ueberheide, Gira Bhabha, Damian C Ekiert</div> <em>Nature</em> (2023-08) <a href="https://pubmed.ncbi.nlm.nih.gov/37495693">https://pubmed.ncbi.nlm.nih.gov/37495693</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-023-06366-0">10.1038/s41586-023-06366-0</a></div></div>
</div>
<div id="ref-t5RQ2moY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">168. </div><div class="csl-right-inline"><strong>Critical assessment of methods of protein structure prediction (CASP)-Round XIII</strong> <div class="csl-block">Andriy Kryshtafovych, Torsten Schwede, Maya Topf, Krzysztof Fidelis, John Moult</div> <em>Proteins</em> (2019-12) <a href="https://pubmed.ncbi.nlm.nih.gov/31589781">https://pubmed.ncbi.nlm.nih.gov/31589781</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1002/prot.25823">10.1002/prot.25823</a></div></div>
</div>
<div id="ref-dCXEowTE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">169. </div><div class="csl-right-inline"><strong>The Protein Data Bank</strong> <div class="csl-block">HM Berman, J Westbrook, Z Feng, G Gilliland, TN Bhat, H Weissig, IN Shindyalov, PE Bourne</div> <em>Nucleic Acids Research</em> (2000-01-01) <a href="https://pubmed.ncbi.nlm.nih.gov/10592235">https://pubmed.ncbi.nlm.nih.gov/10592235</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/28.1.235">10.1093/nar/28.1.235</a></div></div>
</div>
<div id="ref-tqicyXuM" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">170. </div><div class="csl-right-inline"><strong>Evolutionary-scale prediction of atomic-level protein structure with a language model</strong> <div class="csl-block">Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, … Alexander Rives</div> <em>Science (New York, N.Y.)</em> (2023-03-17) <a href="https://pubmed.ncbi.nlm.nih.gov/36927031">https://pubmed.ncbi.nlm.nih.gov/36927031</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.ade2574">10.1126/science.ade2574</a></div></div>
</div>
<div id="ref-mG5bRSDU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">171. </div><div class="csl-right-inline"><strong>High-resolution de novo structure prediction from primary sequence</strong> <div class="csl-block">Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, … Jian Peng</div> <em>bioRxiv</em> (2022-07-22) <a href="https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1">https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2022.07.21.500999">10.1101/2022.07.21.500999</a></div></div>
</div>
<div id="ref-1Dw3zwV6h" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">172. </div><div class="csl-right-inline"><strong>Accurate structure prediction of biomolecular interactions with AlphaFold 3</strong> <div class="csl-block">Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, … John M Jumper</div> <em>Nature</em> (2024-06) <a href="https://www.nature.com/articles/s41586-024-07487-w">https://www.nature.com/articles/s41586-024-07487-w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-024-07487-w">10.1038/s41586-024-07487-w</a></div></div>
</div>
<div id="ref-SnkCHfpX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">173. </div><div class="csl-right-inline"><strong>AI-enhanced protein design makes proteins that have never existed</strong> <div class="csl-block">Michael Eisenstein</div> <em>Nature Biotechnology</em> (2023-03-01) <a href="https://www.nature.com/articles/s41587-023-01705-y">https://www.nature.com/articles/s41587-023-01705-y</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41587-023-01705-y">10.1038/s41587-023-01705-y</a></div></div>
</div>
<div id="ref-Cicp6dTw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">174. </div><div class="csl-right-inline"><strong>Artificial Intelligence's Impact on Drug Discovery and Development From Bench to Bedside</strong> <div class="csl-block">KS Vidhya, Ayesha Sultana, Naveen Kumar M, Harish Rangareddy</div> <em>Cureus</em> (2023-10) <a href="https://pubmed.ncbi.nlm.nih.gov/37881323">https://pubmed.ncbi.nlm.nih.gov/37881323</a> <div class="csl-block">DOI: <a href="https://doi.org/10.7759/cureus.47486">10.7759/cureus.47486</a></div></div>
</div>
<div id="ref-e2upB1EQ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">175. </div><div class="csl-right-inline"><strong>Artificial intelligence challenges in the face of biological threats: emerging catastrophic risks for public health</strong> <div class="csl-block">Renan Chaves de Lima, Lucas Sinclair, Ricardo Megger, Magno Alessandro Guedes Maciel, Pedro Fernando da Costa Vasconcelos, Juarez Antônio Simões Quaresma</div> <em>Frontiers in Artificial Intelligence</em> (2024) <a href="https://www.frontiersin.org/articles/10.3389/frai.2024.1382356">https://www.frontiersin.org/articles/10.3389/frai.2024.1382356</a></div>
</div>
<div id="ref-12hNAUYKX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">176. </div><div class="csl-right-inline"><strong>Navigating the legal and ethical challenges of AI in healthcare - KPMG UK</strong> <div class="csl-block">Caroline Rivett Simpson Isabel</div> <em>KPMG</em> (2024-03-01) <a href="https://kpmg.com/uk/en/home/insights/2024/03/navigating-the-legal-and-ethical-challenges-of-ai-in-healthcare.html">https://kpmg.com/uk/en/home/insights/2024/03/navigating-the-legal-and-ethical-challenges-of-ai-in-healthcare.html</a></div>
</div>
<div id="ref-17K5Ty7Fj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">177. </div><div class="csl-right-inline"><strong>Ethical Issues of Artificial Intelligence in Medicine and Healthcare</strong> <div class="csl-block">Dariush D Farhud, Shaghayegh Zokaei</div> <em>Iranian Journal of Public Health</em> (2021-11) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8826344/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8826344/</a> <div class="csl-block">DOI: <a href="https://doi.org/10.18502/ijph.v50i11.7600">10.18502/ijph.v50i11.7600</a></div></div>
</div>
<div id="ref-ye7UmZPY" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">178. </div><div class="csl-right-inline"><strong>Ethical AI in Life Sciences: Impact &amp; Guidelines</strong> <a href="https://www.aciinfotech.com/blogs/ethical-ai-in-life-sciences-impact-guidelines">https://www.aciinfotech.com/blogs/ethical-ai-in-life-sciences-impact-guidelines</a></div>
</div>
<div id="ref-FzEFtq9e" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">179. </div><div class="csl-right-inline"><strong>An extensive benchmark study on biomedical text generation and mining with ChatGPT</strong> <div class="csl-block">Qijie Chen, Haotong Sun, Haoyang Liu, Yinghui Jiang, Ting Ran, Xurui Jin, Xianglu Xiao, Zhimin Lin, Hongming Chen, Zhangmin Niu</div> <em>Bioinformatics (Oxford, England)</em> (2023-09-02) <a href="https://pubmed.ncbi.nlm.nih.gov/37682111">https://pubmed.ncbi.nlm.nih.gov/37682111</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btad557">10.1093/bioinformatics/btad557</a></div></div>
</div>
<div id="ref-nIxULWpe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">180. </div><div class="csl-right-inline"><strong>Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations</strong> <div class="csl-block">Qingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, Hua Xu</div> <em>arXiv</em> (2024-01-20) <a href="http://arxiv.org/abs/2305.16326">http://arxiv.org/abs/2305.16326</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2305.16326">10.48550/arxiv.2305.16326</a></div></div>
</div>
<div id="ref-1DrOhLdUs" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">181. </div><div class="csl-right-inline"><strong>Large Language Models Encode Clinical Knowledge</strong> <div class="csl-block">Karan Singhal, Shekoofeh Azizi, Tao Tu, SSara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, … Vivek Natarajan</div> <em>arXiv</em> (2022-12-26) <a href="http://arxiv.org/abs/2212.13138">http://arxiv.org/abs/2212.13138</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2212.13138">10.48550/arxiv.2212.13138</a></div></div>
</div>
<div id="ref-TKv5YqOB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">182. </div><div class="csl-right-inline"><strong>Large language models in medicine</strong> <div class="csl-block">Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu Wei Ting</div> <em>Nature Medicine</em> (2023-08) <a href="https://pubmed.ncbi.nlm.nih.gov/37460753">https://pubmed.ncbi.nlm.nih.gov/37460753</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41591-023-02448-8">10.1038/s41591-023-02448-8</a></div></div>
</div>
<div id="ref-Q1QW0BcS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">183. </div><div class="csl-right-inline"><strong>Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence</strong> <div class="csl-block">NIST</div> (2023-10-10) <a href="https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence">https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence</a></div>
</div>
<div id="ref-3WhKhYXF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">184. </div><div class="csl-right-inline"><strong>What Does AI Red-Teaming Actually Mean?</strong> <div class="csl-block">Tessa Baker</div> <em>Center for Security and Emerging Technology</em> (2023-10-24) <a href="https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean/">https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean/</a></div>
</div>
<div id="ref-TKrwMi6z" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">185. </div><div class="csl-right-inline"><strong>Purple Teaming: A comprehensive and collaborative approach to cyber security</strong> <div class="csl-block">Erik Van Buggenhout</div> <em>Cyber Security: A Peer-Reviewed Journal</em> (2024) <a href="https://ideas.repec.org//a/aza/csj000/y2024v7i3p207-216.html">https://ideas.repec.org//a/aza/csj000/y2024v7i3p207-216.html</a></div>
</div>
<div id="ref-9ODUtCAc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">186. </div><div class="csl-right-inline"><a href="https://www.researchgate.net/publication/372592054_Violet_Teaming_AI_in_the_Life_Sciences_A_Preprint">https://www.researchgate.net/publication/372592054_Violet_Teaming_AI_in_the_Life_Sciences_A_Preprint</a></div>
</div>
<div id="ref-hhou3YWO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">187. </div><div class="csl-right-inline"><strong>The Promise and Peril of Artificial Intelligence -- Violet Teaming Offers a Balanced Path Forward</strong> <div class="csl-block">Alexander J Titus, Adam H Russell</div> <em>arXiv</em> (2023-08-27) <a href="http://arxiv.org/abs/2308.14253">http://arxiv.org/abs/2308.14253</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arXiv.2308.14253">10.48550/arxiv.2308.14253</a></div></div>
</div>
<div id="ref-ntSixaJH" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">188. </div><div class="csl-right-inline"><strong>Integrating MLSecOps in the Biotechnology Industry 5.0</strong> <div class="csl-block">Naseela Pervez, Alexander J Titus</div> <em>IntechOpen</em> (2024-05-10) <a href="https://www.intechopen.com/online-first/89417">https://www.intechopen.com/online-first/89417</a> <div class="csl-block">ISBN: 9780850144840</div></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen()) ||
        heading.dataset.collapsed === "true"
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script on DOMContentLoaded instead of load to ensure this plugins
  // runs before other plugins
  window.addEventListener("DOMContentLoaded", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script type="module">
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
