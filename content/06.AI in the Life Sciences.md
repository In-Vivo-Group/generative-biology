# AI in the Life Sciences

The intersection of AI and the life sciences (AIxBio) has given rise to new capabilities where advanced computational techniques are applied to understand the complexities of biological systems and engineer novel solutions to pressing challenges in medicine and biotechnology [@url: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5819974]. The two primary modern AI categories used in the life sciences are large language models (LLMs) and bio-AI tools.

LLM-based chatbots like ChatGPT are designed to process human language inputs and generate output in human-like fashion. In the life sciences, ChatGPT can assist researchers by drafting and editing scientific manuscripts, generating hypotheses, summarizing datasets, and retrieving information from the scientific literature. LLM-based chatbots can also streamline literature reviews and facilitate the comprehension of complex biological concepts.

As a general-purpose LLM, ChatGPT and its equivalents are trained on a broad range of text from the internet. This results in models that function across topics and contexts. However, the generalist nature comes at the cost of precision and depth required for highly specialized tasks. For example, LLM-based chatbots can provide outputs with information with unfounded details, aiming to fill knowledge gaps. This behavior is known as “Confabulation”, and it can limit the utility of the tool. Furthermore, ethical concerns related to biased outputs are often attributed to biases within the training data.  

Additionally, training and using general-purpose language models can be computationally expensive, time-consuming, and resource and energy intensive. Given the cost of training general purpose LLMs and their limitations, evaluations are essential for understanding their performance. Evaluations help developers identify strengths and weaknesses of the model, and often measure generalizability of models to real-world applications. This process can also identify biased or misleading model outputs. Typically, models undergo evaluation on standardized benchmarks such as GLUE (General Language Understanding Evaluation) [@url:https://gluebenchmark.com], SuperGLUE [@url:https://super.gluebenchmark.com], HellaSwag [@url:https://arxiv.org/abs/1905.07830], TruthfulQA [@url:https://arxiv.org/abs/2109.07958], and MMLU (Massive Multitask Language Understanding) [@url:https://arxiv.org/abs/2009.03300] using established metrics, as shown in Table 2. 

### Table 2. Common Benchmarks for LLMs

| Benchmark            | Description                                                                                          | Format of Task                                                            |
|----------------------|------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| MMLU                 | MMLU (Massive Multitask Language Understanding) evaluates how well the LLM can multitask             | Multiple-choice                                                          |
| TruthfulQA           | Measures truthfulness of model responses                                                             | Generation, Multiple-choice                                               |
| HellaSwag            | Evaluates how well an LLM can complete a sentence                                                    | Sentence completion                                                       |
| SuperGLUE Benchmark  | Compares more challenging and diverse tasks with GLUE, with comprehensive human baselines            | Sentence- and sentence-pair classification (main task), coreference resolution and question answering |
| GLUE Benchmark       | GLUE (General Language Understanding Evaluation) benchmark provides a standardized set of diverse NLP tasks to evaluate the effectiveness of different language models | Classification and prediction                                            |

The behavior of LLMs can be modified through model alignment, domain-specific pre-training, and supervised fine-tuning. These methods can be used to address limitations of generic LLMs, tailor behavior to meet specific requirements, and infuse general knowledge into the LLMs. Domain-specific language models, trained or fine-tuned on specific datasets relevant to particular domains, offer more contextually accurate responses for specific domains. Evaluating domain-specific or fine-tuned models typically involves comparing their performance against a ground truth dataset if available. This process is crucial because it ensures that the model performs as expected and generates the desired outputs.

In the life sciences, specialized models can interpret complex biological data, provide detailed insights, thereby enhancing both the accuracy and reliability of the information provided. These models are known as scientific large language models (Sci-LLMs) [@url:https://arxiv.org/abs/2401.14656].

![**Figure 4**: AI-enabled tools used in the biological sciences; Large Language Models (LLMs) and Bio-AI.](https://github.com/samadon1/generative-biology/assets/56901167/3e2e3d63-981d-409a-89e3-0c979aa9debc)

## Scientific Large Language Models (Sci-LLMs)

LLMs in the life sciences have been trained on natural language, molecular, protein, and genomic sequence data. These LLMs are collectively known as Scientific Large Language Models (Sci-LLMs). Sci-LLMs are specialized models designed to process and understand various types of scientific data. They extend the capabilities of general LLMs to handle domain-specific tasks in biology, chemistry, and other scientific fields. Sci-LLMs in the biological field include Textual Scientific Large Language Models (Text-Sci-LLMs), Protein Large Language Models (Pro-LLMs), and Genomic Large Language Models (Gene-LLMs) [@url:https://arxiv.org/abs/2401.14656]. 

## Textual Scientific Large Language Models (Text-Sci-LLMs)

Text-Sci-LLMs are trained on vast amounts of scientific textual data, such as scientific publications. Text-Sci-LLMs excel at understanding, generating, and interacting with written human language from scientific domains. LLMs trained on vast, diverse datasets, such as BERT [@url:https://arxiv.org/abs/1810.04805] and its variations which have been fine-tuned specifically on biological corpora with the encoder-only architecture, have demonstrated significant potential in natural language processing (NLP) tasks within biology. Models initially trained on broad corpora such as Wikipedia and textbooks and then fine-tuned on specific biological NLP tasks, show substantial improvements in various downstream tasks including biological terminology understanding, named entity recognition, text similarity, and relation extraction [@url:https://arxiv.org/abs/1901.08746; @url:https://arxiv.org/abs/2010.06060; @url:https://arxiv.org/abs/2007.15779; @url:https://aclanthology.org/2021.bionlp-1.24; @url:https://arxiv.org/abs/2203.15827].

GPT and its variants [@url:https://arxiv.org/abs/2005.14165; @url:https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035; @url:https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe], with decoder-only architectures, have become dominant in the field of biological NLP because they can generate textual information as an output. BioGPT [@url:https://arxiv.org/abs/2210.10341], an extension of GPT-2 [@url:https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe], has been extensively fine-tuned on biomedical literature, showcasing remarkable performance in biomedical relation extraction and question answering. It also generates coherent and fluent descriptions within the biomedical context. BioMedGPT-LM [@url:https://arxiv.org/abs/2308.09442], incrementally pre-trained on LLaMA2 [@url:https://arxiv.org/abs/2302.13971], enables a comprehensive understanding of various biological modalities and aligns them with natural language. BioGPT and BioMedGPT-LM are both specialized language models designed for biomedical applications; however, BioGPT focuses on generating and understanding biomedical literature, while BioMedGPT-LM integrates a broader range of tasks including text generation, question answering, and classification within the biomedical domain.

### Capabilities Evaluation

The evaluation of LLMs often uses Bloom’s taxonomy [@url:https://www.scirp.org/reference/referencespapers?referenceid=2799954; @url:https://www.researchgate.net/publication/242400296_A_Revision_of_Bloom's_Taxonomy_An_Overview], which includes six cognitive levels: 

### Table 3. Bloom’s Taxonomy

| Cognitive Level | Description                                                | Examples of Activities/Tasks                                      |
|-----------------|------------------------------------------------------------|-------------------------------------------------------------------|
| Remember        | Recall facts and basic concepts                            | List, define, identify, memorize, repeat, state                   |
| Understand      | Explain ideas or concepts                                  | Describe, explain, interpret, summarize, paraphrase, discuss      |
| Apply           | Use information or existing knowledge in new contexts      | Use, demonstrate, solve, implement, execute, carry out            |
| Analyze         | Explore connections, causes, and relationships among ideas | Differentiate, organize, relate, compare, contrast, examine       |
| Evaluate        | Justify a decision or course of action based on sound analysis | Judge, critique, recommend, justify, assess, appraise             |
| Create          | Produce new or original work using existing information    | Design, assemble, construct, develop, formulate, author           |

SciEval [@urlhttps://arxiv.org/abs/2308.13149] has recently introduced a framework for evaluating scientific LLMs across four dimensions: basic knowledge, knowledge application, scientific calculation, and research ability. These dimensions are based on the cognitive domains in Bloom’s taxonomy. KnowEval [@url:https://arxiv.org/abs/2401.14656] assesses the depth of knowledge LLMs can grasp, aiming for human-level comprehension. KnowEval categorizes Text-Sci-LLMs into Pre-college, College, and Post-college levels based on the complexity of scientific knowledge.

### Table 4. Categories for KnowEval

| Category           | Description                                                                                                                                                                                                                               |
|--------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Pre-college Level  | This level covers fundamental concepts and principles, aligning with the Remember and Understand stages of Bloom’s taxonomy and the basic knowledge dimension of SciEval. Evaluations focus on basic knowledge comprehension, using benchmarks like MMLU [@url:https://arxiv.org/abs/2009.03300] and C-Eval [@url:https://arxiv.org/abs/2305.08322]          |
| College Level      | At this level, knowledge becomes more specialized and abstract, requiring logical reasoning and proof. It corresponds to the Apply and Analyze stages of Bloom’s taxonomy and the knowledge application and scientific calculation dimensions of SciEval. Evaluations like PubMedQA [@url:https://arxiv.org/abs/1909.06146] and SciQ [@url:https://arxiv.org/abs/1707.06209] focus on this advanced understanding. |
| Post-college Level | This level involves mastering current knowledge and generating innovative ideas, aligning with the Evaluate and Create stages of Bloom’s taxonomy and the research ability dimension of SciEval. It requires capabilities beyond standard question-answering, including summarizing advancements and designing novel experiments. Few benchmarks, such as a subset in the SciEval dataset [@url:https://arxiv.org/abs/2308.13149], assess these high-level capabilities.            |

### Benchmarks for Text-Sci-LLMs 

### Table 5. Summary of Benchmarks for Text-Sci-LLMs

| Benchmark          | Description                                                                                                                                                                                                                  | Type                           |
|--------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------|
| MMLU               | Offers a detailed and challenging benchmark that tests the comprehension and problem-solving capabilities of LLMs across a wide spectrum of tasks and subjects.                                                               | Multiple choice                |
| C-Eval             | Consists of 13,948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels.                                                                                                                         | Multiple choice                |
| AGIEval            | Evaluates the general abilities of foundation models in tasks pertinent to human cognition and problem-solving.                                                                                                               | Multiple choice                |
| ScienceQA          | A dataset designed for question answering in the scientific domain, covering various scientific topics and requiring reasoning over structured and unstructured information.                                                  | Multiple choice / Question answering (QA) |
| SciEval            | A benchmark dataset for evaluating language models in the scientific domain, covering a range of tasks related to scientific text understanding and generation.                                                               | Multiple choice / Question answering (QA) |
| Bioinfo-Bench-QA   | A benchmark dataset focused on question answering in the field of bioinformatics, covering topics related to biological information processing and analysis.                                                                  | Multiple choice                |
| SciQ               | A dataset designed for evaluating language models in scientific question answering tasks, covering various scientific disciplines and requiring both factual and reasoning-based answers.                                       | Multiple choice                |
| ARC                | A dataset that challenges models with questions that require a mix of comprehension and reasoning skills across a wide range of topics, including science.                                                                     | Multiple choice                |
| BLURB              | A comprehensive set of datasets and tasks designed to evaluate the performance of natural language processing (NLP) models specifically in the biomedical domain.                                                             | Multiple NLP tasks             |
| PubMedQA           | A dataset designed for question answering based on biomedical literature available on PubMed, aiming to evaluate models' ability to comprehend and extract information from scientific articles.                              | True or False                  |


## Protein Large Language Models (Prot-LLMs). 

Protein Large Language Models (Prot-LLMs) are trained on protein-related sequence data, including amino acid sequences, protein folding patterns, and other biological information. As a result, they can accurately predict protein structures, functions, and interactions. Prot-LLMs can be categorized into three main types based on their architectures: encoder-only, decoder-only, and encoder-decoder models, each suited for various protein research applications. For instance, encoder-only models are primarily used for predicting protein functions or properties, while decoder-only models are mainly employed for protein generation tasks.

**Encoder-only models**: Encoder-only models are a specialized form of the transformer architecture, dedicated solely to understanding and encoding input sequences.The essence of an encoder-only model revolves around extracting significant context from input sequences. These models encode protein sequences into fixed-length vectors for tasks like pattern recognition and prediction. Techniques like the Pairwise Masked Language Model (PMLM) [@url:https://arxiv.org/abs/2110.15527] and mixed-chunk attention aim to capture co-evolutionary information and reduce complexity. Non-parametric models like ProteinNPT [@url:https://www.biorxiv.org/content/10.1101/2023.12.06.570473v1.full] handle sparse labels and multitask learning. Some models, like ESM-GearNet [@url:https://openreview.net/forum?id=AAML7ivghpY] and LM-GVP [@url:https://pubmed.ncbi.nlm.nih.gov/35477726], integrate 3D structure information for better performance.

**Decoder-only models**: Utilizing the GPT [@url:https://arxiv.org/abs/2005.14165] architecture, these models, such as ProGen [@url:https://arxiv.org/abs/2004.03497] and ProGen2 [@url:https://arxiv.org/abs/2206.13517], are essential for controllable protein generation. They explore unseen regions of the protein space while designing proteins with nature-like properties. Similar capabilities are exemplified by models like RITA [@url:https://arxiv.org/abs/2205.05789], PoET [@url:https://arxiv.org/abs/2306.06156], and LM-Design [@url:https://arxiv.org/abs/2302.01649].

**Encoder-decoder models**: Used for sequence-to-sequence tasks, these models, including ProstT5 [@url:https://www.biorxiv.org/content/10.1101/2023.07.23.550085v2] and pAbT5 [@url:https://arxiv.org/abs/2301.02748] are adept at tasks where an input sequence is transformed into an output sequence. A common example of a sequence-to-sequence task is machine translation, where a model translates a sentence from one language to another. In the context of Prot-LLMs, sequence-to-sequence tasks could involve tasks such as translating between protein sequences and structures. They can incorporate Multiple Sequence Alignment (MSA) modules to improve sequence generation and utilize reinforcement learning for structure-based design, as seen in Fold2Seq [@url:https://arxiv.org/abs/2106.13058].

### Capabilities Evaluation

Prot-LLMs are evaluated in three key areas: protein structure prediction, protein function prediction, and protein sequence generation.

**Protein Structure Prediction**: Prot-LLMs can predict the 3D structure of proteins from their sequences, which aids in understanding protein function, drug design, and biomedical research. Based on the 3D structure of known proteins, prot-LLMs can predict the three-dimensional structure of proteins based on an input sequence, which includes determining the atomic coordinates and the spatial relationships between atoms. Encoder-based Prot-LLMs are used to extract sequence information from the training data and predict tertiary and quaternary structures.

**Protein Function Prediction**: Prot-LLMs can predict the biological function of proteins and their interactions with other biomolecules. These tasks can be grouped into several categories. Firstly, protein classification involves categorizing proteins based on their structure, function, or sequence similarity. Prediction of protein-protein interactions focuses on identifying and forecasting interactions crucial for various biological processes. Localization and homology detection tasks include predicting a protein's subcellular location and identifying distant relationships between protein sequences. Spectral characteristics and stability prediction involve forecasting fluorescence properties and stability under specific conditions, respectively. Furthermore, specific tasks such as 𝛽-Lactamase activity prediction, solubility prediction, and mutation effect prediction focus on understanding specific protein functions, compound solubility, and the effects of genetic mutations on protein function, respectively. These tasks collectively contribute to explaining the complex functions and behaviors of proteins in biological systems. Biological systems are inherently complex and multifaceted, often requiring the simultaneous optimization of multiple properties. Unlike single-objective optimization, which focuses on one specific goal, multi-objective optimization allows researchers to consider and balance several objectives at once. This is particularly important in protein function prediction, where factors such as stability, activity, solubility, and interaction with other molecules need to be optimized concurrently. By providing a more comprehensive optimization framework and utilizing techniques such as Pareto optimization, researchers can identify solutions that offer the best trade-offs among different objectives, rather than a single optimal solution for one objective. multi-objective optimization can enhance the practical applicability of Prot-LLMs, leading to more effective and efficient solutions in understanding and manipulating protein functions.

**Protein Sequence Generation**: Prot-LLMs can propose amino acid sequences not found in nature and with a predicted function, useful in drug design and enzyme engineering. It includes:

- De novo protein design: Proposing protein sequences with a desired property that are not based on existing proteins with some or all of the desired property. Autoregressive generative models, such as the ProGen series, are commonly utilized for tasks involving the generation of protein sequences.
- Protein sequence optimization: proposing modification to an existing protein sequence to alter (i.e., optimize) its function or characteristic in an intended manner. 

### Benchmarks for Prot-LLMs 

### Table 6. Summary of Benchmarks for Prot-LLMs

| Benchmark  | Description                                                                                                                                                                                                                                                                             |
|------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| CASP       | CASP (Critical Assessment of Structure Prediction) evaluates different methods and algorithms for protein structure prediction, providing a standard assessment for progress in the field.                                                                                                 |
| EC         | EC (Enzyme Commission) dataset is used to classify enzymes based on the chemical reactions they catalyze. This system is used to evaluate the functional prediction of proteins, specifically enzymes.                                                                                    |
| GO         | GO (Gene Ontology) provides a framework for the representation of gene and gene product attributes across species. GO terms are used to annotate proteins with their associated biological processes, cellular components, and molecular functions.                                     |
| CATH       | CATH (Class, Architecture, Topology, Homologous superfamily) is a protein structure classification database that organizes protein domains into a hierarchical structure based on their folding patterns. It is used to classify protein domains into these categories: Class, Architecture, Topology, Homologous superfamily. |
| SCOP       | SCOP (Structural Classification of Proteins) classifies proteins based on their structural and evolutionary relationships. SCOP benchmarks evaluate the ability of computational methods to classify protein structures into appropriate categories: Class, Fold, Superfamily, and Family.  |
| ProteinGym | ProteinGym is a benchmark suite designed for evaluating the generalization capabilities of machine learning models in protein sequence prediction tasks. It includes various datasets and metrics to assess the performance of models in predicting protein sequences and related properties under different conditions.               |
| TAPE       | TAPE (Task Assessing Protein Embeddings) is a benchmark suite designed to evaluate the performance of protein sequence embeddings learned by machine learning models. It includes a variety of tasks, such as secondary structure prediction, contact prediction, and remote homology detection, to assess how well these embeddings capture the underlying biological properties of proteins. |

## Genomic Large Language Models (Gene-LLMs)

Gene-LLMs, specialized in genomic data, are trained to comprehend and predict genetic and genomic aspects of biology. They analyze DNA sequences, interpret genetic variations, and aid in genetic research, like identifying disease-related genetic markers or exploring evolutionary biology. Built on the Transformer architecture, genomic LLMs effectively model nucleic acid sequence data, capturing long-range dependencies for prediction and generation tasks. Through self-supervised learning on genomic sequences, Gene-LLMs gradually grasp genome understanding. Once fine-tuned or contextually learned, they prove valuable for downstream tasks, enhancing accuracy and reducing manual intervention.

**Encoder-only models**: With an encoder-only architecture for genomics, numerous significant models utilize the Transformer encoder to process gene sequences and extract meaningful patterns. Models like SpliceBERT, DNABERT, DNABERT-2, iEnhancer-BERT [@url:https://arxiv.org/abs/1907.01356; @url:https://pubmed.ncbi.nlm.nih.gov/33538820; @url:https://arxiv.org/abs/2306.15006; @url:https://www.springerprofessional.de/en/ienhancer-bert-a-novel-transfer-learning-architecture-based-on-d/23365796], and others employ mask training mechanisms to predict and complete masked gene sequences, achieving improved performance in tasks such as promoter prediction and transcription factor binding site prediction.

For instance, MoDNA [@url:https://www.researchgate.net/publication/362540943_MoDNA_motif-oriented_pre-training_for_DNA_language_model] adopts a BERT-like encoder with a unique stacked Generator-Discriminator training paradigm, facilitating motif-oriented learning. GENA-LM [@url:https://www.biorxiv.org/content/10.1101/2023.06.12.544594v1.full] introduces encoder-based foundational DNA language models capable of handling sequences up to 36,000 base pairs. The Nucleotide-Transformer model [@url:https://www.biorxiv.org/content/10.1101/2023.01.11.523679v1], pre-trained on diverse human and species genomes, enhances the prediction of molecular phenotypes from DNA sequences. EpiGePT [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370089] predicts genome-wide epigenomics signals, offering insights into gene regulation. Uni-RNA [@url:https://www.biorxiv.org/content/10.1101/2023.07.11.548588v1] predicts RNA structures and functions, useful in RNA research and drug development. Models like Enformer [@url:https://pubmed.ncbi.nlm.nih.gov/34608324] and LOGO [@url:https://www.researchgate.net/publication/354105080_LOGO_a_contextualized_pre-trained_language_model_of_human_genome_flexibly_adapts_to_various_downstream_tasks_by_fine-tuning] address the quadratic time complexity of attention mechanisms in handling long sequences, while BioSeq-BLM [@url:https://pubmed.ncbi.nlm.nih.gov/34581805] integrates traditional analysis methods with language models, marking advancements in pre-training and fine-tuning.

**Decoder-only models**: Decoder-only models, like GenSLMs [@url:https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1] and DNAGPT [@url:https://www.biorxiv.org/content/10.1101/2023.07.11.548628v1], demonstrate generative capabilities, capturing the evolutionary dynamics of viruses and enabling species identification and regulatory factor prediction. HyenaDNA [@url:https://arxiv.org/abs/2306.15794] stands out for its exceptional ability to efficiently handle ultra-long DNA sequences while preserving single-nucleotide resolution. This unique combination of features enables researchers to analyze and manipulate genetic data at an unprecedented level of detail. Its capability to handle long sequences while maintaining single-nucleotide resolution greatly enhances its utility in various genomic applications, representing a significant advancement in computational genomics.

**Encoder-decoder models**: Encoder-decoder models in genomics, such as ENBED [@url:https://arxiv.org/abs/2311.02333], combine the strengths of both components to compress and encode genomic data into meaningful representations. These representations are then used by the decoder to generate sequences or make predictions, enhancing bioinformatics research capabilities.

### Capabilities Evaluation

Gene-LLMs undergo evaluation across four key domains: function prediction, structure prediction, sequence generation, and sequence variation and evolution analysis.

**Protein Function Prediction**: Traditionally, gene function prediction relied on models trained on specific sequences. With the advent of LLMs, pre-training on extensive genomic data followed by task-specific fine-tuning has enhanced accuracy and contextual understanding. Key subtasks include promoter prediction, enhancer prediction, and binding site prediction, tackled by models like DNABERT [@url:https://pubmed.ncbi.nlm.nih.gov/33538820] and EpiGePT [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10370089]

**Structure Prediction**: Leverages computational tools to identify and model biologically significant nucleic acid structures, aiding in the design of novel molecular architectures for nanotechnology and synthetic biology. Recent advancements include predicting RNA three-dimensional structures directly from sequences and designing sequences for predefined DNA and RNA nanostructures, demonstrating that nucleic acid structure can be both predictable and controllable. Subtasks include chromatin profile prediction and DNA/RNA-protein interaction prediction, addressed by models like HyenaDNA [@url:https://arxiv.org/abs/2306.15794] and TFBert [@url:https://pubmed.ncbi.nlm.nih.gov/36136096].

**Sequence Generation**: Proposing artificial sequences resembling real biological ones is crucial for bioinformatics, particularly for creating artificial human genomes serving as tools to safeguard genetic privacy and reduce costs linked with genetic sample collection [@url:https://pubmed.ncbi.nlm.nih.gov/31797628; @url:https://pubmed.ncbi.nlm.nih.gov/33539374]. The generated data strives to retain the utility of the source data by replicating most of its characteristics. Consequently, they could serve as viable alternatives for many genomic databases that are either not publicly available or face accessibility barriers. DNAGPT [@url:https://www.biorxiv.org/content/10.1101/2023.07.11.548628v1] excels in this task, generating artificial genomes covering regions of single nucleotide polymorphisms (SNPs).

**Sequence Variation and Evolution Analysis**: Understanding biological sequence variation and evolution is vital for uncovering the genetic basis of traits, disease, and evolutionary patterns. Models like GenSLMs [@url:https://www.biorxiv.org/content/10.1101/2022.10.10.511571v1] and GPN-MSA [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10592768] analyze the evolutionary landscape of genomes, focusing on species-specific and whole-genome sequence alignments.

### Benchmarks for Gene-LLMs

### Table 7. Summary of Benchmarks for Gene-LLMs

| Benchmark                                      | Description                                                                                                                                                                                                                                                                               |
|------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| CAGI5 Challenge Benchmark                      | The Critical Assessment of Genome Interpretation (CAGI) is a benchmark designed to rigorously assess computational methods in predicting a wide array of genetic and genomic outcomes.                                                                                                      |
| Protein-RNA Interaction Prediction Benchmark (Protein-RNA) | A set of 37 machine learning (primarily deep learning) methods for in vivo RNA-binding proteins RBP–RNA interaction prediction. This benchmark systematically evaluates a subset of 11 representative methods across hundreds of CLIP-seq datasets and RBPs. |
| Nucleotide Transformer Benchmark (NT-Bench)    | A comprehensive evaluation framework designed to assess the performance of genomics foundational models. This benchmark pits the Nucleotide Transformer models against other prominent genomics models, such as DNABERT, HyenaDNA (with both 1kb and 32kb context lengths), and Enformer.  |

## Multimodal Scientific Large Language Models (MM-Sci-LLMs)

Multimodal scientific large language models (MM-Sci-LLMs) possess the ability to process and combine various types of scientific data, including text, molecules, and proteins, making them indispensable for interdisciplinary research requiring insights from multiple domains. An emerging research area, MM-Sci-LLMs utilize LLMs as their core to handle diverse data types effectively. These models exhibit remarkable adaptability in incorporating text, images, audio, and other forms of information, enabling comprehensive problem-solving across scientific domains, particularly in biological sciences encompassing protein, molecular, and genomic studies.

Categorized into four distinct groups based on the specific modality they focus on, MM-Sci-LLMs demonstrate specialized capabilities.

### Table 8. Summary of MM-Sci-LLMs

| Category | Description | Encoder-only models | Encoder-Decoder models | Decoder-only models |
|---|---|---|---|---|
| Molecule-to-text | Leverage various techniques like multimodal embedding and cross-modal learning to associate chemical structures with textual descriptions, enhancing tasks such as cross-modal retrieval and molecular property prediction. | Text2Mol, KV-PLM, MoMu | DrugChat, MolReGPT, Text+Chem, ChatMol, GIT-Mol | MoIET5, MolFM, GPT-MoI |
| Protein-to-text models | Utilize textual data for protein function prediction and multimodal representation learning, enriching protein annotation and design by integrating natural language descriptions with protein data. | ProTranslator, ProtST-ProtBert | InstructionProtein | ProteinDT, Prot2Text,  ProtST-ESM-1B, ProtST-ESM-2 |
| Protein-to-molecule models  | Focus on linking protein sequences with molecular information, improving drug discovery through techniques like adversarial networks and contrastive learning. | DrugCLIP | DrugGPT | ChemBERTaLM, DeepTarget |
| Comprehensive models  | Integrate multiple scientific modalities to excel in diverse tasks like biological data analysis, and material prediction, leveraging advanced multimodal learning techniques to support fundamental science research. | BioTranslator | Galactica, ChatDrug,  DARWIN-MDP, BioMedGPT-10B, Mol-Instructions    | BioT5 |

### Capabilities Evaluation

MM-Sci-LLMs undergo evaluation focusing on three pivotal areas: cross-modal prediction, retrieval, and generation.

**Cross-Modal Prediction**: This involves using multimodal models to predict the functionality of biological entities like molecules, proteins, and genomes based on textual instructions. Models like MoleculeSTM [@url:https://arxiv.org/abs/2212.10789] and Mol-Instructions [@url:https://arxiv.org/abs/2306.08018] integrate molecular structures and text data for function prediction, which is crucial for bioinformatics and drug discovery.

**Cross-Modal Retrieval**: Involves retrieving information from one modality based on a query from another modality. Key models like KV-PLM [@url:https://pubmed.ncbi.nlm.nih.gov/35165275] and ProtST-ESM-1b [@url:https://arxiv.org/abs/2301.12040] enable retrieving molecules, proteins, or genes based on textual descriptions, aiding drug discovery and biological mechanism understanding.

**Cross-Modal Generation**: Aims to create data in one modality based on data from another. Models like Text2Mol [@url:https://aclanthology.org/2021.emnlp-main.47] and ProteinDT [@url:https://arxiv.org/abs/2302.04611] generate molecular information from text descriptions, while models like Prot2Text [@url:https://arxiv.org/abs/2307.14367] and ChemBERTaLM [@url:https://pubmed.ncbi.nlm.nih.gov/36124801] convert protein sequences into detailed text descriptions. This capability facilitates cohesive multi-modal data creation, bridging the gap between different modalities in scientific research.

### Benchmarks for MM-Sci-LLMs

### Table 9. Summary of Benchmarks for MM-Sci-LLMs

| Benchmark  | Description                                                                                                                                                                                                                                                               |
|------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| MoleculeNet | MoleculeNet is a large-scale benchmark for molecular machine learning. It curates multiple public datasets, establishes metrics for evaluation, and offers high-quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms. |
| MARCEL     | MARCEL (MoleculAR Conformer Ensemble Learning) provides a comprehensive platform for evaluating learning from molecular conformer ensembles. It focuses on diverse molecular conformer structures, marking a significant shift in molecular representation learning.               |
| GuacaMol   | GuacaMol is an evaluation framework designed for de novo molecular design. It aims to generate molecules with specific property profiles through virtual design-make-test cycles.                                                                                          |

Our technical exploration is primarily confined to Transformer-based languages, excluding alternative neural architectures like graph neural networks and diffusion models, despite their widespread applications in protein folding. However, the concepts discussed in biological languages can be extended to other scientific languages, such as molecular and mathematical languages. 

Molecular large language models (Mol-LLMs) are specialized LLMs trained on molecular data, enabling them to understand and predict the chemical properties and behaviors of molecules. This specialized knowledge makes them invaluable tools in drug discovery, materials science, and the study of complex chemical interactions.

Encoder-only Mol-LLMs, like SMILES-BERT [@url:https://www.semanticscholar.org/paper/SMILES-BERT%3A-Large-Scale-Unsupervised-Pre-Training-Wang-Guo/3d99747cc3e13d22f21e02c35e82b57d2e351e2a], focus on understanding and interpreting input molecules, making them ideal for tasks requiring a deep comprehension of molecular structures and properties. SMILES-BERT, for instance, leverages the BERT architecture to interpret SMILES representations of molecules.

Decoder-only Mol-LLMs, such as MolGPT [@url:https://pubs.acs.org/doi/10.1021/acs.jcim.1c00600] and SMILESGPT [@url:https://chemrxiv.org/engage/chemrxiv/article-details/6142f60742198e8c31782e9e], use SMILES strings as input to navigate the vast chemical space. These models are crucial in drug discovery and materials science, enabling the synthesis of molecules with specific properties. MolGPT, which utilizes GPT for molecular generation with conditional training for property optimization, excels in molecular modeling and drug discovery by demonstrating strong control over multiple properties for accurate generation.

In encoder-decoder Mol-LLMs, encoders convert raw molecules into latent vectors, which decoders then reconstruct into functional chemical structures. Most Transformer-based encoder-decoder models use SMILES or SELFIES as inputs for the encoder, with outputs varying by task. For example, in chemical reaction prediction, the decoder generates the anticipated outcomes for reactants. The Molecular Transformer [@url:https://pubs.acs.org/doi/10.1021/acscentsci.9b00576], a Transformer-based model for reaction prediction, effectively handles complex, long-range sequence interactions.

Biological data with graph structures can be modeled in two primary ways: molecular structure-based modeling and biological network-based modeling. In molecular structure-based modeling, atoms or valid chemical substructures are used as nodes, and bonds serve as edges to construct the molecular graph. Molecular graphs are extensively used for predicting molecular properties and designing new molecules.

In biological network-based modeling, nodes represent various entities such as genes, diseases, or RNAs, with edges indicating known associations between pairs of entities, such as miRNA–disease interactions. This creates a relational network. Graph Neural Networks (GNNs) excel at extracting information from graph structures, making them suitable for processing omics data in fields such as genomics, proteomics, RNomics, and radiomics. By applying GNNs to these omics data using the aforementioned modeling methods, a variety of tasks can be performed, including molecular property prediction, de novo molecular design, link prediction, and node classification in biological networks.

## Bio-AI Tools (BDTs)

Bio-AI tools, commonly referred to as biological design tools (BDTs) are computational tools that help design proteins, viral vectors, or other biological agents. Traditional methods molecular biology like site-directed mutagenesis (SDM) involve the deliberate alteration of specific nucleotide sequences in DNA to create desired changes in the resulting protein. This process typically requires designing and synthesizing specific DNA primers, followed by PCR amplification and cloning steps to introduce the mutated DNA into a host organism. While SDM allows for precise modifications at predetermined sites, it can be time-consuming and labor-intensive, especially when multiple iterations are required to achieve the desired outcome. Additionally, the success rate of SDM experiments can vary depending on factors such as the efficiency of DNA synthesis and the stability of the resulting mutant proteins. 

Random mutagenesis, another traditional method, involves introducing random mutations throughout the genome of an organism using techniques such as chemical mutagenesis or UV irradiation. This approach generates a pool of mutants with diverse genetic variations, which are then screened to identify individuals with desired phenotypic traits. While random mutagenesis can uncover novel genetic variants and phenotypes, it lacks the precision and control offered by targeted mutagenesis techniques like SDM. A related concept that enhances the utility of random mutagenesis is directed evolution. Directed evolution is an iterative process where organisms undergo random mutations, are tested against a screening process, and the best performers are selected for subsequent rounds of mutation. This cycle of mutating, screening, and selecting can be analogized to the training process of deep learning models. In deep learning, a model makes predictions based on input data, receives feedback on the accuracy of these predictions, and then adjusts its parameters through a process known as backpropagation.

In directed evolution, the organism's genetic material is repeatedly altered and tested, much like a model's parameters are iteratively refined to improve performance. Each cycle of directed evolution involves creating genetic diversity through random mutations, screening the resultant mutants for desirable traits, and then selecting the top performers for the next round of mutations. This method has been instrumental in fields such as enzyme engineering, where it has led to the development of proteins with enhanced or novel functions. However, It is resource-intensive, requiring significant time and high-throughput screening capabilities.

In contrast, BDTs can accelerate experimentation by suggesting optimized properties of biological agents upfront, thereby potentially reducing the number of tests required to achieve desired outcomes. While the speed of individual experiments may not change, the efficiency of the overall experimentation process is enhanced, as researchers may need to conduct fewer experiments to reach the same or improved results [@url:https://www.sciencedaily.com/releases/2022/09/220923090832.htm]. Examples of BDTs include RFDiffusion [@url:https://www.biorxiv.org/content/10.1101/2022.12.09.519842v1], Protein MPNN [@url:https://pubmed.ncbi.nlm.nih.gov/36108050], and protein language models like ProGen2 [@url:https://arxiv.org/abs/2206.13517] and Ankh [@url:https://arxiv.org/abs/2301.06568]. These models can be considered both Prot-LLMs and specific instances within the broader category of BDTs due to their training and output characteristics.

A crucial difference between LLMs and BDTs is both the training data — as LLMs are trained on natural language while BDTs are trained on biological data — and the output — LLMs typically produce outputs in natural language while BDTs produce outputs in the form of biological sequences, structures, and predictions. Although BDTs currently focus on creating sequences by optimizing for a single function, they may eventually evolve to design complex proteins and enzymes with multiple functions and properties. BDTs may eventually develop the capability to engineer whole organisms optimized for various functions and characteristics, addressing a comprehensive range of biological properties.

Of all the categories of AI-enabled BDTs, protein structural prediction tools have the highest relative maturity. Protein structure prediction tools, commonly referred to as 'folding tools,' contribute to the field by predicting a protein’s 3D structure, including its secondary, tertiary and quaternary structures from its amino acid sequences. This prediction aids in understanding protein function and interactions. Determining the precise structure of proteins, vital for their functions, has historically posed significant challenges in experimental biology [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3032381], often requiring years of dedicated effort. However, the landscape has shifted with the advent of AI, tailored to predict protein structures directly from their amino acid sequences. 

Notably, pioneering AI systems like AlphaFold [@url:https://www.nature.com/articles/s41586-021-03819-2] and RoseTTAFold [@url:https://pubmed.ncbi.nlm.nih.gov/34282049] have emerged, revolutionizing the field by drastically reducing structure determination times from months to mere hours. While AlphaFold provides measured structures based on experimental data and computational predictions, RoseTTAFold predicts structures solely through computational methods, sometimes eliminating the need for experimental measurements. AlphaFold 2, released in 2021, marked a significant breakthrough for deep learning in biology by unveiling a vast array of previously unknown protein structures. It quickly became a valuable tool for researchers working to understand everything from cellular structures [@url:https://www.science.org/doi/10.1126/science.add2210] to tuberculosis [@url:https://pubmed.ncbi.nlm.nih.gov/37495693]. It also inspired the development of other biological deep learning tools. Most notably, the biochemist David Baker and his team at the University of Washington developed a competing algorithm in 2021 called RoseTTAFold, which, like AlphaFold2, predicts protein structures from sequence data. Both systems have since been enhanced with new features. RoseTTAFold Diffusion is designed to create new proteins that do not exist in nature, while AlphaFold Multimer focuses on the interaction of multiple proteins. These advancements have propelled the development of numerous complementary tools that contextualize biochemical data, screen for protein interactions, and aid in experimental structure elucidation. Furthermore, the predictions from these tools have been integrated into publicly accessible databases, fostering widespread access and collaboration. 

Proteins, intricate molecular machines honed by evolution, are built from a repertoire of 20 canonical amino acids, intricately arranged to yield diverse structures crucial for biological functions. Understanding a protein's 3D structure is paramount, as it dictates its functional properties; for instance, an enzyme's precise folding enables effective catalysis. Thus, deciphering protein structures not only determines their biological roles but also sheds light on disease-related mutations and their impacts. A longstanding aspiration in structural biology has been the computational prediction of protein structures, circumventing the laborious and expensive experimental methods. Milestones such as the Critical Assessment of Structure Prediction (CASP) [@url:https://pubmed.ncbi.nlm.nih.gov/31589781] have gauged progress in this domain. AlphaFold's breakthrough at the 13th CASP competition, and subsequent advancements like AlphaFold2 and RoseTTAFold at 14th CASP competition, harnessed the pattern recognition prowess of machine-learning algorithms, trained on vast structural data repositories like the Protein Data Bank (PDB) [@url:https://pubmed.ncbi.nlm.nih.gov/10592235]. These algorithms, unencumbered by prior exposure to certain proteins, demonstrated remarkable accuracy in structure prediction.

Following the 14th CASP competition, a proliferation of AI-enabled structure predictors has emerged. These predictors employ diverse strategies but share a common goal of understanding spatial proximity among amino acids by tracing evolutionary relationships. Multiple sequence alignment structure predictors (MSA-SPs), exemplified by AlphaFold 2 and RoseTTAFold, analyze co-evolutionary signals gleaned from input sequences to predict structures. In contrast, protein language model structure predictors (pLM-SPs), exemplified by ESMFold [@url:https://pubmed.ncbi.nlm.nih.gov/36927031] and OmegaFold [@url:https://www.biorxiv.org/content/10.1101/2022.07.21.500999v1], embed evolutionary insights directly into their algorithms, eliminating the need for explicit MSA generation.
 
AlphaFold 3 [@url:https://www.nature.com/articles/s41586-024-07487-w], a successor to previous AlphaFold models, was released in 2024 by Google DeepMind. This new version extends its capabilities by predicting the structures of nearly all biological molecules and modeling their interactions. While researchers have previously developed specialized computational methods for modeling interactions between specific types of biological molecules, AlphaFold 3 is the first system capable of predicting interactions between almost all molecular types with state-of-the-art performance. The properties and functions of molecules in biological systems typically depend on their interactions with other molecules. Experimental methods to understand these interactions can take years and be prohibitively expensive. However, if these interactions can be accurately estimated computationally, biological research can be significantly accelerated. For instance, researchers looking for a promising drug candidate that binds a specific protein site can use computational systems like AlphaFold 3 to test potential drug molecules efficiently.

Other subcategories of BDTs include:

### Table 10. Other subcategories of BDTs







