# Computers, Algorithms and the Internet

## 1950s and 1960s: Early computers and algorithms

Computers were used in the early 1950s for population genetics calculations [@url:https://pubmed.ncbi.nlm.nih.gov/4859964]. Notably, the inception of computational modeling in biology dates to the origins of computer science itself. British mathematician and logician Alan Turing, often referred to as “the father of computing”, used primitive computers to implement a model of biological morphogenesis (the emergence of pattern and shape in living organisms) in 1952 [@url:https://royalsocietypublishing.org/doi/10.1098/rstb.1952.0012]. At about the same time, a computer called MANIAC was used for measuring speculative genetic codes; it  was originally built for weaponry research at the Los Alamos National Laboratory in New Mexico [@url:https://link.springer.com/article/10.1007/BF02628301].

Computers were used for the study of protein structure by the 1960s, and other increasingly diverse analyses. These developments marked the rise of the computational biology field, stemming from research focused on protein crystallography, in which scientists found computers indispensable for carrying out laborious Fourier analyses to determine the three-dimensional structure of proteins [@url:https://pubmed.ncbi.nlm.nih.gov/13517261; @url:https://pubmed.ncbi.nlm.nih.gov/18990802].

In addition to advances in determination of protein structures through crystallography, the first sequence of protein, insulin, was published [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198157; @url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198158]. More efficient protein sequencing methods, such as the Edman degradation technique [@url:https://pubmed.ncbi.nlm.nih.gov/18134557], enabled sequencing 15 different proteins over a decade [@url:https://pubmed.ncbi.nlm.nih.gov/11252753]. COMPROTEIN, one of the first bioinformatics softwares developed in the early 1960s, was designed to overcome the limitations of Edman sequencing [@url:https://www.semanticscholar.org/paper/Comprotein%3A-a-computer-program-to-aid-primary-Dayhoff-Ledley/5c73ddc7e6e3e142736210c8a2e71cc6e647bc41]. In an effort to simplify the handling of protein sequence data for the COMPROTEIN software, a one-letter amino acid code was developed [@url:https://febs.onlinelibrary.wiley.com/toc/14321033/5/2]. This one-letter code was first used in the Atlas of Protein Sequence and Structure [@url:https://pubmed.ncbi.nlm.nih.gov/20665074], the first biological sequence database, laying the groundwork for paleogenetic studies.

Development of methods to compare protein sequences followed. The Needleman-Wunsch algorithm [@url:https://pubmed.ncbi.nlm.nih.gov/5420325], the first dynamic programming algorithm developed for pairwise protein sequence alignments, was introduced in the 1970s. Multiple sequence alignment (MSA) algorithms followed in the early 1980s. Progressive sequence alignment was introduced by Feng and Doolittle in 1987 [@url:https://pubmed.ncbi.nlm.nih.gov/3118049]. The MSA software CLUSTAL, a simplification of the Feng-Doolittle algorithm [@url:https://pubmed.ncbi.nlm.nih.gov/3243435] was developed in 1988. It is still used and maintained to this day [@url:https://pubmed.ncbi.nlm.nih.gov/24170397].

## 1970s: From protein to DNA analysis

The deciphering of all 64 triplet codons of the genetic code in 196817 fueled a desire to efficiently determine the sequence of DNA that existed into the 1970s. This desire led to the development of  cost-efficient DNA sequencing methods, such as the Maxam-Gilbert and Sanger sequencing techniques in the mid-1970s [@url:https://pubmed.ncbi.nlm.nih.gov/265521; @url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198157; @url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1198158]. With this new ability to generate DNA sequence data, a paradigm shift from protein analysis to DNA analysis occurred in the late 1970s. Concurrently, concerns over recombinant DNA research led to safety protocols established during the 1975 Asilomar conference [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432675].

New DNA sequencing techniques resulted in significantly more data to be analyzed, a task at which computation could help. The first software dedicated to analyzing Sanger sequencing reads was published in 1979 [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC327874]. DNA sequences began to be utilized in phylogenetic inference with pioneering methods like maximum likelihood for inferring phylogenetic trees from DNA sequences [@url:https://pubmed.ncbi.nlm.nih.gov/7288891]. Several bioinformatics tools and statistical methods were developed following this work. The adoption of Bayesian statistics in molecular phylogeny in the 1990s was inspired by this [@url:https://pubmed.ncbi.nlm.nih.gov/8703097] and is still commonly used in biology today [@url:https://pubmed.ncbi.nlm.nih.gov/28983516]. Yet, numerous computational limitations needed to be overcome during the latter half of the 1970s to expand the utilization of computing in the life sciences, especially in DNA analysis. The subsequent decade proved instrumental in addressing these challenges.

![**Figure 1**: The history of parallel advancements in computing and the life sciences: A timeline of major milestones.](https://github.com/samadon1/generative-biology/assets/56901167/4d765e41-8a46-4eb8-9b45-c206e0ab22e3)

## 1980s: Simultaneous advances in computing and biology

Parallel advancements in biology and computing propelled bioinformatics forward during the 1980s and 1990s. Molecular techniques like gene targeting and amplification, using enzymes like restriction endonucleases and DNA ligases, laid the groundwork for genetic engineering [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC432675]. The polymerase chain reaction (PCR) transformed gene amplification, while innovations like Taq polymerase and thermal cyclers optimized the process [@url:https://www.nobelprize.org/prizes/chemistry/1993/mullis/lecture].

Computing accessibility surged with microcomputers like the Commodore PET, Apple II, and Tandy TRS-80, along with bioinformatics software like the GCG software suite [@url:https://pubmed.ncbi.nlm.nih.gov/6546423] and DNASTAR [@url:https://link.springer.com/protocol/10.1385/1-59259-192-2:71], another sequence manipulation suite capable of assembling and analyzing Sanger sequencing data. Other sequence manipulation suites were developed to run on CP/M, Apple II, and Macintosh computers [@url:https://pubmed.ncbi.nlm.nih.gov/6320099] in the years 1984 and 1985. Free code copies of this software were offered on demand by some developers. This propelled an upcoming software-sharing movement in the programming world [@url:https://www.gnu.org/gnu/manifesto.en.html; @url:https://www.researchgate.net/publication/221307757_The_Free_Software_Movement_and_the_GNULinux_Operating_System].

The free software movement, led by the GNU project, promoted open-source bioinformatics tools. Major sequence databases (EMBL, GenBank, DDBJ) standardized data formatting and enabled global sharing. Bioinformatics journals, like CABIOS, which is now known as Bioinformatics (Oxford, England) accentuated computational methods' importance. Desktop workstations with Unix-like systems and scripting languages aided bioinformatics analyses, and scripting languages simplified tool development.

## 1990s: The genomics era and web-based bioinformatics

The genomics era began in the mid-1990s with the complete sequencing of the Haemophilus influenzae genome [@url:https://pubmed.ncbi.nlm.nih.gov/7542800], initiating genome-scale analyses. This milestone was followed by the publication of the human genome at the beginning of the 21st century, which served as the definitive catalyst for the genomic era [@url:https://pubmed.ncbi.nlm.nih.gov/11181995]. This transformative event  spurred the design and development of several specialized Perl-based software to assemble whole-genome sequencing reads: PHRAP [@url:https://pubmed.ncbi.nlm.nih.gov/9521923], Celera Assembler [@url:https://pubmed.ncbi.nlm.nih.gov/10731133] among others. 

Tim Berners-Lee's pioneering work at CERN in the early 1990s resulted in the World Wide Web, transforming global communication and ushering in an era of unprecedented access to information. With the advent of the internet, researchers gained a powerful platform to share and access vast amounts of biological data efficiently. This facilitated collaborative efforts in biology and genomics, leading to the establishment of foundational databases such as the EMBL Nucleotide Sequence Data Library [@url:https://pubmed.ncbi.nlm.nih.gov/8332519] and the GenBank database became the responsibility of the NCBI [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3531190] in 1992. Also, the famous NCBI website came online in 1994, featuring the efficient pairwise alignment tool BLAST [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC441573]. After that, the world saw the birth of major databases we still rely on today: Genomes (1995), PubMed (1997), and Human Genome (1999) [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC29859; @url:https://pubmed.ncbi.nlm.nih.gov; @url:https://academic.oup.com/nar/article/26/1/94/2379498].

The proliferation of web-based resources transformed access to bioinformatics tools, democratizing their availability and usability for researchers worldwide. Through the development of web platforms, bioinformatics tools became more user-friendly and accessible. This shift enabled researchers to interact with sophisticated analytical tools without needing extensive computational expertise or access to specialized hardware. Consequently, the widespread adoption of web-based bioinformatics resources facilitated broader participation in genomic and molecular research, accelerating scientific discovery and collaboration on a global scale. Graphical web servers emerged as a convenient alternative to traditional UNIX-based systems, simplifying data analysis without the need for complex installations. The continued relevance of servers for scientific purposes is exemplified by the AlphaFold Server which uses the latest AlphaFold 3 model [@url:https://pubmed.ncbi.nlm.nih.gov/38718835], released in 2024, to provide highly accurate biomolecular structure predictions in a unified platform. 

The internet facilitated the dissemination of scientific research through online publications, challenging traditional print-based methods. Early initiatives like BLEND [@url:https://www.tandfonline.com/doi/abs/10.1080/00140138208924954] paved the way for internet-based scientific publishing by shedding insights into the potentials and obstacles associated with using the internet for scientific publications. This study paved the way for leveraging the Internet for both data set storage and dissemination, leading up to the establishment of preprint servers like arXiv (est. 1991) [@url:https://arxiv.org] and bioRxiv (est. 2013) [@url:https://www.biorxiv.org] which changed the way scientific findings are shared and accessed. These platforms democratized access to scientific knowledge by enabling researchers to share their work rapidly and openly, facilitating interdisciplinary collaborations and the cross-pollination of ideas.

The experimental determination of the first three-dimensional structure of a protein, specifically, myoglobin, occurred in 1958 via X-ray diffraction [@url:https://pubmed.ncbi.nlm.nih.gov/13517261]. However,  earlier groundwork by Pauling and Corey with the publication of two articles in 1951 that reported the prediction of α-helices and β-sheets [@url:https://pubmed.ncbi.nlm.nih.gov/16578412] laid the foundation for predicting protein structures. Similar to advances in other biological sciences, the utilization of computers has made it feasible to conduct calculations aimed at predicting the secondary and tertiary structure of proteins, with varying levels of confidence. This capability has been notably enhanced by the development of fold recognition algorithms, also known as threading algorithms [@url:https://pubmed.ncbi.nlm.nih.gov/28040746; @url:https://archive.org/details/computationalmet0000unse_u4q5]. However, proteins are dynamic entities, requiring advanced biophysical models to describe their interactions and movements accurately. Force fields have been formulated to describe the interactions among atoms, enabling the introduction of tools for modeling the molecular dynamics of proteins during the 1990s [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4655909]. Used to study the behavior and interactions of atoms and molecules over time, molecular dynamics simulations calculate the positions and velocities of atoms based on physical principles. Despite the theoretical advancements and availability of tools, executing molecular dynamics simulations remained challenging in practice due to the substantial computational resources they demanded.

Graphical processing Units (GPUs) have made molecular dynamics more accessible [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3673555], with applications extending to other bioinformatics fields requiring intensive computation. However, the internet's role in data dissemination, coupled with increasing computational power, has led to the proliferation of 'Big Data' in bioinformatics.

## 2000s: High-throughput sequencing and big data

Second-generation sequencing technologies democratized high-throughput bioinformatics. For example '454' pyrosequencing, a high-throughput DNA sequencing technique played a significant role in advancing genomics research by enabling rapid and cost-effective sequencing of DNA samples, particularly for applications such as whole-genome sequencing [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7122948], but computational challenges arose with increased data volumes. Decreasing sequencing costs resulted in more data being generated, emphasizing data organization and accessibility. Specialized repositories and standardization efforts were needed to ensure data interoperability. High-performance computing adaptation became vital to address the increased amounts of data within bioinformatics projects. The surge in bioinformatics projects, accompanied by a vast influx of data, prompted adjustments from funding bodies to accommodate the demand for high-performance computing resources and collaborative initiatives. 

While basic computer setups suffice for some projects, others demand complex infrastructures and substantial expertise. Government-sponsored entities like [Compute Canada](https://computecanada.ca/), [New York State’s High-Performance Computing Program](https://esd.ny.gov/doing-business-ny/new-york-state-high-performance-computing-program), [The European Technology Platform for High-Performance Computing](https://www.etp4hpc.eu/), and [National Center for High-Performance Computing](https://www.nchc.org.tw/?langid=2) served researchers' computational needs. Companies like Amazon, Microsoft, and Google, among many others, offer bioinformatics and life sciences services, emphasizing the field's importance.

### **Table 1.** Organizations providing High-Performance Computing Resources for Bioinformatics and Life Sciences
| Organization | Computing Resources |
|--------------|---------------------|
| **Compute Canada** | Provides high-performance computing resources and support services to researchers and innovators across Canada. They offer supercomputers, cloud platforms, data storage, and training programs to advance scientific research and innovation in various fields. |
| **New York State’s High-Performance Computing Program** | Provides researchers, businesses, and educational institutions with access to high-performance computing (HPC) resources and expertise to support their computational research and development efforts. |
| **The European Technology Platform for High-Performance Computing** | Fosters collaboration among industry, research, and academic stakeholders to advance high-performance computing (HPC) technology in Europe. |
| **National Center for High-Performance Computing** | Facility for high-performance computing (HPC) resources including large-scale computational science and engineering, cluster and grid computing, middleware development, visualization and virtual reality, data storage, networking, and HPC-related training. |
| **National Center for Supercomputing Applications** | Offers high-performance computing resources such as the Blue Waters supercomputer, provides advanced data storage solutions, data analysis, and visualization tools, and supports interdisciplinary research in fields such as astrophysics, climate modeling, and genomics. |
| **Oak Ridge Leadership Computing Facility** | Provides supercomputing resources, such as the Summit supercomputer, for scientific research, offers support services including software development, data storage, and visualization, and facilitates research in various fields including climate science, biology, and materials science. |
| **Swiss National Supercomputing Centre** | Provides high-performance computing systems including the Piz Daint supercomputer, offers cloud computing services, data management, and user support, and facilitates scientific research in areas such as climate modeling, physics, and life sciences. |
| **Barcelona Supercomputing Center** | Provides access to MareNostrum, one of the most powerful supercomputers in Europe, offers resources for high-performance computing, data storage, and computational sciences, and supports research in fields including bioinformatics, computational biology, and engineering. |
| **Japan's RIKEN Center for Computational Science** | Houses the Fugaku supercomputer, one of the world's fastest supercomputers, provides resources for computational science, data processing, and artificial intelligence, and supports research in fields such as life sciences, materials science, and disaster prevention. |
| **National Supercomputing Centre Singapore** | Provides high-performance computing resources and support services, offers data storage, cloud computing, and software development services, and supports research in fields including bioinformatics, environmental modeling, and smart cities. |


Community computing platforms democratized participation and expanded bioinformatics research's reach. Platforms like BOINC enabled broad participation in bioinformatics. Experts can submit computing tasks to BOINC, while non-experts and science enthusiasts can volunteer their computer resources to process these tasks. Several life sciences projects are available through BOINC, including protein-ligand docking, malaria simulations, and protein folding [@url:https://link.springer.com/article/10.1007/s10723-019-09497-9].

## 2010+: The present and future

The integration of computers into biology has ushered in a new era of research possibilities, allowing for increasingly complex studies. While before, the focus was on individual genes or proteins, advancements today enable the analysis of entire genomes or proteomes [@url:https://pubmed.ncbi.nlm.nih.gov/18761469]. This shift toward a holistic approach in biology is evident in disciplines like genomics, proteomics, and glycomics, which have limited interconnection between them.

The next leap at the intersection of computing and the life sciences lies in modeling entire living organisms and their environments simultaneously, integrating all molecular categories. This has already been achieved in a whole cell model of Mycoplasma genitalium, in which all its genes, products and their known metabolic interactions have been reconstructed [@url:https://pubmed.ncbi.nlm.nih.gov/22817898]. Driven by advancements in measurement techniques, improved computational performance and artificial intelligence (AI) techniques, whole-cell modeling is increasingly becoming realistic and feasible. In contrast to traditional bottom-up approaches relying on molecular interaction networks, a predictive model has been developed for genome-wide phenotypes of budding yeast using deep learning [@url:https://www.nature.com/articles/nmeth.4627]. The main applications of whole-cell modeling have been in producing useful substances and discovering drugs, such as antimicrobials [@url:https://pubmed.ncbi.nlm.nih.gov/26471224; @url:https://pubmed.ncbi.nlm.nih.gov/24556244; @url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7426639; @url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3856890] since whole-cell modeling was first directed toward unicellular organisms. Meanwhile, models of cultured human cells have also been developed, which have found applications in cell differentiation and medical research [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5966287]. The possibility of modeling entire multicellular organisms may not be far off, considering the rapid pace of technological and computational advancements like artificial intelligence (AI) .



