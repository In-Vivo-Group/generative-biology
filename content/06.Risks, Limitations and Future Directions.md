# Risks, Limitations and Future Directions

While recent advancements in AI have enabled rapid progress in the life sciences, it also has several limitations and presents potential risks.

## Risks and Limitations

### Inaccurate outputs from AI models

The effectiveness of AI tools relies heavily on the quality of their algorithms and the data they are trained on. When these algorithms contain errors or the datasets are biased or incomplete, the AI models can produce inaccurate outputs. If the models and logic underlying an AI algorithm are incorrect, the AI's predictions or recommendations will also be incorrect. This can occur due to coding errors, incorrect assumptions in the model design, or inadequate tuning of the model parameters. AI models learn from the data they are trained on. If the training data is biased (e.g., over-represents certain conditions or populations) or incomplete (e.g., missing critical variables or having insufficient diversity), the model's outputs will reflect these shortcomings. This means the AI could give incorrect advice or predictions, which in biological experiments can lead to wasted time and resources as researchers follow flawed directions. The inaccuracies can misguide researchers, causing them to conduct experiments based on false premises. This not only wastes valuable resources like time, money, and materials but can also delay scientific progress.

### Development of harmful biological agents

AI models have the potential to assist in the creation and distribution of harmful biological agents. They could, for example, enable an actor to design a biological agent with favorable properties [@url:https://www.nature.com/articles/s41587-023-01705-y] and modify the agent’s delivery mechanism in a manner that optimizes infectious doses and ensures environmental survival [@url:https://pubmed.ncbi.nlm.nih.gov/37881323].This possibility raises significant biosecurity concerns. Amateur users are unlikely to utilize BDTs, but experts with malicious intent could leverage their scientific training and specific AI models to design new pathogens, develop synthetic DNA strands that evade screening measures, or enhance the efficiency of bioweapon production [@url:https://www.frontiersin.org/articles/10.3389/frai.2024.1382356/full]. As with any AI system, BDTs depend on the quality of their training data, which can sometimes be limited by incompleteness or unintentional biases. While BDTs have been used to digitally generate potentially risky genetic sequences, research has yet to show if the synthesized sequences could be used to create harmful biological agents. Establishing empirical baselines metrics is essential for conducting risk assessments and tracking changes in risk over time [@Titus2023]. In AI applications within the life sciences, these metrics and baselines are not yet defined. To assess this risk, we need to systematically evaluate current AI systems’ abilities to generate new sequences versus enhancing existing ones. 

### Ethics in AI for Life Sciences

The integration of artificial intelligence (AI) in life sciences presents significant ethical challenges that must be addressed to ensure responsible and beneficial use. Key ethical considerations include data privacy, informed consent, and the potential for bias in AI algorithms. Ensuring data privacy is paramount, as AI systems often require access to vast amounts of sensitive biological and medical data. This necessitates robust data protection measures and compliance with legal standards to prevent misuse and unauthorized access [@url:https://kpmg.com/uk/en/home/insights/2024/03/navigating-the-legal-and-ethical-challenges-of-ai-in-healthcare.html]. Informed consent is another critical issue, as individuals must be fully aware of how their data will be used and the potential implications of AI-driven analyses [@url:https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8826344]. Additionally, AI algorithms can inadvertently perpetuate or exacerbate existing biases if the training data is not representative of diverse populations, leading to inequitable outcomes in healthcare and research [@url:https://kpmg.com/uk/en/home/insights/2024/03/navigating-the-legal-and-ethical-challenges-of-ai-in-healthcare.html]. Addressing these ethical concerns requires a multi-faceted approach, including rigorous testing of AI systems, transparency in AI operations, and the establishment of ethical guidelines and governance frameworks to guide the development and deployment of AI in life sciences [@url:https://www.aciinfotech.com/blogs/ethical-ai-in-life-sciences-impact-guidelines]. By prioritizing these ethical considerations, we can harness the transformative potential of AI while safeguarding human rights and promoting equitable access to its benefits.

## Future Directions

### Introduction of new benchmarks

Recent studies have highlighted the shortcomings of existing benchmarks in evaluating LLMs for clinical applications [@url:https://pubmed.ncbi.nlm.nih.gov/37682111; @url:https://arxiv.org/abs/2305.16326]. Traditional benchmarks, which focus mainly on accuracy in medical question-answering, fail to capture the full range of clinical skills necessary for LLMs [@url:https://arxiv.org/abs/2212.13138]. Critics argue that using human-centric standardized medical exams to evaluate LLMs is insufficient, as passing these tests does not reflect the nuanced expertise required in real-world clinical settings [@url:https://arxiv.org/abs/2212.13138].

There is a growing consensus on the need for more comprehensive benchmarks. These new benchmarks should assess capabilities such as sourcing information from authoritative medical references, adapting to the evolving medical knowledge landscape, and clearly communicating uncertainties [@url:https://arxiv.org/abs/2212.13138; @url:https://pubmed.ncbi.nlm.nih.gov/37460753]. To further enhance their relevance, benchmarks should include scenarios that test an LLM’s performance in real-world applications and its ability to adapt to feedback from clinicians while maintaining robustness. Given the sensitive nature of healthcare, these benchmarks should evaluate factors like fairness, ethics, and equity, which are crucial yet challenging to quantify [@url:https://arxiv.org/abs/2212.13138]. By expanding benchmarks to encompass scientific domains, especially the biological domain, we can ensure that LLMs are rigorously evaluated across a broad spectrum of applications, thereby promoting their responsible and effective use in advancing scientific and medical knowledge.

### Red, blue and violet teaming

Due to increasing concerns about the safety, security, and trustworthiness of Generative AI models, both practitioners and regulators emphasize the importance of AI red-teaming [@url:https://www.nist.gov/artificial-intelligence/executive-order-safe-secure-and-trustworthy-artificial-intelligence]. Originally from cybersecurity, red-teaming involves adopting an adversary's perspective to find vulnerabilities. In AI, this means simulating attacks on AI applications to identify weaknesses and develop preventive measures [@url:https://cset.georgetown.edu/article/what-does-ai-red-teaming-actually-mean]. For example, red teams can simulate backdoor attacks or data poisoning to test the AI model's defenses. Prompt injection, a common attack on generative AI models like LLMs, tricks the model into producing harmful content. Red teams can also prompt AI systems to extract sensitive information from training data.

Blue teaming, which focuses on defending against these simulated attacks, and purple teaming, which combines both red and blue teams for a comprehensive security assessment [@url:https://ideas.repec.org/a/aza/csj000/y2024v7i3p207-216.html]. However, as AI systems continuously evolve, these strategies might be insufficient, especially in critical sectors like the life sciences [@url:https://www.researchgate.net/publication/372592054_Violet_Teaming_AI_in_the_Life_Sciences_A_Preprint].

Violet teaming goes further by pairing red and blue teams to build resilient systems that intend to simultaneously minimize harm and maximize benefit using the very technology that poses potential security risks [@url:https://arxiv.org/abs/2308.14253]. In the life sciences, this might involve using AI models to screen for harmful sequences generated by the models themselves, preventing them from being produced and shared with the end user. 

Additionally, Machine Learning Security Operations (MLSecOps) could play a crucial role in ensuring the safety of AI models in the life sciences by employing machine learning (ML) techniques to protect against cyber threats and secure AI/ML models [@url:https://www.intechopen.com/online-first/89417]. MLSecOps focuses on encrypting sensitive genome data, detecting ransomware and Trojan attacks, and ensuring the integrity of ML algorithms used in critical applications. It also addresses vulnerabilities in software and IoT devices within biotechnology labs, enhances supply chain security, and mitigates biases in healthcare ML systems. 


